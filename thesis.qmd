---
format:
  pdf:
      # - paperwidth=17cm
      # - paperheight=24cm
    documentclass: scrbook
    toc: true
    lof: true
    lot: true
    
    # toccolor: BrickRed
    # biblio-style: biblatex
    csl: nature.csl
    css: my_styles.css
    # classoption: nottoc
    # biblio-title: "References"
    mainfont: Zilla Slab Light
    sansfont: Montserrat
    fontsize: 10pt
    geometry:
      - margin=20mm
      - paperwidth=17cm
      - paperheight=24cm
    indent: false
    colorlinks: true
    linkcolor: DarkSlateBlue
    urlcolor: DarkRed
    citecolor: DarkSlateBlue
    link-citations: true
    tbl-cap-location: top
    number-sections: false
    pdf-engine: lualatex
    keep-tex: true
    template-partials: 
      - before-body.tex
    include-in-header: 
      text: |
       % Page setup and typography
        \usepackage[margin=20mm, paperwidth=17cm, paperheight=24cm]{geometry}
        \pagestyle{plain}
        \usepackage{sectsty}
        \usepackage{color}
        \definecolor{color1}{RGB}{80, 80, 80}
        \allsectionsfont{\sffamily \color{color1}}
        \usepackage{multicol}
        
        \let\originaltextbf\textbf
        \renewcommand{\textbf}[1]{\textcolor{color1}{\originaltextbf{#1}}}
    
        \let\originalbfseries\bfseries
        \renewcommand{\bfseries}{\originalbfseries\color{color1}}
        
        % Landscape handling
        \usepackage{lscape}
        \newcommand{\blandscape}{\begin{landscape}}
        \newcommand{\elandscape}{\end{landscape}}
        
        % Captions and listings
        \usepackage[font=small,labelfont=bf]{caption}
        \captionsetup{font=footnotesize}
      
        % Other utilities
        \usepackage{pdfpages}
        \usepackage{hyperref}
        \usepackage{afterpage}
        \usepackage[nottoc,numbib]{tocbibind}
        \newcommand{\aftertocpagenum}{
          \cleardoublepage
          \pagenumbering{arabic}
        }
        
        % Continuous numbering for figures/tables
        \usepackage{chngcntr}
        \counterwithout{figure}{chapter}
        \counterwithout{table}{chapter}
bibliography: refs.bib
editor: source
editor_options: 
  chunk_output_type: console
---

\aftertocpagenum

# English Summary

**Introduction:** Sleep is an important element in promoting health, and the quantification of sleep has been improved with modern technology. Polysomnography, considered the gold standard, provides in-depth insight into sleep but is costly. In contrast, accelerometry is a cheaper and less invasive method, especially for longer home-based recordings. Machine learning is a tool that has the potential to automate and facilitate the estimation of sleep from accelerometer data. However, there are three challenges: producing reliable training data, ensuring data integrity through accurate removal of non-wear, and effectively using data to estimate sleep. Firstly, it is necessary to have sufficient and accurate annotations in the data for effective machine learning, emphasizing the importance of methods for manual annotations based on accelerometer data. Secondly, it is essential to detect and remove periods when the device is not worn to perform accurate analyses. Identifying periods of non-wear is challenging, as traditional methods like logbooks can be prone to bias. Existing algorithms removes bias, but their accuracy is still debated. Finally, once data is correctly collected and processed, it is crucial to apply it effectively. Current methods for estimating sleep using accelerometers are based on data from wrist-worn and hip-worn devices, while data from thigh-worn accelerometers remains largely untapped for sleep estimation.

**Aims:** This thesis has the following objectives. Firstly, we will assess the accuracy of manual annotation of bedtime in raw accelerometer data compared to EEG-based bedtime and sleep diaries. Secondly, we will assess heuristic algorithms and machine learning models for detecting non-wear. Finally, we will develop machine learning models for sleep classification and the estimation of sleep quality metrics using data from thigh-worn accelerometers and compare them with EEG-based sleep recordings. Overall, this thesis aims to understand the potential and challenges of using machine learning to estimate sleep via accelerometry.

**Methods:** The data for the papers included in this thesis are sourced from the SCREENS pilot trial (paper I), the PHASAR study, and an internal validation study (paper II), as well as the SCREENS trial (paper III). Accelerometer data, sleep recordings, and diaries of bedtimes are used from the SCREENS experiments, while PHASAR and the internal validation study provide accelerometer data. All accelerometer data were collected using Axivity AX3 triaxial accelerometers, and sleep was recorded using the EEG-based Zmachine® Insight+ system.

For paper I, accelerometer data from the hip and thigh of 14 children and 19 adults were used. Using Audacity, an open-source audio editing program, three raters annotated each accelerometer recording by marking the times when the person went to bed and when they got out of bed. Two rounds of annotations were performed to test reliability. The manual annotations were evaluated against both sleep log and EG-based sleep recordings. Concordance and agreement was evaluated using the intraclass correlation coefficient and Bland-Altman analyses.

Paper II used accelerometer data from sensors placed on the wrist, thigh, and hip. In data from 64 PHASAR participants and 42 participants in an internal validation study, periods of non-wear were manually annotated in the same way as described in paper I. Three variants of decision trees were trained on 79.2% data from the hip and thigh and were evaluated against a selection of heuristic algorithms and recently developed machine learning models. The remaining data were used for testing purposes for all included algorithms and models. Decision tree hyperparameters were optimized through five-fold cross-validation. External validation was performed on wrist data from all 42 participants from the internal validation study. All included algorithms and models were evaluated using metrics derived from confusion matrices.

For paper III, accelerometry and EEG-based sleep recordings from children aged 4-17 years were used. The predicted sleep stage classes from the EEG-based sleep recording were reduced to "awake" and "sleep", as information about sleep stages is not relevant for generating the sleep quality metrics of interest for this thesis. Data preprocessing included a low-pass Butterworth filter, removal of periods non-wear using the method described paper II, and a set of 64 predictors were constructed. Sleep recordings were median filtered in 5 and 10-minute windows before models were trained to better capture true awakenings. Two model strategies were used, a sequential approach with four types of binary classification models, and the other strategy used a multi-class model. Data for the four pairs of sequential models were divided into training and test sets, and hyperparameter optimization was performed using ten-fold Monte Carlo cross-validation. An imbalance in training data was addressed using the synthetic minority oversampling technique. Data for training the multi-class model was split in a ratio of 50/25/25 for training, validation, and testing. For both strategies, the F1 score was used as an optimization target. To evaluate the performance of all models, metrics derived from confusion matrices were used, and to understand the effectiveness of our models in estimating sleep quality measures, Bland-Altman plots and Pearson correlations were used. The following sleep quality measures were evaluated: sleep period time, total sleep time, sleep efficiency, latency until persistent sleep, and wake after sleep onset.

**Results:** In paper 1, we compared manual annotations of three expert raters of in-bed timestamps with EEG-based and sleep diary in-bed timestamps. The results indicated excellent inter- and intra-rater agreement. Furthermore, the Bland--Altman limits of agreement were approximately ±30 min, showcasing only a minimal mean bias of manual annotation compared to EEG-based and sleep diary in-bed timestamps.

In paper 2, our focus was on non-wear detection. For non-wear periods longer than 60 minutes, the established consecutive zeros algorithms were the most effective, registering F1-scores above 0.96. However, for durations shorter than 60 minutes, decision trees stood out, achieving F1-scores of over 0.74 across all sensor locations. Notably, the newly developed deep learning and random forests models couldn't match these performances.

Paper 3 examined sleep classification and the estimation of sleep quality metrics using thigh-worn accelerometers. Here, the XGBoost model excelled, especially when analyzing 5-minute filtered data. The model demonstrated only small discrepancies in several sleep quality metrics: sleep period time (0.2 minutes), total sleep time (-7.0 minutes), sleep efficiency (-1.1%), and wake after sleep onset (-0.9 minutes). Additionally, this model exhibited a moderate correlation of 0.66 with total sleep time. It's worth noting that the limits of agreement in our findings mirrored those in previous studies on hip and wrist devices. Specifically, total sleep time exhibited LoAs of (95%CI): -95.5 (-105.2 to -88) minutes to 81.4 (72.4 to 92.5) minutes.

**Conclusions:** Overall, the findings of this thesis underscore the reliability and precision of emerging technological methods in sleep and non-wear detection research. Paper 1 validated the credibility of manual annotation techniques, reinforcing their alignment with traditional benchmarks. Paper 2 emphasized the nuances of non-wear detection, revealing clear strengths in certain algorithms for specific durations and highlighting areas where newer models need enhancement. Paper 3 highlights the XGBoost model for sleep assessment with thigh-worn accelerometers, situating it as a valid alternative compared to methods employed on thigh and wrist accelerometer data. However,challenges remain in identifying in-bed awake periods and in assessing sleep quality metrics on an individual-basis, consistent with previous findings from wrist and hip-worn devices.

# Dansk Resume

**Introduktion:** Søvn er et vigtigt element i sundhedsfremme og kvantificeringen af søvn er blevet forbedret med moderne teknologi. Polysomnografi betragtes som guldstandarden, og giver en dybdegående indsigt i søvn, men er omkostningsfuld. Omvendt er accelerometri en billigere og mindre invasiv metode, især til længere optagelser i hjemmet. Maskinlæring er et værktøj, der har potentialet til at automatisere og lette arbejdet med at estimere søvn fra accelerometridata. Dog er der tre udfordringer: at producere pålidelig træningsdata, sikre integriteten af data og effektivt bruge data til at estimere søvn. For det første er det nødvendigt at have tilstrækkeligt med nøjagtige annotationer i data for effektiv maskinlæring, hvilket understreger vigtigheden af metoder til manuelle annotationer baseret på accelerometridata. For det andet, for at udføre korrekte analyser, er det essentielt at detektere og fjerne perioder, hvor sensoren ikke er båret. Det kan være udfordrende at identificere perioder, hvor sensorene ikke bæres, da traditionelle metoder som logbøger kan være fejlbehæftede. Eksisterende algoritmer kan forbedre denne detektering, men deres nøjagtighed er stadig genstand for debat. Endelig, når data er blevet korrekt indsamlet og bearbejdet, er det afgørende at anvende det effektivt. Nuværende metoder til at estimere søvn ved brug accelerometre er baseret på data fra håndleds- og hoftebårne sensorer, mens data fra accelerometre, der bæres på låret, stort set er uudnyttede i forhold til at estimere søvn.

**Formål:** Denne afhandling har følgende formål. For det første vurderes præcisionen af manuel annotation af sengetider i accelerometridata sammenlignet med EEG-baserede sengetider og søvndagbøger. For det andet undersøges eksisternede og nye algoritmer og maskinlæringsmodeller til at detektere perioder, hvor accelerometeret ikke er båret. Endeligt udvikles maskinelæringsmodeller til søvnklassifikation og estimering af søvnkvalitetsmål ved brug af data fra accelerometre, der bæres på låret og sammenligner med EEG-baserede søvnoptagelser. Samlet set søger denne afhandling at forstå potentialet og udfordringerne ved at anvende maskinlæring til at estimere søvn via accelerometri.

**Metoder:** Data til artiklerne i denne afhandling stammer fra SCREENS piloteksperimentet (artikel I), PHASAR-studiet og en intern valideringsundersøgelse (artikel II) samt SCREENS-eksperimentet (artikel III). Fra SCREENS-eksperiemterne gøres brug af accelerometerdata, søvnoptagelser og dagbøger over sengetider, mens PHASAR og det interne valideringsstudie leverer accelerometerdata. Al accelerometerdata blev indsamlet ved hjælp af Axivity AX3 triaksiale accelerometre, og søvnen blev registreret ved hjælp af det EEG-baserede Zmachine® Insight+-system.

Til artikel I benyttedes accelerometerdata fra hofte og lår fra 14 børn og 19 voksne. Ved hjælp af Audacity, et open-source lydredigeringsprogram, annoterede tre bedømmere hver accelerometeroptagelserne ved at markere tidspunkter for, hvornår personen gik i sengen, og hvornår de stod ud af sengen. Der blev udført to runder med annotationer for at teste pålideligheden. 'Ground truth' baseredes på EEG-søvnoptagelserne. Overensstemmelse blev målt ved hjælp af intraklassekorrelationskoefficienten og Bland-Altman-analyser.

Artikel II anvendte accelerometerdata fra sensorer placeret på håndleddet, låret og hoften. I data fra 64 PHASAR-deltagere og 42 deltagere i den interne valideringsundersøgelse annoteredes manuelt perioder hvor sensorerne ikke blev båret på samme måde som metoden beskrevet i artikel I. Tre varianter af decision trees blev trænet på 79,2% data fra hofte og lår og det resterende data blev brugt til test. Hyperparametre blev optimeret gennem en fem-foldig krydsvalidering. Ekstern validering blev udført på håndledsdata fra alle 42 deltagere fra den interne valideringsundersøgelse. Alle inkluderede algoritmer og modeller blev evalueret ved hjælp af mål afledt af confusion matricer.

Til artikel III benyttedes accelerometri og EEG-baserede søvnoptagelser fra børn i alderen 4-17 år. Prædiktionsklasserne fra søvnoptagelsen blev reduceret til "vågen" og "sove", da information om søvnstadier ikke er relevant for generere søvnkvalitetsmål af interesse for denne afhandling. Dataforarbejdningen omfattede et lowpass Butterworth-filter, fjernelse af perioder, hvor sensorerne ikke blev båret via metode fra artikel II og et sæt på 64 prædiktorer blev konstrueret. Søvnoptagelserne blev medianfiltreret i 5 og 10 minutters vinduer inden modellerne blev trænet, for at fange sande opvågninger bedre. To model-strategier blev anvendt, en sekventiel tilgang med fire typer af binære klassifikationsmodeller og den anden strategi anvendte en multiklasse model. Data til de fire par sekventielle modeller blev delt op i trænings- og testsæt og hyperparameteroptimering blev udført ved hjælp af ti-fold Monte Carlo krydsvalidering. En ubalance i træningsdata blev løst ved hjælp af synthetic minority oversampling technique. Data til træning af multiklasse-modellen blev opdelt i et forhold på 50/25/25 for træning, validering og test. For begge strategier blev F1 score anvendt som optimeringsmål. For at vurdere præstationen på alle modeller blev der anvendt mål afledt af confusion matricer og for at forstå effektiviteten af vores modeller til at estimere søvnkvalitetsmål blev Bland-Altman-plots og Pearson-korrelationer anvendt. Følgende søvnkvalitetsmål blev evalueret: tid i sengen, total sovetid, søvneffektivitet, tid til første søvn og vågentid efter første søvn.

**Resultater:** I den første artikel sammenlignes manuelle annotationer med ZM og søvndagbøger. Resultaterne viste fremragende enighed både mellem bedømmere og inden for samme bedømmer. Derudover var Bland-Altman limits of agreement cirka ±30 minutter samtidig med en minimal gennemsnitsbias.

I artikel II undersøges detekteringen af perioder, hvor accelerometrene ikke bliver båret. For perioder af denne type længere end 60 minutter var de etablerede algoritmer, som på forskellig vis detekterer perioder uden acceleration, de mest effektive og opnåede F1-score over 0,96. Decision trees viste sig at præstere bedst på perioder kortere end 60 minutter og opnåede en F1-score på over 0,74 på tværs af alle sensorplaceringer. De nyligt udviklede deep learning- og random forest-modeller kunne ikke matche disse resultater.

Den tredje og sidste artikel beskæftiger sig med søvnklassifikation og søvnkvalitetsmål ved hjælp af accelerometre, der var båret på låret. Her udmærkede XGBoost-modellen sig, især når den analyserede data filtreret i 5 minutter. Modellen viste små afvigelser i flere søvnkvalitetsmål: tid i sengen (0,2 minutter), total sovetid (-7,0 minutter), søvneffektivitet (-1,1%) og vågen efter først søvn (-0,9 minutter). Derudover viste denne model en moderat korrelation på 0,66 med total søvntid. Det er værd at bemærke, at limits of agreements i vores resultater var sammenlignelige med tidligere studier på hofte- og håndledssensorer. Specifikt viste total søvntid limits of agreements på -95,5 minutter til 81,4 minutter.

**Konklusion:** Samlet set undersøger denne afhandling pålideligheden og præcisionen af metoder inden for bearbejdning af accelerometerdata og søvndetektering. Artikel I understreger at manuel annotatering stemmer overens med EEG-baserede og selvrapporterede sengetider. Artikel II fremhævede nuancerne ved detektering af perioder, hvor sensorerne ikke bæres og viste at visse metoder præsterer bedst for specifikke varigheder af perioderne. Artikel III fremhæver XGBoost-modellen som bedst til at klassificere søvn på data fra accelerometre på låret og viser sammenligelige resultater i forhold til metoder, der anvender maskinlæringsmodeller på data fra hofter og håndled. Dog er der stadig udfordringer med at identificere perioder, hvor man er vågen i sengen. Derudover understreger limits of agreements udfordringer i forhold til at vurdere individuelle søvnkvalitetsmål, hvilket er i tråd med tidligere fund fra sensorer, der bæres på håndleddet og hoften.

# Introduction

## Why Track Sleep in Health Research

Physical behaviors throughout a day encompass a range of activities, including sleep, physical activity, and sedentary behavior. A plethora of research over the past decade has strongly emphasized the health benefits of optimal sleep, high pysical activity levels, especially moderate-to-vigorous physical activity[@kraus_physical_2019; @lee_effect_2012], minimal sedentary periods[@wilmot_sedentary_2012], and adequate sleep across all age groups[@cappuccio_sleep_2010; @jennum_søvn_sundhed_2015]. These findings have informed public health guidelines,[@kl_physical_2018; @el-zine_fysisk_nodate-1; @el-zine_fysisk_nodate] as well as recommendations on sleep duration[@hirshkowitz_2015]. The integration of sleep tracking in health research is increasingly crucial for a comprehensive understanding of individual well-being. Sleep and physical activity are instrumental in influencing a broad spectrum of health aspects, ranging from mental health[@biddle_physical_2011] to physical fitness[@warburton_health_2017], and even extending to disease prevention[@strath_guide_2013; @arem_leisure_2015]. With advancements in wearable technology and tracking systems, we now have tools that enable in-depth investigations into the complex interplay between sleep, physical activity, and general health[@rollo_whole_2020]. Given the established significance of sleep and physical activity in health, underscored by extensive research and public health guidelines, integrating sleep tracking into broader health research becomes essential. Modern wearable technologies now grant us the ability to deeply investigate the relationship between sleep, physical activity, and overall health. Thus, a thorough study of sleep is crucial for a holistic grasp of a healthy life.

We spend approximately one-third of our lives asleep, yet much about this state of consciousness remains elusive, including its biological function and what defines its quality[@ma_sleep_2017]. However, what is clear is the essential role of adequate sleep in maintaining both physical and psychological well-being, consolidating memories, and regulating emotions[@worley_2018; @matricciani_2019; @scott_2021]. On the flip side, insufficient sleep has been linked to a range of negative health outcomes such as weight gain, obesity, heart disease, stroke, impaired immune function, and even an elevated risk of death[@consensus_conference_panel_recommended_2015; @ji_2020; @hale_2020]. The impact of sleep deprivation is felt not just in the long term but also immediately. Short-term consequences include reduced alertness, heightened stress levels, diminished concentration, delayed reaction times, and a propensity for risk-taking behavior[@shochat_2014; @kecklund_2016; @obrien_2005; @bonnet_1985]. When poor sleep becomes a chronic issue, it can severely affect one's quality of life. For example, daytime sleepiness increases the risk of accidents in occupational settings and while driving, as well as negatively impacts academic and work performance, with broader social and economic repercussions like school dropouts or job loss[@connor_2002; @dewald_2010; @roth_1996]. The concern over daytime sleepiness becomes particularly acute given that it is widespread, affecting around 10-20% of society[@wang_2019]. Various factors contribute to this, ranging from lifestyle choices like irregular bedtime and shift work to medical conditions that affect the central nervous system and certain medications[@roth_1996].

## Determinants of Sleep in Relation to Health

Sleep is complex, defined by its structure, length, quality, and when it occurs during a day. Growing research indicates that disrupted sleep can harm health, elevating the risk of issues like obesity and diabetes[@reutrakul_2018], heart-related diseases[@cappuccio_sleep_2010], and mental health challenges[@joão_2018].

Human sleep consists of two primary phases: non-REM (Rapid Eye Movement) and REM sleep. Typically, healthy adults begin their sleep in the non-REM phase, which is further broken down into three stages. As individuals transition from one stage to the next, their sleep deepens[@roebuck_2014]. The most regenerative sleep stage, slow-wave sleep or deep sleep, takes place during the third stage of non-REM sleep and usually occurs early at night. On the other hand, REM sleep is marked by heightened brain activity, and its durations extend as the night progresses[@roebuck_2014]. Throughout the night, these sleep stages cycle, usually rotating four to six times in intervals of roughly 90-120 minutes [@roebuck_2014]. Research into disrupted sleep, such as sleep interrupted by alarms, has shown that hindering slow-wave sleep, even without altering total sleep time, can lead to decreased insulin sensitivity, diminished glucose tolerance, elevated sympathetic activity, and higher morning cortisol levels[@stamatakis_2010; @herzog_2013].

Epidemiological and experimental studies have convincingly shown a link between sleep duration and health effects. Cross-sectional studies indicate a "U"-shaped relationship, where both shorter sleep durations (typically less than 6 hours) and longer durations (typically more than 8 hours) correlate with heightened risks of obesity, mental health disorders, coronary heart disease, stroke, and diabetes[@cappuccio_sleep_2010; @reutrakul_2018; @cappuccio_2011; @joão_2018; @cappuccio_2008]. In controlled experiments, depriving healthy adults of sleep negatively impacted their endocrine functions and triggered adverse metabolic and inflammatory reactions[@banks_2007; @vancauter_2008]. Besides total sleep time (TST), sleep quality is primarily determined by factors like sleep onset latency also referred to as latency until persistent sleep (SOL or LPS; the time needed to fall asleep), wake after sleep onset (WASO; the awake duration after initially falling asleep), sleep efficiency (SE; a comparison of total sleep time to total time spent in bed), and the number of times one wakes up during the night[@buysse_2014]. Studies have indicated that subpar sleep quality is linked to a higher risk of chronic illnesses in adults, including obesity, diabetes, and cardiovascular disease[@basnet_2016].

## The Gold Standard for Measuring Sleep

The challenge of studying sleep has become significantly more manageable due to advancements in technology and our understanding of neuroscience. It wasn't until the 1950s that sleep study became scientifically feasible, thanks to the pioneering work of Nathaniel Kleitman and Eugene Aserinsky[@aserinsky_1953], who demonstrated the brain's active involvement in sleep. Utilizing electroencephalography (EEG), Kleitman and Aserinsky were able to measure brain activity and identified that it synchronizes over multiple regions, predominantly within specific frequency bands. This groundbreaking discovery enabled them to define distinct "sleep stages" that the brain cycles through, fundamentally transforming the way sleep is measured and understood. A visual example of these sleep stage cycles can be seen in @fig-hypno. Therefore, measuring sleep, given its complexities, not only is critical but also possible thanks to these technological and scientific milestones.

In a relaxed wakeful state, the EEG predominantly displays alpha activity within the frequency band of 8-10 Hz and amplitudes usually ranging from 10-50 μV. As an individual begins to fall asleep, they enter the non-rapid eye movement (NREM) sleep stage 1 (N1). During this drowsy phase, the brain's EEG activity transitions to the theta range frequencies of 4-6 Hz. Simultaneously, muscle relaxation is evident, respiratory rates decelerate, and there's a drop in both distal body temperature and heart rate. This initial stage is followed by NREM sleep stage 2 (N2), where the EEG spectra frequencies are further reduced. This stage introduces sleep spindles---periodic high-frequency waves oscillating between 12-14 Hz lasting for 0.5-1.5 seconds---and K-complexes, which are characteristic reactive EEG elements of N2 sleep.

Progressing deeper into sleep, one enters NREM sleep stage 3 (N3), often termed "deep sleep". Here, the EEG mainly exhibits high-amplitude oscillations within the delta band of 1-4 Hz. Following these NREM stages, the sleep cycle culminates in REM sleep. Interestingly, the EEG patterns during REM sleep closely resemble the alpha activity seen in wakeful states. This stage is marked by evident rhythmic eye movements, while the respiration and heart rate display enhanced amplitudes and variability. The brain stem suppresses most voluntary body movements at this time, making it a period of relative physical inactivity. Dreaming tends to be more frequent during REM sleep. A single REM episode can span a few minutes and tends to extend in duration during the latter part of the night. On average, an entire sleep cycle, from N1 to REM, spans 90-110 minutes and is typically repeated multiple times throughout the night.

![Sample hypnogram showing the sleep stage cycles of an eight-hour polysomnography recording. The sleep stages (REM, NREM 1-3) and arousals are shown.](figures/hypnogram.pdf){#fig-hypno}

Polysomnography (PSG) is a gold-standard technique in sleep research, allowing simultaneous assessment of various physiological signals influenced during sleep[@sadeh_2015]. PSG gathers electrophysiological data from the brain using a 6-channel EEG, specifically from locations F3, F4, C3, C4, O1, and O2, contrasted against the contralateral mastoids (M1, M2). In addition, it uses electrooculography (EOG) to assess eye movements, electromyography (EMG) to track chin muscle tone and occasional arm and leg movements, and electrocardiography (ECG) to monitor heart rate. The study is augmented by methods to assess respiratory airflow, respiratory effort indicators, as well as peripheral pulse oximetry (PPG)[@ibáñez_2018]. For enhanced data interpretation, an infrared-equipped video camera captures the sleeping subject. Typically, PSG is carried out overnight in a specialized clinical sleep laboratory, assisting in diagnosing various sleep disorders. This technique provides detailed insights into an individual's sleep architecture, revealing sleep and wake durations as well as aiding in the classification of sleep stages[@sadeh_2015]. Sleep, as per standard PSG scoring, is classified into four distinct stages: three stages of non-REM (NREM) sleep and one stage of rapid-eye-movement (REM) sleep[@roebuck_2014]. Such detailed data enables accurate clinical research and the diagnosis of various sleep disorders, such as sleep apnea and periodic movements during sleep[@sadeh_2015]. While PSG offers an unparalleled depth of sleep data, essential for diagnosing an array of sleep disorders, it comes with its own set of limitations. The procedure can be costly, often restricted to one or two nights in a specialized setting under a technician's supervision. This controlled environment may not truly mirror free-living sleep conditions[@sadeh_2015]. Moreover, PSG necessitates specialized personnel to oversee, score, and interpret the data, making it less feasible for expansive or free-living studies[@girschik_validation_2012] and also introducing inter and intra-rater differences in scoring the PSG data[@levendowski_2017]. Hence, while invaluable, PSG is predominantly reserved for individuals presenting sleep-related complaints and for the conclusive diagnosis of sleep disorders.

## The Zmachine® Insight+

Automated EEG data scoring presents a cost-effective alternative that mitigates the subjectivity tied to manual scoring by technicians[@redline_2013]. While there has been an uptick in technological advancements recently, substantial progress is still needed to create objective, dependable, and valid methods to determine sleep metrics[@berthomier_2013]. From the relatively few studies on automated scoring algorithms, some have shown encouraging outcomes. For instance, Malhotra et al.[@malhotra_2013] explored an automated system, comparing its PSG data scoring with visual evaluations done by PSG professionals. They found that their computer-based method yielded outcomes comparable to those of seasoned technologists. Yet, this algorithm relies on several physiological data channels, including EEG, chin EMG, and electrooculography. In the past decade, single-channel EEG-based sleep staging algorithms have started to gain attention among researchers who have proposed a variety of potential scoring methods that are compared against traditional visual scoring[@koley_2012; @fraiwan_2012; @zhu_2014].

The Zmachine® Insight+ (ZM) emerges as an important asset in sleep studies. With positive validation against PSG[@kaplan_performance_2014; @wang_evaluation_2015], the ZM delivers data on par with this gold standard, but without the hefty expenses or the demand for specialized oversight typical of PSG. Notably, the ZM's user-friendliness[@pedersen_self-administered_2021] allows for multi-night evaluations in real-world settings, capturing genuine sleep pattern fluctuations. This offers an edge over one-night PSG studies, making it an ideal primary data source for machine learning analyses. This is because it offers several nights of data without inconsistencies from different raters. However, despite its advantages, the ZM, much like the PSG, still has considerable costs and demands on participants. This underscores the importance of more convenient and cost-effective options like accelerometers.

## Sleep Questionnaires and Diaries

Sleep has often been assessed using a sleep questionnaire in larger-scale studies. These are cost-effective and quick, making them suitable for first-line diagnosis. They quantify a patient's subjective perception of their sleep quality. While these questionnaires are inherently subjective, they've been validated as accurate in numerous studies[@silva_2011; @el-sayed_2012; @firat_2012; @luo_2014; @pataka_2014]. Typically, medical professionals are not needed to administer these questionnaires; they can be self-completed, even at home. For example, several apps exist that instantly provides a report after questionnaire completion, assisting those with potential sleep issues to seek specialist care. It's vital to understand that not all questionnaires measure the same aspect of sleep. While some assess sleep quality, others like the FOSQ-10 evaluate sleepiness[@chasens_2009] whereas instruments like the Pittsburgh Sleep Quality Index offers insights into an individual's overall satisfaction with their sleep over a defined time-frame, often a month[@sadeh_2015]. In population studies, self-report sleep assessments are common but flawed. They can overestimate sleep duration and miss subtle sleep quality details. Their design, summarizing sleep data over weeks, risks recall biases, especially when remembering older sleep patterns[@sadeh_2015]. Factors like weight, ethnicity, and regular sleep duration can influence these self-reports' accuracy[@lauderdale_2008].

Stepping away from these broad self-reports, sleep diaries stand out as a more detailed and structured tool. Often framed as the "gold standard" in subjective sleep assessment, they dive deep into various sleep parameters, like total sleep duration, efficiency, onset latency, and wake periods post sleep onset. Their strength lies in offering a day-by-day account, making it easier to spot disturbances, ascertain precise sleep timings, and decipher the rhythm of daily sleep-wake patterns over an extended duration[@ibáñez_2018]. However, like all tools, they aren't perfect. Their accuracy hinges on participants' memory retention and commitment to regular, detailed diary entries. From the researchers' standpoint, sifting through these extensive diaries can be time-intensive and, for participants, the process can sometimes be seen as taxing, potentially affecting their consistency in logging entries[@thurman_2018].

## Accelerometry for Assessing Sleep

To address the limitations of EEG-based systems and sleep diaries, accelerometers have emerged as a valuable alternative. Generally, an accelerometer is an electronic device that measures both static and dynamic acceleration forces. Static forces, arising from gravitational pull, let devices determine orientation, like a smartphone's landscape or portrait mode. Dynamic forces, resulting from movement or vibrations, are utilized in activities such as step-counting in fitness trackers or detecting collisions in vehicle airbag systems. Through technologies like piezoelectric and MEMS (microelectromechanical systems), accelerometers measure acceleration across various axes, serving diverse applications from aerospace to consumer electronics. Essentially, they are crucial sensors relaying motion or position changes to electronic systems.

While sleep researchers refer to these as "actigraphy devices", those in physical activity studies call them "accelerometers." Both terms denote devices employing accelerometer sensors to detect motion. For physical activity measurement and physical activity type distinction, devices are often placed on the hip or thigh[@migueles_accelerometer_2017; @heesch_2018; @skotte_detection_2014; @brønd_2020; @arvidsson_re-examination_2019], mainly detecting vertical acceleration associated with walking or running. This stemmed from early studies using uni-axial accelerometers that sensed movement in one direction. Devices for sleep, however, are usually wrist-worn. Omnidirectional accelerometers can sense movement in multiple directions, giving a composite signal, whereas triaxial accelerometers, with three orthogonal units, measure acceleration in three planes[@chen_2005]. These tools offer objective insights into sleep patterns in free-living settings over consecutive nights[@conley_agreement_2019]. Their affordability and non-intrusive nature make them more appealing than PSG systems for population studies. However, many studies focus exclusively on sleep or activity, leading participants to wear the device either during the day for activity or at night for sleep. This can result in inaccuracies, like not wearing the device immediately upon waking or removing it before sleep[@meredith-jones_2016]. To improve consistency, guidelines recommend 24-hour device wear[@tudor-locke_2012].

Over the last 30 years, several research studies have highlighted the dependability and accuracy of actigraphy as an alternative to PSG for determining nocturnal sleep-wake patterns[@sadeh_activity-based_1994; @cole_automatic_1992; @hjorth_measure_2012; @tilmanne_2009; @desouza_2003; @littner_2003; @sazonov_activity-based_2004; @granovsky_actigraphy-based_2018]. The findings from these investigations indicate a consistent epoch-by-epoch concordance of 80 to 95% between accelerometer-based sleep-wake scoring methods and the conventional PSG-based scoring. Due to this high degree of accuracy, the inclusion of actigraphy devices has become a standard in sleep medicine protocols for diagnosing various sleep disorders[@smith_2018].

Actigraphy employs algorithms to distinguish sleep from wake states, using movement as an indicator of wakefulness. Though these algorithms vary depending on factors like device brand and placement, they share a common principle: they categorize each epoch based on surrounding activity levels. Early actigraphy devices, due to technical constraints, converted raw acceleration data into activity counts for storage[@neishabouri_2022]. The first of such algorithms emerged in 1982, validated against PSG[@webster_activity-based_1982]. Its successor, the Cole-Kripke algorithm, became widely accepted by 1992[@cole_automatic_1992]. These algorithms typically analyzed activity count-based features around a specific time frame and utilized linear or logistic regression for binary sleep-wake predictions[@sazonov_activity-based_2004]. In response to technological evolution and research demands, manufacturers began offering raw acceleration data from actigraphy devices. This shift allowed the creation of sleep algorithms rooted directly in raw acceleration rather than aggregated activity counts. A notable development in this area was by van Hees et al. (2015), who utilized raw data to calculate the forearm's angle, subsequently establishing a sleep-wake classification method based on the angle's variations over time[@hees_novel_2015] and more presently an algorithm developed for data collected from thigh-worn devices that relies on a constantly changing variable called 'sleep index' that is affected by movement[@johansson_development_2023]. The widespread use of accelerometer devices has produced a wealth of data. However, traditional interpretation methods might not tap into the full potential of this data. This makes a shift towards machine learning methods increasingly crucial, as they offer advanced tools for analyzing these vast datasets.

## Machine Learning and Its Use With Accelerometry

Machine learning, a subfield of artificial intelligence, focuses on the development of algorithms that enable computers to learn from and make predictions or decisions based on data[@hastie01statisticallearning]. Rather than being explicitly programmed to perform a task, these algorithms use statistical techniques to learn patterns from data[@bishop_2006]. This capability to learn from experience allows machines to improve their performance over time, adapting to new information without human intervention. At the heart of machine learning is the concept of training. Given a set of data known as the training dataset, an algorithm iteratively makes predictions and adjusts itself based on any errors it makes. Over time, the algorithm becomes more adept at its task, be it recognizing images, understanding spoken language, forecasting sales, or detecting sleep in accelerometry data. Deep learning, an advanced branch of machine learning, utilizes neural networks with multiple layers---hence the term "deep"---to discern complex patterns within datasets. Notably, architectures such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) stand out in deep learning. CNNs excel at identifying spatial hierarchies and localized patterns in data, while RNNs capture temporal or sequential dependencies. These capabilities enable deep learning models to analyze a vast array of data types with precision and nuance[@Goodfellow-et-al-2016].

There are various types of machine learning and deep learning, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the training data includes both the input and the desired output. The model is "supervised" as it learns from this data until it can accurately predict outcomes for new, unseen data. Unsupervised learning, on the other hand, involves training the model on data without labeled responses, with the goal of discovering hidden patterns in the data. Reinforcement learning is a paradigm where the model learns by interacting with an environment and receiving feedback in the form of rewards or penalties[@sutton_1998]. In this thesis, the focus will be on supervised learning for all tasks regarding detection of behaviors in accelerometry data.

There exists a plethora of algorithms tailored for various supervised learning tasks ranging from simple linear regressions to complex deep learning models. While each algorithm has its unique strengths, their collective objective is to extract patterns from data, facilitating accurate predictions and insights. Linear Regression is a foundational algorithm, aimed at predicting a continuous outcome variable based on one or more predictor variables. The relationship between the variables is considered linear[@kutner_2005]. Support Vector Machines are supervised models designed for classification and regression. By finding the optimal hyperplane, Support Vector Machines are adept at dividing datasets into distinct classes[@cortes_1995]. K-Means Clustering, an unsupervised algorithm, aims to segment data into clusters based on similarity[@macqueen_1967]. Although there exists an almost endless number of learning algorithms, in this thesis, all developed models are based on the following algorithms:

1.  **Logistic Regression**: Contrary to its name, logistic regression is used for binary classification rather than regression. It predicts the probability of an instance belonging to a particular category. The outcome is transformed using the logistic function, ensuring it lies between 0 and 1, making it interpretable as a probability[@McCullagh_1989].

2.  **Decision Trees**: Decision trees offer a graphical representation of possible outcomes to a decision, making them easily interpretable. By segmenting the dataset based on feature values, they can handle both categorical and numerical data. They are employed in diverse areas, including medical diagnosis and credit risk analysis[@quinlan_1986].

3.  **Multilayer Perceptron**: Often termed a single-layer, feed-forward neural network, multilayer perceptrons consist of at least three layers: an input layer, a hidden layer, and an output layer. They can model complex relationships by adjusting weights between nodes during training. Activation functions, like the sigmoid or ReLU, introduce non-linearity into the model[@Goodfellow-et-al-2016].

4.  **XGBoost**: An optimized gradient boosting machine learning library, XGBoost is renowned for its performance and speed. It works by sequentially adding weak learners (typically decision trees) and correcting errors from previous stages, resulting in a strong prediction model. The algorithm has been a dominant force in various Kaggle competitions and is favored for structured or tabular data[@Chen_xgboost_2016]

5.  **Bidirectional LSTM (Long Short-Term Memory)**: LSTM networks, a type of Recurrent Neural Network (RNN), are designed to remember patterns over long sequences and are particularly effective for time-series and NLP tasks. The bidirectional variant processes the sequence data from both past-to-future and future-to-past directions, enhancing the context information available to the network[@graves_2005].

Machine learning and its advanced subset, deep learning, have revolutionized numerous sectors, from healthcare and finance to entertainment and technology. At the core of these advancements lies data, often termed as the new "oil" (a term coined by Clive Humby, 2006) of the digital era. The efficacy and accuracy of machine learning models are inherently tied to the volume, variety, and quality of the data they are trained on. The rationale behind this reliance on data is twofold. Firstly, machine learning algorithms, by design, identify patterns and relationships within data. The richer and more diverse the dataset, the better these algorithms can generalize and make accurate predictions on new, unseen data. Sparse or limited data can lead to overfitting, where the model becomes overly specialized to the training data and performs poorly in real-world applications. Secondly, especially deep learning models, which often employ multi-layered neural networks, have a vast number of parameters. To train these parameters effectively and avoid overfitting, a substantial volume of data is necessary. It's akin to fitting pixels in an image together---the more pixels (or data) you have, the clearer the overall picture (or pattern) becomes. It's worth noting, however, that the sheer volume isn't the only concern. The quality, relevance, and diversity of data play equally crucial roles. High-quality datasets that encompass a broad spectrum of scenarios and edge cases ensure that machine learning and deep learning models are robust, versatile, and reliable in diverse applications.

The availability of large datasets from wearable devices have set the stage for the incorporation of machine learning techniques into research using accelerometry. These algorithms have the capability to analyze vast and complex data sets, delivering insights that were previously difficult or impossible to discern. Whether it's recognizing subtle patterns indicative of sleep apnea for timely intervention[@cappuccio_sleep_2010], or evaluating the effectiveness of antidepressant medications in patients with sleep disturbances[@paruthi_consensus_2016], machine learning adds a layer of precision and depth to data analysis. Machine learning and deep learning methodologies have been effectively applied to sleep research[@tilmanne_2009; @granovsky_actigraphy-based_2018; @palotti_benchmark_2019; @sundararajan_sleep_2021] to identify and categorize sleep and sleep stages. For instance, Tilmanne et al.[@tilmanne_2009] explored the application of Multilayer Perceptrons and Decision Trees in sleep-wake scoring algorithms. They found these techniques to surpass the accuracy of algorithms by Sazonov and Sadeh[@sadeh_activity-based_1994; @sazonov_activity-based_2004]. In another study, Granovsky et al.[@granovsky_actigraphy-based_2018] utilized a Convolutional Neural Networks to classify sleep-wake patterns utilizing actigraphy data from 35 patients with chronic cluster headaches with strong precision and recall scores. While the results from Granovsky et al. are encouraging, their assessment diverges from other related studies, as it wasn't benchmarked against PSG, making it less detailed. Moreover, deep learning models showed enhanced precision over traditional models in the MESA sleep dataset[@lutsey_objectively_2015; @palotti_benchmark_2019] and the algorithm developed by van Hees et al. in 2015[@hees_novel_2015] has since been improved in accuracy with a random forest model that outperformed both its predecessor and established algorithms like Cole-Kripke and Sadeh[@sundararajan_sleep_2021]. Despite these advancements, several challenges persist in the trajectory of creating machine learning models tailored for sleep detection which will be outlined in the following sections.

## Importance of Accurate Data Annotation

The complexity of a machine learning model is closely tied to the number of parameters it must learn. As the number of features a model considers increases, there's a proportional demand for more data. For instance, in predicting housing prices where a model evaluates variables such as location, number of bedrooms, and the neighborhood, a diverse dataset is important to understand the influence of each variable[@hastie01statisticallearning]. While basic algorithms can often produce satisfactory outcomes with relatively limited data, more intricate algorithms, especially those within the deep learning spectrum, have a heightened data requirement. One of the standout attributes of deep learning, in contrast to traditional machine learning, is its ability to draw insights directly from raw data without the need for manual feature engineering. This capability necessitates a richer and more diverse dataset for optimal model performance[@Goodfellow-et-al-2016]. Data volume also depends on the task's complexity and the acceptable error margins for the application. A weather prediction model might tolerate a 20% error, but medical diagnostics require near-perfect accuracy. Lastly, the unpredictability or diversity of input can significantly influence data needs. Take, for instance, an online virtual assistant. Given that users can pose a myriad of queries in various styles and with occasional grammatical errors, the underlying model must be trained on a broad dataset to handle this range of unpredictability[@Goodfellow-et-al-2016].

The complexities and nuances associated with data requirements in machine learning underscore the importance of not only having the right volume and diversity of data but also ensuring its quality and precision[@Goodfellow-et-al-2016]. One of the pivotal aspects ensuring this precision, particularly in supervised learning scenarios, is data labeling. It means marking data with specific tags, guiding models to learn and predict. These labels, which are typically manually added by experts, help the models identify patterns. They can indicate things like categories, feelings, or any task-specific information. The better the annotation, the better the model performs, so thorough annotations is essential. Yet, the manual annotation process, especially by experts, can be complex. Simple visualization might not always offer a comprehensive understanding, potentially leading to inaccurate labels. With the vast data volumes at play, this procedure is not just lengthy but could also be error-prone, especially for lengthy annotation tasks. Inherent limitations in the precision of manual labeling mean that any missteps can profoundly skew results.

In the context of machine learning models tailored for sleep detection in multi-day accelerometry recordings, accurately annotating bedtimes and wake-up moments is essential. While one might consider sourcing annotations from sleep diaries or EEG recordings, many studies have not integrated these within their study design, leading to an information gap which can be circumvented via manually annotating targets of importance. Moreover, many current methods aimed at detecting sleep don't effectively capture the 'in-bed' time also termed the sleep period time. An exception is the HDCZA algorithm by Van Hees et al.[@van_hees_estimating_2018]. The question then becomes whether manual annotations of 'in-bed' times in accelerometry data can serve as a reliable alternative to sleep diaries or EEG recordings. If they can, this would offer a means to enrich a vast amount of existing data that currently lacks associated sleep logs or other 'in-bed' time indicators, making it more suitable for machine learning tasks. However, as of now, there's no research showcasing the effectiveness and precision of such manual annotations.

## Integrity of Accelerometry Data

Data integrity is a cornerstone of any credible research or analytic endeavor. This is especially true for the world of accelerometry, where data accuracy and completeness are important for understanding human motion and behavior. In the context of accelerometry, devices are commonly mounted using tape, elastic belts, or other secure mechanisms. However, as mundane as this step might sound, its significance cannot be overstated. Proper mounting ensures consistent data capture, while flawed or insecure attachments can introduce erroneous readings. Moreover, there's the potential of a device being unintentionally flipped or reattached to an incorrect wearsite after a non-wear period, further complicating the data quality. These seemingly minor missteps can lead to substantial data inaccuracies, casting doubts on the ensuing analyses and findings.

As outlined in previous sections, accelerometry-based activity monitors have proven their worth by providing objective data on movement intensity in a cost-effective manner while minimizing participant burden[@dowd_systematic_2018; @loyen_sedentary_2017; @montoye_raw_2018; @migueles_comparability_2019]. However, challenges emerge when considering non-wear time, the intervals during which devices aren't worn due to activities like swimming, sleeping, certain sports, or sensor malfunctioning or other unforeseen incidences. This non-wear time introduces what can be equated to 'missing data' in the dataset. Such missing data could be overlooked or replaced using statistical imputations like zero-inflated Poisson and Log-normal distributions[@lee_missing_2018]. Yet, this process is not without its pitfalls, as imputation might infuse biases rooted in data assumptions. The importance of an accurate classification of non-wear time thus gains paramount significance, ensuring the data quality remains pristine, especially in free-living scenarios.

To circumvent this classification challenge, some research methods require subjects to maintain a log diary of all non-wear periods, an approach that is susceptible to errors and can be taxing on participants[@ainsworth_recommendations_2012]. Seeking better accuracy, the scientific community turned to both rule-based methods and more advanced algorithms. A notable early method designed for ActiGraph counts data would identify non-wear time when consecutive zero counts surpassed a certain duration[@hecht_methodology_2009; @ruiz_objectively_2011; @troiano_how_2020]. But even a minor tweak in this duration threshold could sway the results by a notable 10%[@aadland_comparison_2018]. The field also grappled with proprietary algorithms that not only lacked transparency but also were affected by factors like age and obesity. These variables subsequently hindered the comparability across multiple studies[@toftager_accelerometer_2013].

But technology, being ever-evolving, introduced accelerometers capable of storing raw acceleration data. This leap promised enhanced granularity of data and a more precise non-wear period classification. Algorithms soon popped up aiming to decode non-wear time from this raw data, with some even harnessing skin temperature measurements for better results[@duncan_wear-time_2018; @rasmussen_short-term_2020; @zhou_classification_2015]. These heuristic approaches are generalizable across various populations, device brands, and wear-sites. In this setting, the term "generalizability" refers to how a model performs when applied to unfamiliar datasets. High generalizability would suggest versatility across varied settings and populations, whereas low generalizability points to the contrary. The limitation of the heuristic duration-based algorithms is that they may erroneously classify true non-wear as inactivity due to their time-specific intervals, missing shorter non-wear episodes. Furthermore, the volume and quality of the data collected do not directly correspond to better performance. Contrary to machine learning models that improves with increasing amounts of data.

With the progress in technology, machine learning methods such as random forests[@sundararajan_sleep_2021] and deep learning[@syed_novel_2021] entered the scene, aimed at classifying non-wear using raw accelerometer data. However, these models must balance between variance and bias. While high bias results in underfitting, high variance leads to overfitting, where a model performs exceptionally on training data but poorly with new data. These advanced machine learning models, even with their robust performance on test data, are wrapped in uncertainty when it comes to unseen, external data. External validation is a crucial aspect, especially considering the universal aim of non-wear detection methods. But often, this step is overlooked either due to limited out-of-distribution data or the intent to harness all data for training. Some studies have tried blending surface skin temperature with raw acceleration data for non-wear classification[@duncan_wear-time_2018, @zhou_classification_2015], yet the broader implications of this combination using machine learning are still unexplored. Despite these strides in technology and methodology, the perfect classification of non-wear time in raw accelerometer data remains elusive. The heart of the matter boils down to pinpointing the optimal algorithm or model capable of best classifying non-wear time on out-of-distribution data.

## Limitations of Current ML Models to Detect Sleep

Although machine learning and deep learning have been applied to accelerometer data with the goal of predicting sleep for over a decade, the field is still in its infancy. Primarily, most methods have been tailored to integrate with data sourced from wrist- and hip-worn devices[@conley_agreement_2019]. The use of machine learning and deep learning on data collected via devices mounted to the thigh remains unexplored despite the potential advantages of this sensor location in estimating physical activity behaviors[@brønd_2020]. Only a few studies have leveraged this sensor-location for heuristic algorithms[@johansson_development_2023; @carlson_validity_2021; @inan-eroglu_comparison_2021; @van_der_berg_identifying_2016 @winkler_identifying_2016]. One must wonder how this skewed focus impacts the adaptability and performance of these models in diverse real-world scenarios. For example, specific sleep-related behaviors or positions might be better captured by thigh-mounted devices compared to their wrist or hip counterparts. Not leveraging this potential data source might lead to overlooked nuances in sleep detection.

A significant hurdle in assessing sleep using accelerometer data is the extraction of the sleep period time window without supplementary data from sleep logs or diaries[@doherty_large_2017; @dasilva_2014; @anderson_assessment_2014]. This dependence on subjective sleep logs can introduce biases, and it inherently assumes participants consistently record accurate and precise timings. In reality, this can be a substantial point of failure, especially in long-term studies or with certain demographics who might be less consistent with logging such as children who often rely on their parents reporting accurate bed times on their behalf. Typical approaches that rely on traditional accelerometers often necessitate participants to diligently log their bedtime, sleep initiation, and wake-up moments[@girschik_validation_2012; @lockley_1999; @littner_2003]. This reliance is burdensome and can lead to incomplete or inaccurate datasets, compromising the integrity and reliability of any derived insights. Moreover, the data foundation used to develop these algorithms often consists of single-night PSG-recordings, like the random forests model by Sundarajan et al.[@sundararajan_sleep_2021]. While this may provide a snapshot of a participant's sleep behavior, it doesn't capture the night-to-night variability. Capturing intra-individual variances in sleep across multiple nights could lead to more robust and generalized models. Unfortunately, multiple nights of PSG recording can be impractical due to the intrusive nature of PSG, its cost, and the inconvenience for participants. Thus, alternative systems like the ZM could offer significant advantages, serving as a more feasible and less intrusive alternative to traditional PSG.

## Thesis Aim and Objectives

The rapidly growing field of wearable technology provides unprecedented opportunities to collect accurate and objective data on human behavior, particularly in the realm of free-living accelerometer recordings. This thesis situates itself within this developing domain, with the ambition of harnessing the potential of wearable accelerometer technology for sleep estimation. At its core, the objective is to innovate methods and models for the analysis and interpretation of sleep. Building on the challenges delineated in earlier sections, this thesis endeavors to advance the field through a series of papers, as detailed below.

The objectives of paper I is to present a method for manually annotating individual bedtime and wake-up times using raw, unprocessed accelerometry data. Furthermore, to validate the accuracy of these annotations by comparing them with findings from a single-channel EEG-based sleep staging system and a conventional sleep diary, and lastly, to assess both inter-rater and intra-rater reliability of the manual annotations.

Paper II lays down two primary objectives. Firstly, it seeks to evaluate decision tree models utilizing data from both thigh and hip-worn accelerometers to detect non-wear time in accelerometry data, also including the role of surface skin temperature. Secondly, it draws a comparison between machine-learned models and heuristic algorithms across accelerometer datasets sourced from devices worn on both the hip, thigh, and wrist.

In Paper III, the primary objective was to assess the performance of a selection of machine learning and deep learning models in estimating in-bed and sleep time, benchmarking all included methods against an EEG-based sleep tracking monitor. Beyond this, the secondary objective was to evaluate the ability of the developed models to quantify essential sleep quality metrics, once again validated against an EEG-based sleep tracking device.

# Paper I: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

This segment of the thesis encompasses the methods and results for Paper I. The study underscores the importance of effective machine learning algorithms for sleep/wake cycles, which ideally necessitate correct data annotations over a span of 7-10 days. Although sleep diaries or EEG recordings can annotate 'time in bed', many researches exclusively rely on accelerometry. This emphasizes the imperative for valid annotation techniques. Our objective is to introduce a manual annotation method, assess its precision, and determine its consistency. Some of the details presented here were previously mentioned in the published version of Paper I[@skovgaard_manual_2021].

## Methods

### Study Population

The data for this study was sourced from the SCREENS pilot trial (www.clinicaltrials.gov, NCT03788525), a two-arm parallel-group cluster-randomized trial with two intervention groups, conducted between October 2018 and March 2019[@rasmussen_feasibility_2021; @rasmussen_short-term_2020]. There was no control group in this trial.

Families from the Middelfart municipality in Denmark were approached for participation if they had a child aged between 6 to 10 years living with them, out of a total of 1686 families. To qualify, the parent's screen media usage had to exceed the median of 2.7 hours per day, based on survey responses from 394 respondents. Additionally, all children in the household needed to be older than 3.9 years to ensure that sleep measurements weren't disrupted by the nocturnal awakenings typical of infants or toddlers. For a comprehensive list of inclusion and exclusion criteria, refer to Pedersen et al.[@pedersen_self-administered_2021].

The present study ultimately included data from 14 children and 19 adults. These participants weren't advised to alter their sleep or bedtime routines for the interventions. While the study focused on nightly sleep time as recorded by the EEG-based sleep staging system, any napping behavior of the participants was deemed irrelevant.

All data collection procedures were reported to the local data protection department, SDU RIO (ID: 10.391), in compliance with the Danish Data Protection Agency's regulations.

### Accelerometry Recordings

Both adults and children participated in 24-hour accelerometry recordings using two triaxial accelerometers, Axivity AX3 (Axivity Ltd., Newcastle upon Tyne, UK). The Axivity AX3 is a compact device, measuring 23 mm × 32.5 mm × 7.6 mm and weighing just 11 g. It was set with a sensitivity of ±8 g and a sampling frequency of 50 Hz. Participants wore the accelerometers at two specific anatomical locations. The first was positioned on the right hip, secured in a pocket attached to a belt around the waist, ensuring the USB connector faced outward from the body's right side. The second accelerometer was placed midway between the hip and knee on the right thigh, housed in a pocket on a belt, with the USB connector also facing away from the body. For both the baseline and follow-up, the devices were worn for a duration of one week (seven consecutive days). This duration aligns with the recommended number of days to reliably assess habitual physical activity[@jaeschke_variability_2018].

### EEG-based and Self-Report Sleep Recordings

Both adults and children were assessed for their sleep patterns using the ZM model DT-200 (General Sleep Corporation, Cleveland, OH, USA), Firmware version 5.1.0. This assessment was concurrent with the accelerometer recordings. At the baseline, the sleep assessment using the ZM spanned 3--4 nights, while during the follow-up, it was conducted over 3 nights.

The ZM device operates by measuring sleep through a single-channel EEG, specifically from the differential mastoid (A1--A2) EEG location, evaluated on a 30-second epoch basis. Designed for use in everyday settings, the ZM provides an objective measurement of various sleep parameters, including sleep duration, sleep stage classification, and latency to different sleep stages. The ZM's algorithm has been benchmarked against PSG in laboratory settings for both adults with and without chronic sleep issues[@wang_evaluation_2015; @kaplan_performance_2014]. Our findings indicate that the ZM is effectively applicable to both children and adults for multi-day measurements in real-world settings[@pedersen_self-administered_2021]. Notably, the device showcased a high accuracy in distinguishing between sleep and wakefulness, with sensitivity, specificity, positive predictive value, and negative predictive values being 95.5%, 92.5%, 98%, and 84.2%, respectively[@kaplan_performance_2014].

For the assessment, three electrodes (Ambu A/S, Ballerup, Denmark, type: N-00-S/25) are positioned on the mastoids (for signal) and the nape (as ground). About half an hour before their intended sleep time, participants' skin areas are cleaned with alcohol swabs, after which the electrodes are affixed. An EEG cable connects these electrodes to the ZM device. A preliminary sensor check ensures all electrodes are correctly mounted; any issues are promptly addressed by replacing the problematic electrodes. Additionally, participants, or parents on behalf of their children, recorded their sleep and wake times daily in a dedicated diary.

### Annotation Software

Audacity®️ is a distinguished free audio editing software[@audacity]. The genesis of Audacity can be traced back to the fall of 1999, when it emerged as an innovative project led by Dominic Mazzoni and Roger Dannenberg at Carnegie Mellon University. By May 2000, it was unveiled to the world as an open-source audio editor. Since its inception, Audacity has undergone significant evolution. The software, developed collaboratively by the community, now boasts of hundreds of unique features, offers complete support for professional-grade 24-bit and 32-bit audio, has a comprehensive manual available in multiple languages, and has witnessed distribution in the millions. Today, a dedicated team of volunteers from various corners of the globe continues to maintain and enhance Audacity. It is disseminated under the GNU General Public License, granting everyone the freedom to utilize the software for personal, educational, or commercial endeavors.

Audacity is not limited to audio processing; it can also serve as a tool for accelerometer data analysis. This software provides researchers with the means to precisely inspect high-resolution raw accelerometer data in great detail. Users can quickly zoom in to explore deeper into specific segments of the recording, like certain patterns around bedtime, or zoom out for a broader perspective, such as data spanning a week. Furthermore, Audacity's sophisticated labeling function is pivotal for annotating the accelerometry data. Any labels created can be saved in a separate file and subsequently incorporated into the training phase of machine learning algorithms. The depth of manual inspection for high-resolution accelerometer data that Audacity provides is, to our knowledge, matched by only a few other software options[@visplore; @label_studio].

Within the Audacity interface, there's the possibility of amalgamating over 100 channels of data. This aids in the merging of distinct signal features derived from acceleration. The integration of multiple signal features is intriguing as it might enhance the visual comprehension and classification of inherent behaviors. Nevertheless, an excessive conglomeration of signal features might obscure the precise identification of targeted behaviors. For our study, we incorporated a total of seven distinct signal features. The criteria for classifying "lying" in the first feature are explicit: if the inclination of the hip accelerometer surpasses 65 degrees and the thigh accelerometer simultaneously identifies as "sitting" based on Skotte et al.'s activity type classification algorithm[@skotte_detection_2014]. The other signal features, barring "time", are directly procured from Skotte et al.'s algorithm. These features, delineated in @tbl-man_signal_features, concern the longitudinal axis of the body. Data derived from accelerometry undergoes processing using a window length of two seconds (60 samples) and has a 50% overlap (30 samples), ensuring a resolution of one second. The methodologies from Skotte et al. and those generating the first feature rely exclusively on the accelerometer's inclination(s). Hence, while they can determine time in bed and participant's posture, they aren't precise indicators for pinpointing exact in-bed and out-of-bed moments.

To provide a visual perspective, @fig-screen_full and @fig-screen_night depict the Audacity interface displaying all seven signal features as cataloged in @tbl-man_signal_features. @fig-screen_full offers a glimpse of a week's data, whereas @fig-screen_night zooms into an approximate 24-hour span, showcasing a single annotated night.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_signal_features
#| tbl-cap: Summary of the specific signal features utilized in Audacity for the accurate detection and analysis of in-bed periods.
#| tbl-cap-location: bottom

source("code/tables.R")

tbl_signal_features %>% as_latex()
```

```{=tex}
\endgroup
```
![Screenshot of the Audacity interface showing the seven horizontal panels representing the included signal features. See @tbl-man_signal_features for a detailed description of the features.](figures/audacity_full_view.png){#fig-screen_full}

![Screenshot of the Audacity interface when zoomed in on a single night for the labeling of the in-bed period. The seven horizontal panels represent the included signal features. See @tbl-man_signal_features for a detailed description of features.](figures/audacity_single_night.png){#fig-screen_night}

### Annotation Process

Three experienced researchers, well-versed in working with accelerometer data, were chosen as raters. Their proficiency ensured that they had the requisite knowledge to accurately interpret the various data channels presented to them. Each rater meticulously reviewed and labeled each wav-file, marking specific timestamps that indicated in-bed and out-of-bed activities. These annotations were then saved as individual text files. For ensuring consistency and reliability in the annotations, each wav-file underwent two rounds of labeling. Importantly, at no point during this process were the raters aware of any prior annotations, either made by themselves or their colleagues. This approach was adopted to prevent any potential biases and ensure the highest degree of objectivity in the annotations.

### Establishing the Ground Truth

The definitive ground truth for in-bed and out-of-bed time frames was obtained from the sleep staging data derived from the ZM device. This was established by identifying the first and last events at night that did not present any sensor-related issues. Nights where the ZM detected sensor problems, either at the onset or conclusion of the recording, were excluded from further consideration. Such sensor issues typically arise due to high impedance because of inadequate attachment of electrodes. To maintain accuracy in data collection, all participants were meticulously instructed to affix the ZM before bedtime and then activate it precisely at their bedtime and to detach it upon waking. These timestamps were then utilized as the ground truth for the study.

### Statistics

For continuous variables, the descriptive characteristics were computed using medians and interquartile ranges. Meanwhile, categorical variables were assessed based on their proportions. To offer a clear distinction, the characteristics for children and adults were presented separately.

Our statistical approach was geared towards discerning the degree of agreement. To achieve this, we employed the intraclass correlation coefficient (ICC) and the Bland--Altman analysis. Recognizing that the human raters were sampled from a broader population, we used a two-way random-effects model when assessing inter-rater reliability between different human raters. Here, ICC(2,*k*) was chosen, reflecting the absolute agreement between multiple rater's ratings[@shrout_1979]. However, when comparing human raters against the ZM or sleep diaries, a two-way mixed-effects model was used. In this context, the human raters are treated as random effects, while the ZM and sleep diaries are treated as fixed effects. The corresponding ICC that represents this model is known as ICC(3,*k*)[@shrout_1979]. For intra-rater agreement, a two-way mixed effects models was employed treating human raters as random effects and occasion (test/retest) as fixed effects. We adopted the ICC(3,*k*) to estimate the agreement of each individual rater's ratings across occasions[@mcgraw_1996]. In general, the ICC serves as a more nuanced tool than simple correlation; it goes further by evaluating the alignment in magnitude between two datasets, serving as a robust metric for consistency between methodologies. The interpretation of ICC values were as follows:

$ICC < 0.5$ indicates poor agreement

$0.5 ≤ ICC < 0.75$ indicates moderate agreement

$0.75 ≤ ICC < 0.9$ indicates good agreement

$ICC ≥ 0.90$ indicates excellent agreement

In this research, the ICC values are presented as ICC (95% CI) and is interpreted based on their 95% confidence intervals, for example, a CI of 0.83-0.94 indicates "good" to "excellent" agreement, while a CI of 0.92-0.99 is solely "excellent" as even the lowest value surpasses 0.9. By doing so, we adhere to recommended guidelines as presented by Koo et al.[@koo_guideline_2016] and was calculated using the R package psych[@psych]. The Bland--Altman analysis, on the other hand, is a tool to measure the concurrence between two measuring techniques[@bland_measuring_1999]. It calculates the average of the differences (representing bias) between the two methods, and also establishes the limits of this agreement. A positive mean difference suggests an earlier underestimation of the in-bed or out-of-bed timestamp relative to the ZM, while a negative difference indicates a later overestimation. To complement these metrics and to offer a visual perspective on the congruence between the methods, we incorporated probability density distribution plots. These plots visualize the distribution of agreement and underscore the symmetry between the methodologies. All statistical analyses for this study were performed using the R statistical software (version 4.0.2, released on 22 June 2020) and RStudio (version 1.1.456).

## Results

Descriptive characteristics of the included subjects of the current study are reported in @tbl-man_describe.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_describe
#| tbl-cap: "Descriptive characteristics of the study participants. ISCE: International Standard Classification of Education"

tbl_man_describe
```

```{=tex}
\endgroup
```
### Intraclass Correlation Coefficient Analyses

The ICCs analysis, as displayed in @tbl-man_icc_zm_man, showed excellent agreement between the ZM and the averaged manual annotations made by the three human raters across all comparisons. ICC values of the baseline comparisons (covering 94 nights) were consistently 0.98 with the lower limit of the 95% confidence intercal only dipped as low as 0.96 indicating excellent agreement. Similarly, all follow-up comparisons (encompassing 54 nights) showed ICCs above 0.95 with the lowest 95% confidence interval scoring 0.92 for the second round of "to-bed" annotations ensuring excellent agreement.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_man
#| tbl-cap: Intraclass correlation coefficients between the ZM and the three human raters. Values are ICC (95% CI).

tbl_icc_zm_man
```

```{=tex}
\endgroup
```
The weakest agreement between self-report- and ZM-determined in-bed periods were observed for the "to-bed" timestamp on the follow-up data yielding a lower limit of the 95% confidence interval of 0.94 still indicating excellent absolute agreement (see @tbl-man_icc_zm_self).

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_self
#| tbl-cap: Intraclass correlation coefficients between self-report and the ZM. Values are ICC (95% CI).

tbl_icc_self_zm
```

```{=tex}
\endgroup
```
Assessing the agreement between the three human raters when annotating timestamps for 'to bed' and 'out of bed' events, the ICC values reflected good to excellent agreement between the raters across both rounds and timestamps. Specifically, the lower bounds of the 95% confidence intervals dipped below 0.9 (ICC \> 0.9 indicating excellent agreement) for round 1 and round 2 "to-bed" on the baseline data, with the least value being 0.88 (see @tbl-man_icc_man_man).

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_man_man
#| tbl-cap: Intraclass correlation coefficients between the three human raters. Values are ICC (95% CI).

tbl_icc_man_man
```

```{=tex}
\endgroup
```
Across baseline and follow-up and on both to-bed and out-of-bed timestamps, each rater displayed good to excellent test-retest agreement, with the lower limits of the 95% confidence interval of hte ICCs values ranging from 0.86 to 0.99. Notably, while Raters 1 and 3 demonstrated a minor dip in their baseline to-bed agreement compared to out-of-bed measures, Rater 2 showed lower agreement on the follow-up to-bed timestamp compared to out-of-bed (refer to @tbl-man_icc_test_retest).

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_test_retest
#| tbl-cap: Test–retest intraclass correlation coefficients between the first and second round of manual annotations. Values are ICC (95% CI).

tbl_icc_test_retest
```

```{=tex}
\endgroup
```
### Bland-Altman Analyses

@tbl-7 outlines the Bland--Altman analyses comparing both human raters and self-report against the ZM in-bed timestamps. The bias observed for human raters against the ZM fluctuates between -6 minutes to 5 minutes, suggesting a relatively narrow range of mean differences across evaluations. Comparatively, the self-report's bias against ZM is slightly more constrained. The ranges of the limits of agreement remained fairly consistent irrespective of the method being contrasted with ZM at -43.81 to -21.3 minutes for the lower LOAs and 20.87 to 35.85 minutes for the upper LOAs. This uniformity in limits of agreement suggest similar agreements of both manual annotations and self-reports when compared with the ZM in-bed periods.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-7
#| tbl-cap: Bland–Altman analysis was conducted to assess the inter-method agreement. This analysis compared manual annotation to ZM and also compared self-report to ZM. All the measurements in the analysis are presented in minutes.

tbl_7

```

```{=tex}
\endgroup
```
### Density Plots

@fig-ridge_plot presents the probability density distribution of the differences between the "to-bed" and "out-of-bed" timestamps, comparing manual annotations and self-reports to the ZM. These plots offer a visual illustration of the bias and spread around zero, showcasing how manual annotations and self-reports diverge from ZM, as previously highlighted[@van_hees_estimating_2018].

![Probability density distributions for differences between manual in-bed annotations and self-report compared to ZM.](figures/paper1_ridge_plot.pdf){#fig-ridge_plot}

# Paper II: Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings

This segment of the thesis encompasses the methods and results for Paper II. Despite advancements in sensor technology and software development, the accurate classification of non-wear time in raw accelerometer data remains a challenge. This raises a pivotal question: which heuristic algorithm or machine-learning model can best classify non-wear time when analyzing unseen accelerometer data? To address this, three datasets were generated, each comprising raw accelerometer data manually annotated for wear and non-wear times and inclusive of surface skin temperature measurements. This study specifically aimed to train three decision tree models using data from thigh and hip-worn accelerometers to classify non-wear time. Furthermore, the importance of surface skin temperature was evaluated, and the potential advantages of limiting the number of features in the model were explored. Lastly, the comparative performance of the developed decision tree models against basic heuristic algorithms and recently developed random forest and convolutional neural network models was assessed. Some of the details presented here were previously mentioned in the published version of Paper II[@skovgaard_generalizability_2023].

## Methods

### Reference Methods

A total of four additional non-wear classification methods were incorporated to assess generalizability and contrast their performance with the three newly developed decision tree models. These pre-existing methods were deliberately chosen to encompass a range of methodological flexibility. This selection ensured representation from the simplest and most commonly used techniques to the latest and most complex ones.

1.  \textsf{\textbf{Consecutive Zeros-Algorithm (cz\_60):}} Over the years, there have been various consecutive zero-algorithms designed for accelerometer data, with the aim of identifying non-wear periods within predefined timeframes, such as 30-, 60-, or 90-minute intervals[@hecht_methodology_2009; @troiano_physical_2008; @choi_validation_2011]. In research by van Hees et al.[@van_hees_estimation_2011], the potential of a simple summary measure derived from raw triaxial accelerometer data to aid in the estimation of PA-related energy expenditure in both pregnant and non-pregnant women was explored. The study involved 108 women from Sweden and 99 women from the United Kingdom who wore a triaxial GENEA accelerometer for durations of 10 and 7 days, respectively. The researchers developed an algorithm to discern wear and non-wear time, basing their estimates on the standard deviation and value range of each accelerometer axis over 30-minute intervals. Intervals were designated as non-wear time if the standard deviation was below 3.0 mg (1 mg = 0.00981 m⋅s−2) for at least two of the three axes, or if the value range was under 50 mg for at least two of the three axes. In a subsequent study by van Hees et al.[@hees_separating_2013], the interval length was extended to 60 minutes to reduce the likelihood of misidentifying sedentary periods as non-wear time. Furthermore, a 15-minute sliding window was introduced to account for overlapping episodes and to pinpoint non-wear episode boundaries more accurately. Another method utilizes a 135-minute interval with adjusted hyperparameters, as introduced by Syed et al.[@syed_evaluating_2020]. In our study, we adopted a straightforward approach to this concept. Using Actigraphy counts, we identified periods of no movement that registered zero counts for at least 60 continuous minutes. Notably, these Actigraphy counts operate with a deadband set at 68 mg, which denotes the minimum detectable acceleration threshold.

2.  \textsf{\textbf{Heuristic Algorithm (heu\_alg):}} As detailed by Rasmussen and colleagues[@rasmussen_short-term_2020], this algorithm merges raw acceleration data with surface skin temperature measurements. Non-wear time is determined for periods surpassing 120 minutes with accelerations less than 20 mg. For durations between 45 to 120 minutes, non-wear is identified if the temperature falls below a personalized non-moving temperature threshold. Additionally, the algorithm can spot non-wear periods ranging from 10 to 45 minutes, but only if these intervals end within the anticipated awake hours (06:00 AM to 10:00 PM).

3.  \textsf{\textbf{Random Forests Model (sunda\_RF):}} Sundararajan et al.[@sundararajan_sleep_2021] delineated a non-wear classification technique grounded in a random forest ensemble model. This model was informed by raw accelerometer data derived from 134 participants aged between 20 to 70 years. These subjects were fitted with an accelerometer on their wrist for a singular overnight PSG recording session. The ground truth labels for non-wear periods were anchored in the assumption that the accelerometer was always worn during the PSG recording. Any epoch with a standard deviation in the acceleration signal exceeding 13.0 mg outside the PSG recording time period was classified as wear time. The model utilized 36 predictors, and a nested cross-validation method was employed both to ascertain the model's generalization capability and to tune its hyperparameters.

4.  \textsf{\textbf{Deep Convolutional Neural Network (syed\_CNN):}} This method, introduced by Syed et al.[@syed_novel_2021], employs a unique approach. It's built upon a deep convolutional neural network that diverges from traditional techniques. Initially, all potential non-wear episodes are discerned using a standard deviation threshold. However, instead of examining the acceleration within these intervals, the focus shifts to the signal shape of the raw acceleration immediately before and after a non-wear episode. Through the convolution neural network, the method discerns non-wear periods by detecting the moments when the accelerometer is removed and reattached. For our study's purposes, we chose a window length of 10 seconds on each side of the identified non-wear episode, as this yielded the most accurate results. The training dataset that informed the CNN consisted of data from hip-mounted accelerometers worn by 583 participants. These individuals ranged in age from 40 to 84 years, with an average age of 63 years and a standard deviation of 10.

### Data Sources and Validation Schema

Data for the current study were sourced from the PHASAR study and an in-house validation study with both studies utilizing the Axivity AX3 accelerometer (Axivity Ltd., Newcastle upon Tyne, UK) to record raw acceleration data along with surface skin temperature. The device, weighing a mere 11 g and with dimensions of 23 mm × 32.5 mm × 7.6 mm, measures acceleration in gravity units (g) across three axes (vertical, mediolateral, and anteroposterior). The sampling frequencies were set at 50 Hz for the PHASAR study and 25 Hz for the in-house study. However, all recorded data from both studies were uniformly resampled to 30 Hz.

The PHASAR study involved a representative sample of over 2000 school-aged children from 31 public schools in Denmark. The study, conducted between 2017 and 2018, captured data from 1,315 boys (49%) and 1,358 girls (51%), aged between 8.1 to 17.9 years (mean age = 12.14, SD = 2.40). Accelerometers were placed at two specific anatomical sites: the right hip and midway on the right thigh. They were worn for a recommended seven consecutive days to reliably estimate habitual physical activity. For this analysis, data from 64 randomly selected participants from the PHASAR cohort were used. A dataset indicating genuine non-wear time was created via manual annotation, a method elaborated in another publication. Essentially, non-wear periods were determined by visually examining raw accelerations coupled with skin temperature readings. True non-wear episodes with specific start and end times were manually labelled in each dataset and were utilized as reference labels in subsequent analyses.

The in-house validation study consisted of accelerometer data from 42 youth athletes, evenly split between boys and girls, aged 14.5 to 16.4 years (mean age = 15.4, SD = 0.37 years). These athletes, part of a specialized talent program in the Region of Southern Denmark, wore the Axivity accelerometer on their non-dominant wrist for 14 consecutive days. This study was initiated in the spring of 2021. A dataset mirroring the one from the PHASAR study was created, including all 42 participants.

The PHASAR study was reviewed by the Regional Committee on Health Research Ethics for Southern Denmark (ID: S-20170031) and was determined not to require an ethics review, as per Danish regulations, which mandate only biomedical research or risk-involved studies to undergo a formal ethics review. Documentation regarding this decision is available upon request from the principal author. Conversely, the in-house validation study received an ethical approval waiver from the Research & Innovation Organization and the legal department of the University of Southern Denmark. All participants, or their legal guardians, provided written informed consent for both studies, which adhered to the Danish Data Protection Agency (2015-57-0008) standards and globally recognized guidelines like the Declaration of Helsinki.

![The flowchart depicts the division of the PHASAR dataset into training and testing segments. On the left, boxes signify 79.2% of the PHASAR data designated for training across five-fold resamples. On the right, the yellow and blue boxes collectively represent 20.2% of the PHASAR data, specifically delineating the hip and thigh data for testing. The green box represents our separate in-house test dataset, which was gathered from wrist-worn devices.](figures/paper2_flowchart.pdf){#fig-paper2_flowchart}

The decision tree models were trained on data from devices worn on the hip and thigh. In contrast, the random forest model was built using data from wrist-worn accelerometers, and a convolutional neural network (CNN) was trained on hip-worn device data. To assess the models' generalizability, each underwent external validation. This involved testing them on datasets representing wear locations different from where they were initially trained, enabling a comprehensive assessment of their generalizability across varied anatomical positions and age spans (see @fig-paper2_flowchart).

### Development of Decision Tree Models

For our decision tree models, we sourced 12 predictors from the raw PHASAR accelerometer data, which encompassed elements like temperature, time of day, indicators for device placement, day of the week, and moving average statistics (detailed in @tbl-8). These moving average metrics were collated in 10-second increments. To train the model, we utilized 79.2% of the PHASAR data, incorporating data from both hip- and thigh-worn devices (as shown in @fig-paper2_flowchart). We made certain that data from individual participants was exclusively allocated to either the training or test datasets. This strategy was crucial in ensuring that the model could effectively generalize to unfamiliar data, rather than overfitting to specific participant data. During the tuning phase, to boost model accuracy and avoid overfitting, we opted for a five-fold cross-validation approach. This process entailed refining several hyperparameters, such as the tree's depth, its cost-complexity, and the minimum amount of data points necessary in a node for it to split further. To effectively explore the hyperparameter space, we employed Latin hypercube sampling. This method systematically divides the parameter range into segments, randomly drawing a value from each segment, resulting in a well-distributed set of parameter combinations. In our case, we established a 10-level parameter grid, guaranteeing a comprehensive exploration of the hyperparameter space. The F1 score was treated as the optimization metric.

Following this procedure, we introduced three distinct model variations:

1.  A full-scope model (*tree_full*) incorporating every predictor.

2.  A refined model (*tree_imp6*) centered on the six most crucial predictors, as established by permutation predictor importance (@fig-importance).

3.  A model excluding surface skin temperature data (*tree_no_temp*).

In sum, our methodology generated 50 distinct models for each decision tree variant. Given our data's distribution - 55.8% wear time compared to 44.2% non-wear time - there was no need to adopt synthetic minority oversampling methods like SMOTE or other balancing techniques.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-8
#| tbl-cap: Predictors derived from the raw sensor signals.

tbl_8

```

```{=tex}
\endgroup
```
![Permutation importance plot depicting the significance of predictors in the decision tree. The top six predictors informed the tree_imp6 model, while a third model, tree_no_temp, was trained using all predictors except temperature.](figures/paper2_vip.pdf){#fig-importance}

### Statistics

Classification performance was evaluated against a ground truth test dataset, encompassing over 7 million epochs of 10 seconds each, derived from 104 unique subjects. Accurate identification of true non-wear time and true wear time yielded true positives (TP) and true negatives (TN), respectively. These correct classifications are essential for ensuring the algorithm's precision in determining non-wear time. Conversely, misclassifications, where true non-wear time is identified as wear time or vice versa, contributed to false negatives (FN) and false positives (FP). By analyzing these 10-second acceleration data intervals against the ground truth, a confusion matrix was constructed. From this matrix, the following performance metrics were extracted:

Overall accuracy: $\frac{TP+TN}{TP+TN+FP+FN}$

Sensitivity: $\frac{TP}{TP+FN}​$

Precision: $\frac{TP}{TP+FP}$

​F1-score: $\frac{2 \cdot TP}{2 \cdot TP+FP+FN}​$

The F1-score, representing the harmonic mean of precision and sensitivity, serves as a robust metric for classification performance. Higher F1-scores signify good classification efficacy. Furthermore, permutation feature importance was employed to assess the performance of the features of the decision tree models. This method determines the significance of each feature by assessing how model accuracy diminishes when that feature's data is randomized. A significant drop in performance suggests role of that particular feature.

For all analyses and model development, we utilized R (version 4.1.2, Bird Hippie) and RStudio (version 2021.9.1.372, Ghost Orchid). The machine learning tasks were primarily facilitated by the Tidymodels[@kuhn_tidymodels_2020] suite of packages, and we used the rpart[@rpart] package as the engine for our decision tree algorithms.

## Results

In our datasets spanning three wear locations, there were 1,598 non-wear time episodes. Of these, 1,148 episodes (or 71.8%) lasted 60 minutes or more, with an average duration of about 13 hours (794 minutes with a standard deviation of 1,142 minutes). In contrast, episodes lasting 60 minutes or less made up 28.2% (450 episodes) with an average duration of 26.4 minutes (SD = 16.4). Interestingly, the briefest episodes (less than 60 minutes) made up just 1.3% of the total non-wear time across all wear locations (refer to @tbl-9). @fig-paper2_nw_dists depicts the frequency distribution for episodes shorter than 60 minutes and those 60 minutes or longer. The PHASAR dataset (hip and thigh) showed a bimodal distribution for shorter episodes, with longer episodes peaking around 10 hours. For the in-house wrist-worn dataset, shorter episodes displayed a uniform distribution, while longer episodes were significantly right-skewed.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-9
#| tbl-cap: Overview of non-wear episodes grouped in short and long non-wear episodes, min = minutes, hrs = hours.

tbl_9
```

```{=tex}
\endgroup
```
![Distribution of the length of the non-wear episodes across hip, thigh, and wrist data. Distributions are shown for episodes shorter than 60 min (binwidth = 1 minute) and longer than 60 min (binwidth = 1 hour).](figures/paper2_plot_nw_dists.pdf){#fig-paper2_nw_dists}

### Classification Performance

In assessing classification performance, @fig-paper2_preds_ex visually contrasts the results from machine-learned models and rule-based algorithms against the ground truth non-wear time, which is highlighted with a light grey background. This visualization underscores that while tree-based models tend to be precise, they can also be unpredictable. On the other hand, threshold-based methods, such as heu_alg and cz_60, offer more consistency. Notably, both cz_60 and heu_alg algorithms fall short in identifying shorter non-wear episodes.

![Visual example of the output of non-wear detection models and algorithms for a random person from the in-house wrist dataset (14 consecutive days). The grey shade is ground-truth non-wear time. Syed_CNN, cz_60, and tree_full are vertically offset for easier interpretation.](figures/paper2_plot_preds_example.pdf){#fig-paper2_preds_ex}

@fig-paper2_performance_all compiles performance metrics from all methods evaluated in this study on all non-wear periods. The tree_full, tree_imp6, cv_60, and heu_alg all achieved similar performance, achieving an accuracy and F1 scores above 90% on both the thigh, hip, and wrist data. These methods showcased a uniform performance across wear location with exception of the three_full model eliciting a slight drop in performance on the wrist data. On the other hand, some models demonstrated varied performances when transitioning from one dataset to another. For instance, the tree_no_temp and the Syed_CNN model's F1 score on the hip and thigh datasets hovered around 65% and 68%, respectively, but when applied to the wrist dataset, there was a noticeable dip to 47% and 59%, respectively. The sunda_RF model, however, showed an upward trajectory, moving from an accuracy of approximately 57% on the hip and thigh datasets to a significantly improved 93% on the wrist dataset.

Further, @fig-paper2_performance_short zeroes in on performance metrics for episodes 60 minutes or shorter. The consecutive zeros algorithm was unable to detect any non-wear, a result absent from the figure. Syed et al.'s deep learning model underperformed, detecting a mere 1--2% of all non-wear time, leading to F1 scores below 5%. Although the heu_alg algorithm boasted high precision, its lackluster sensitivity resulted in F1 scores spanning from 12% to 16% across wear locations. The random forest model displayed average results for thigh and wrist data but faltered with hip data, recording F1 scores of 46%, 57%, and 8%, respectively. Among the trio of decision tree models, the one leveraging the six most important features outshone the rest, with F1 scores between 72% and 79%. Meanwhile, the decision tree model encompassing all predictors faced challenges with hip data due to a 23% sensitivity score. The decision tree model excluding the surface skin temperature exhibited commendable precision; however, its low sensitivity culminated in F1 scores ranging from 45% to 57%.

![Classification performance metrics on all non-wear episodes for the seven included methods for classifying non-wear time. Metrics are shown for the three different ground-truth dataset including hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_all.pdf){#fig-paper2_performance_all}

![Classification performance for episodes no longer than 60 min in length. Metrics are shown for the three different gold-standard dataset including hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_short.pdf){#fig-paper2_performance_short}

# Paper III: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

This segment of the thesis encompasses the methods and results for Paper III. Polysomnography, the premier method for sleep evaluation, is not always feasible for extensive research due to its high costs and impracticality. Wearable accelerometers present an affordable solution. While wrist and hip-worn devices dominate sleep studies, the potential of thigh-worn accelerometers remains largely untapped. This paper delves into the use of machine learning and deep learning models that leverage data from thigh-worn accelerometers to estimate sleep and its quality by comparing these models with an EEG-based sleep monitor. The primary aim was to assess various machine and deep learning models designed to estimate in-bed and sleep time using raw data from a tri-axial, thigh-worn accelerometer. For a robust validation, the outcomes of these models were compared with results from ZM, which served as our gold standard for sleep assessment in this study. Additionally, a secondary objective was to assess the efficacy of these models in determining key sleep quality metrics, including sleep period time (SPT), total sleep time (TST), sleep efficiency (SE), latency until persistent sleep (LPS), and wake after sleep onset (WASO).

## Methods

### Dataset and Participants

The current study uses data from the SCREENS trial, which took place from June 2019 to March 2021 in the Region of Southern Denmark. Conducted by Rasmussen and Pedersen[@rasmussen_short-term_2020; @pedersen_effects_2022], the trial aimed to evaluate the impact of limiting screen media usage among Danish families. We specifically analyzed data from children between the ages of 4 and 17, with a mean age of 9.1 years, who were part of the SCREENS cohort. To gather our primary data, we utilized accelerometer readings from Axivity AX3 devices attached to the children's thighs and sleep metrics derived from EEG readings using the ZM device by General Sleep Corporation.

The Axivity AX3 is a 3-axis accelerometer positioned midway between the hip and knee on the right anterior thigh. This device records movement data with a 50 Hz sampling frequenzy, and it is unobtrusive, allowing for natural behavior from the children. On the other hand, the ZM device uses advanced EEG hardware and signal processing algorithms to gather sleep data into epochs of 30 seconds in duration. It features three self-adhesive, disposable sensors placed outside the hairline, ensuring reliable EEG signal acquisition. Participants were instructed to attach the device at bedtime and remove it upon leaving bed. To process the EEG data, the ZM device uses two proprietary algorithms, Z-ALG and Z-PLUS. The former is known for its accurate sleep detection capabilities as supported by Kaplan et al.[@kaplan_performance_2014], while the latter differentiates between sleep stages and aligns well with expert evaluations using PSG data, as demonstrated by Wang et al.[@wang_evaluation_2015]. As stated earlier in this thesis, the ZM has shown high complance among both children and adults for in-home monitoring[@pedersen_self-administered_2021].

@fig-paper3_flow outlines the selection criteria for children's recordings from the SCREENS study. Only recordings from ZM with complete accompanying accelerometer data and durations between 7 and 14 hours were considered. Any nights where ZM indicated sensor issues were discarded. This left us with a total of 585 nights from 151 children, averaging 3.87 nights per child (SD = 1.86). The age of these children averaged at 9.4 years with a standard deviation of 2.1 years. Across these recordings, ZM predictions spanned 696,779 epochs, each lasting 30 seconds. Significantly, around 84% of the overall ZM recording time was classified as sleep, indicating a class imbalance in the nightly recordings.

Lastly, we affirm that the SCREENS trials adhered to ethical guidelines, receiving approval from the Regional Scientific Committee of Southern Denmark. All data handling processes were in compliance with the General Data Protection Regulation (GDPR), ensuring the secure and ethical management of participant information.

![Flowchart of eligible ZM recording nights included in the study.](figures/paper3_flowchart.pdf){#fig-paper3_flow}

### Data preprocessing and Feature Extraction

In this study, we began by processing raw accelerometer data through a low-pass filtration step, utilizing a 4th order Butterworth filter with a 5 Hz cut-off frequency to remove high-frequency noise as described by Skotte and colleagues[@skotte_detection_2014]. Non-wear data was identified and eliminated using the decision tree classifier, tree_imp6, as outlined in Paper II[@skovgaard_generalizability_2023], and the remaining data was resampled into 30-second epochs to align with ZM recordings. We then conducted feature extraction, generating 64 features that offered a comprehensive characterization of the data. These features were derived from both accelerometer and temperature signals and included temporal elements, which utilized both lag and lead values to capture dynamic data trends. Additionally, we took inspiration from Walch et al.[@walch_sleep_2019] to include sensor-independent features that encapsulate circadian rhythms, offering unique insights that are not directly discernible from sensor outputs (see @fig-paper3_sensor_independent). We further enriched the feature set by incorporating signal characteristics such as vector magnitude, mean crossing rate, skewness, and kurtosis for each of the x, y, and z dimensions. All features are summarized in @tbl-features. The ZM recordings and the corresponding accelerometer data were then merged. Any time overlap between these two sets of data was categorized as 'in-bed' time, while the remaining time was considered 'out-of-bed.' This process yielded a comprehensive dataset that provided a 24-hour view of each participant's activity and sleep patterns, with a target feature which consisted of three classes of interest; "out of bed awake", "in bed awake", and "in bed asleep".

![Sensor-independent features of circadian rhythms across two consecutive nights. A) cosinus feature, B) linear feature.](/home/esbenlykke/projects/thesis/figures/paper3_sensor_independent.pdf){#fig-paper3_sensor_independent}

```{=tex}
\begingroup
```
\footnotesize

| Feature Category          | Count  | Summary of Features                                              |
|---------------------------|--------------------|-------------------------|
| Misc Features             | 3      | age, weekday, vector_magnitude                                   |
| Inclination & Orientation | 2      | incl, theta                                                      |
| Signal Means              | 4      | mean for x, y, and z                                             |
| Signal SDs                | 8      | SDs for temp, x, y, and z (30-sec and 15-min windows) and sd_max |
| Clock Proxies             | 2      | clock_proxy_cos, clock_proxy_linear                              |
| Time-Dependent            | 36     | 1-, 5-, and 10-minute lag and lead features                      |
| Crossing Rates            | 3      | mean crossing rate for x, y, and z                               |
| Signal Distributions      | 6      | kurtosis and skewness for x, y, and z                            |
| **Total**                 | **64** |                                                                  |

: Features included in the models aggregated in 30-second epochs. {#tbl-features tbl-colwidths="\[30,10,60\]"}

```{=tex}
\endgroup
```
Upon examining the raw ZM predictions, we observed that the device appeared to overestimate the number of awakenings among the children studied. Although the ZM software addresses many of these awakenings by counting only three consecutive awake epochs towards wake time, this approach renders the raw predictions less suitable as training data for machine learning algorithms. In fact, many of these awakenings, labeled by the ZM, would be more aptly described as arousals rather than actual awakenings. Separately, the ZM device's sleep efficiency rating for our sample was 83%, which is below recognized standards. An efficiency of 85% is considered good, and over 90% is seen as ideal. This contrasts with prior research on similar child cohorts that reported a sleep efficiency of 88.3%[@galland_2018]. Recommendations from an expert panel by the National Sleep Foundation emphasized that fewer than 2 awakenings lasting more than 5 minutes each night qualify as good sleep across all age groups[@ohayon_2017]. Additionally, it's widely recognized that children typically experience between five to eight sleep cycles every night, with awakenings most likely occurring at the conclusion of each cycle[@galland_normal_2012]. However, definitions of a "waking bout" vary across studies. Some demand at least 5 continuous minutes of wakefulness for it to be counted as one bout, while others find a 1-minute duration adequate. Of particular note, the vast majority short arousal epochs labeled as awake by the ZM did not show any relevant responses in the accelerometer signal as inferred by visual examination. This misalignment might distort underlying patterns for machine learning algorithms. While this might not be outright mislabeling, categorizing all such epochs as true awakenings would introduce noise, jeopardizing model accuracy. In light of these observations, we opted to process both the raw ZM output and versions with 5-minute and 10-minute median filtering for our model training and evaluation. This approach minimized noise and offered an awakening count more aligned with typical patterns in children's sleep (see @tbl-10 and @fig-paper3_raw_filt for details).

![The difference in number of awakenings between the raw ZM predictions vs. 5-minute, and 10-minute median filtered predictions for a random night (boy, 9 years). Grey line is the raw predictions, black line is the median filtered predictions. A: 5-minute median filter on raw ZM predictions, B: 10-minute median filter on raw ZM predictions.](/home/esbenlykke/projects/thesis/figures/paper3_zm_raw_vs_filtered.pdf){#fig-paper3_raw_filt}

### Algorithms and Modelling Strategies

We utilized two distinct model strategies to analyze sleep patterns from accelerometer data mounted on the thigh. The first model strategy was designed as a sequence of two models, each functioning as a binary classifier. This method aimed to simplify the prediction task by breaking down the multiclass problem of distinguishing between 'out-of-bed-awake', 'in-bed-awake', and 'in-bed-asleep' into two binary phases: first identifying 'in-bed' periods, followed by determining 'sleep' periods. The output from the first binary classifiers in sequence, which estimated in-bed time, underwent a 5-minute median filter to eliminate blips of in-bed time. This step allowed us to define a singular continuous interval recognized as the SPT, representing the total duration spent in bed attempting to sleep. This SPT then acted as input to the subsequent stage of binary classifiers predicting sleep within the SPT. Four machine learning algorithms were applied in this sequential strategy:

1.  Logistic Regression: Logistic regression served as a simple and fast baseline model. However, due to its linear nature, it may struggle with capturing complex relationships and non-linear patterns present in the accelerometer data.

2.  Decision Tree: Decision trees are capable of handling non-linear patterns and are easily interpretable. However, they are prone to overfitting, particularly when dealing with complex patterns that require simultaneous consideration of multiple features. To combat this, we used a maximum tree depth of 8.

3.  Single-layer Feed-forward Neural Network: Single-layer feed-forward neural networks can effectively capture non-linear relationships, even with their relatively simple structure. However, they tend to be more challenging to interpret compared to simpler models. Additionally, careful tuning of the network's architecture and training process is required to mitigate the risk of overfitting.

4.  XGBoost: XGBoost is a powerful algorithm known for its ability to provide highly accurate predictions and handle complex, non-linear patterns in the data. It also incorporates built-in methods to prevent overfitting. However, training XGBoost models can be computationally intensive, and interpreting the predictions it generates can pose challenges.

Simultaneously, we also explored a second modeling strategy using a multiclass algorithm based on a bidirectional Long Short-Term Memory (biLSTM) neural network[@hochreiter_long_1997]. This model was designed to predict the three distinct classes: 'out-of-bed-awake,' 'in-bed-awake,' and 'in-bed-asleep.' It featured four layers and 128 hidden units per layer, balancing model complexity and training efficiency. The bidirectional architecture doubled the hidden units at each time step, enhancing data interpretation and reducing the risk of overfitting. The model accepted sequences of tensors spanning 10 minutes with a step size of one epoch. As demonstrated by previous studies such as those by Sano et al.[@sano_multimodal_2019] and Chen et al.[@chen_attention_2021], LSTM models have shown great promise in sleep detection using accelerometer data, thanks to their ability to capture complex temporal patterns.

### Model Training

We trained a total of four pairs of models in sequence, with each pair distinguishing between in-bed/out-of-bed and asleep/awake classes, respectively. The dataset was randomly split into a training and a testing set, each containing approximately half of the subjects. To ensure the robustness of the results, we made sure that data from the same night was not distributed across both sets. To optimize our models, we used a specific set of hyperparameters for each type of machine learning algorithm. For the Decision Tree, we tuned the cost complexity, tree depth, and minimum number of samples required at a leaf node. The decision tree model was set up using the rpart[@rpart] engine, with tree depth ranging from 3 to 7. For Logistic Regression, implemented using the glmnet[@friedman_glmnet_2010] engine, we tuned the penalty and mixture hyperparameters controlling regularization. The feed-forward neural network was implemented with a single-layer feed-forward architecture using the nnet[@nnet] engine, with the maximum number of allowable weights set to 7000 as a form of regularization. The hyperparameters we tuned for this model were the number of hidden units, the penalty, and the number of epochs. The range for the number of hidden units was between 3 and 27. Lastly, the XGBoost model was configured with the xgboost[@xgboost] engine. The hyperparameters subjected to tuning included tree depth, learning rate, loss reduction, minimum number of samples required at a leaf node, sample size, and number of trees. For this algorithm, the number of trees was specifically tuned within a range of 200 to 800. All model hyperparameters were optimized using a 10-fold grouped Monte Carlo cross-validation, carried out on a regular grid comprising different combinations of these parameters. By providing the range and the specific hyperparameters considered for each model, we ensured the most robust and optimal model fitting.

After identifying the best-performing hyperparameters, we proceeded to fit the models to the full training dataset. This approach allowed us to use all available data for model parameter estimation, thereby maximizing performance. To tackle the imbalance in the extracted in-bed time from our sequential modelling strategy, where the 'awake in-bed' class made up only about 15% of the training data for the sleep/wake classifiers, we employed the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE works by generating synthetic samples in the feature space to balance out the classes. Specifically, it creates synthetic observations by randomly selecting a minority class instance and its nearest neighbors, then producing a new instance that is a blend of the two. This method, as outlined by Chawla et al. [@chawla_smote_2002], mitigates biases during model training that arise when models are skewed towards the majority class. Using the themis R package [@themis], we implemented SMOTE to achieve a balanced distribution of training samples for both classes. The optimization was driven by the F1 score as a metric since it harmonizes precision and recall, rendering it more resilient to class imbalance.

In parallel to these sequential models, we also trained a bidirectional Long Short-Term Memory (biLSTM) model to classify three distinct classes: "out-of-bed awake", "in-bed awake", and "in-bed asleep". The data for this model was divided into training, validation, and test sets, adhering to a 50/25/25 split ratio. Again, caution was exercised to avoid having data from the same night across different sets. For efficient and adaptive learning, the Adam optimizer was used during the training process. Given that we were dealing with a multiclass classification task with mutually exclusive classes, the cross-entropy loss function was employed. At the output layer, a softmax activation function was applied to obtain a probability distribution over the classes. We monitored the model's performance using the F1 score for both the training and validation sets and employed early stopping with a patience of 3 epochs, ceasing training if no improvement in the validation loss was observed over three consecutive epochs.

### Model Validation

In the current study, we utilized standard evaluation metrics dereived from confusion matrices to assess the performance of each model on an epoch-to-epoch basis. These include $$accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$ $$sensitivity = \frac{TP}{TP+FN}$$ $$specificity = \frac{TN}{TN+FP}$$ $$precision = \frac{TP}{TP+FP}$$ $$NPV = \frac{TN}{TN + FN}$$ $$F_1 = 2 \cdot \frac{precision \cdot sensitivity}{precision + sensitivity}$$

where $NPV$ is negative predictive value, $F_1$ is the F1 score, $TP$ is true positives, $FP$ is false positives, $TN$ is true negatives, and $FN$ is false negatives.

In our sequential modelling strategy, the models in the first stage that carried out the binary classification task of distinguishing between in-bed and out-of-bed states was evaluated using the F1-score, accuracy, sensitivity, specificity, and precision. In the second stage of the sequential modelling strategy, models responsible for distinguishing between 'asleep' and 'awake' states were evaluated using the same metrics, with the addition of the negative predictive rate. Given the class imbalance, the F1 score was calculated as an unweighted macro-average, allowing the metric to represent predictions for both classes effectively. We also scrutinized a multiclass biLSTM classifier using the same metrics, interpreting its multiclass output as two separate binary classifications: out-of-bed versus all other states, and in-bed-awake versus in-bed-asleep. Moreover, to give a comprehensive view of model performance, we present confusion matrices for the entire dataset, covering both in-bed and out-of-bed data. These matrices report relative counts, column percentages for accurate predictions of the true class, and row percentages for correctly classified predictions. Both in-bed/out-of-bed and awake/asleep classification tasks were treated as binary, designating 'in-bed' and 'asleep' as positive labels and 'out-of-bed' and 'awake' as negative labels, in line with prior studies [@hjorth_measure_2012; @kushida_comparison_2001].

To evaluate how well our models performed in generating sleep quality metrics, we employed Bland-Altman plots and Pearson correlations. Specifically, the Bland-Altman approach was used to estimate the level of agreement between two different measurement techniques. Given the nature of our dataset, which contains multiple observations per subject but not necessarily equal number of observations, we used a bootstrap procedure to account for this extra variability. We initially calculated the mean difference or bias, and then determined the limits of agreement (LOA) as the bias ± 1.96 times the standard deviation of these differences. Given the possibility of non-normal distribution and skewness in our data, we opted for a bias-corrected and accelerated (BCa) bootstrap method [@diciccio_bootstrap_1996]. This allowed for more accurate estimation, taking into account intra-subject variability. Using 10,000 bootstrap replicates, we estimated the 95% confidence intervals for both the bias and LOA, thereby ensuring robust measurements. The sleep quality metrics conformed to ZM definitions and included the following:

1.  Sleep Period Time (SPT) - This refers to the total duration of time in bed with the intention to sleep, which is defined as the time from the start to the end of the ZM recording.
2.  Total Sleep Time (TST) - This is the time spent asleep within the SPT.
3.  Sleep Efficiency (SE) - This is the ratio between TST and SPT, representing the proportion of the sleep period that was actually spent asleep.
4.  Latency Until Persistent Sleep (LPS) - This metric represents the time it takes to transition from wakefulness to sustained sleep. It is calculated as the time from the beginning of the ZM recording until the first period when 10 out of 12 minutes are scored as sleep.
5.  Wake After Sleep Onset (WASO) - This refers to the time spent awake after initially falling asleep and before the final awakening. In our analysis, a period is counted as 'awake' only if it consists of 3 or more contiguous 30-second epochs which is also how the ZM summarizes WASO.

The technical frameworks used for model development and analyses were R version 4.3.0 [@rcoreteam_2023] along with the Tidymodels[@kuhn_tidymodels_2020] and Tidyverse[@wickham_tidyverse_2019] package suites. For the biLSTM model, we used Python version 3.10.6 [@vanrossum_python_2009] and PyTorch [@paszke_pytorch_2019].

## Results

As indicated in @tbl-10, the application of 5-minute and 10-minute median filters led to modifications in the sleep quality metrics derived from ZM predictions. SPT remained consistent between raw and filtered data sets, with a mean duration of 9.2 ± 2.1 hours, which aligns with the length of the ZM recording. TST and SE increased in the filtered data, implying the filters designate some wakefulness as sleep. Specifically, the mean TST rose from 7.7 ± 1.9 hours in the raw data to 8.1 ± 2.0 hours with a 5-minute filter and to 8.2 ± 2.1 hours with a 10-minute filter. Similarly, SE increased from an initial mean of 82.6 ± 12.0% to 86.4 ± 12.7% and 87.5 ± 12.9% for the 5-minute and 10-minute filters, respectively. Furthermore, the LPS also saw an increase, implying that the filters are removing brief asleep periods at the onset of sleep, thereby lengthening the time it takes to achieve persistent sleep (i.e., 10 out 12 minutes classified as asleep). On the other hand, the WASO metric decreased from a raw average of 39.0 ± 33.6 minutes to 30.6 ± 46.8 minutes and 22.3 ± 55.4 minutes in the 5-minute and 10-minute filtered data, respectively. Notably, the application of these filters also led to a significant reduction in the average number of awakenings per night. In the unfiltered data, the mean number of awakenings (or arousals) stood at 34.46 ± 11.33, which dramatically dropped to 4.43 ± 3.26 and 1.95 ± 2.01 in the 5-minute and 10-minute filtered datasets, respectively.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-10
#| tbl-cap: "Overview of characteristics of the ZM sleep quality summaries per night (585 nights from 151 children). Values are represented as mean (SD). Hrs: hours, min: minutes."

tbl_10
```

```{=tex}
\endgroup
```
### Performance on Epoch-to-Epoch Basis

As delineated in @tbl-11, the epoch-to-epoch evaluation for predicting in-bed time shows virtually identical performance across the various model types. The F1 score fluctuates slightly, ranging from 94.4% in the Decision Tree model to 95.4% in the XGBoost model. Likewise, accuracy varies minimally from 95.3% for the Decision Tree model to 96.1% for the XGBoost model. Other metrics such as Sensitivity, Precision, and Specificity also exhibit uniform performance across the different models. While the XGBoost model does exhibit the highest performance with an F1 score of 95.4% and an accuracy of 96.1%, it only marginally surpasses the other models in these metrics.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-11
#| tbl-cap: Performance metrics of the classification of in-bed/out-of-bed time of the included models.

tbl_11
```

```{=tex}
\endgroup
```
@tbl-12 illustrates the performance metrics for differentiating sleep/wake of all included models. In raw ZM predictions, the XGBoost model stood out with an F1 score of 76.2% and a precision of 92.8%. However, the biLSTM model struggled in this category, particularly with specificity, which was the lowest at 26.9%, even though its sensitivity was notably high at 98.1%. When a 5-minute median filter was applied, the XGBoost model further improved with an F1 score of 79.2%, and an increased NPV of 74.0%. The Decision Tree, during this phase, achieved an F1 score of 75.5% but a decreased specificity of 59%. The Neural Network's performance remained consistent, with an F1 score around 71.7% and precision of around 95.8%. The application of a 10-minute median filter saw the XGBoost's performance peak with an F1 score of 80.9% and a precision of 94.9%. In contrast, the biLSTM improved slightly with an F1 score of 70.9% but still lagged in specificity at 42.4%. Overall, while models like XGBoost seemed to demonstrate the most potential, the consistent challenge across models remained in achieving high specificity values.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-12
#| tbl-cap: Performance metrics of the sleep/wake classification of the included models.

tbl_12
```

```{=tex}
\endgroup
```
@fig-bin_conf_mat and @fig-mul_conf_mat presents a comprehensive set of confusion matrices generated from data that includes both out-of-bed and in-bed periods. These matrices offer insights into the epoch-to-epoch performance of all sequential models when differentiating between 'awake' and 'asleep' states, irrespective of whether the subject is in bed or out of bed. However, it's crucial to acknowledge that the sequential models, owing to their binary nature, are not equipped to directly classify the 'in-bed-awake' state. In contrast, the biLSTM model, which does identify the 'in-bed-awake' state as a separate class, seems to be less successful in classifying this specific state.

![Confusion matrices for the binary predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage. i) decision tree, ii) logistic regression, iii) feed-forward neural net, iv) XGBoost](figures/all_binary_conf_mats.pdf){#fig-bin_conf_mat}

![Confusion matrices for the biLSTM predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage.](figures/all_multiclass_conf_mats.pdf){#fig-mul_conf_mat}

### Evaluation of Sleep Quality Metrics

A comprehensive analysis of the included models predicting sleep quality metrics --- namely SPT, TST, SE, LPS, and WASO --- is detailed in \@tbl-13. This assessment spans sleep quality metrics derived from three data categories: ZM raw data, 5-minute median filtered ZM data, and 10-minute median filtered ZM data.

Against the sleep quality metrics derived from the ZM raw data, biLSTM showed a prominent -36.7-minute bias for SPT, while XGboost stood out for its minimal bias and top correlation score of 0.56. Across metrics, varying biases emerged; for instance, in SE, the Feed-Forward Neural Net and biLSTM displayed contrasting biases. For LPS and WASO, biLSTM generally showed strong biases, with overall correlations being modest.

Using the sleep quality metrics of the 5-minute median filtered ZM data, biLSTM's bias for SPT remained significant. Yet, XGboost consistently shined in correlation, even when other models like Logistic Regression exhibited notable biases. The correlations for LPS, however, remained weak across models, while XGboost dominated in the WASO metric.

Lastly, against the sleep quality metrics of 10-minute median filtered ZM data, biLSTM's bias persisted, especially in SPT, whereas XGboost and the Decision Tree exhibited competitive correlations. Notably, Logistic Regression and the Feed-Forward Neural Net displayed pronounced biases, especially in TST and SE. Overall, while biases varied across models and metrics, XGboost consistently showed high correlations, indicating its strong predictive performance relative to other models.

Overall, the decision tree model consistently underestimated SPT, TST, and SE, and overestimated LPS and WASO in comparison to ZM. The logistic regression model had similar trends, with more pronounced underestimation in TST and overestimation in LPS. The feed-forward neural network also exhibited similar bias as the decision tree and the logistic regression models, but with a higher overestimation in WASO. On the other hand, the XGBoost model showed least bias among all, especially in its 5-minute median predictions. Considering LOA, the decision tree had higher variability in the differences across different sleep quality metrics and filtering techniques, particularly for LPS and WASO, which indicates lower agreement with ZM. Other models had comparable LOA but with notable exceptions. For example, TST LOA for the logistic regression model was particularly wide in the 5-minute median predictions. Correlation-wise, the pearson coefficient, revealed that the XGBoost model consistently had the highest correlation with ZM across all sleep quality metrics and filtering methods Notably, the XGBoost's 5-minute median predictions showed the strongest correlation (0.66) for TST among all models and filterings.

```{=tex}
\begingroup
```
\scriptsize

```{r}
#| echo: false
#| message: false
#| label: tbl-13
#| tbl-cap: Summary of bias, limits of agreement, and Pearson correlation for various sleep parameter predictions (SPT, TST, SE, LPS, WASO) using different machine learning and deep learning models (decision tree, logistic regression, feed-forward neural network, XGBoost) on raw ZM predictions, 5-minute and 10-minute median predictions. Each value is provided with its 95% confidence interval (CI).

tbl_13

```

```{=tex}
\endgroup
```
As established from the @tbl-13, the XGBoost model trained on the 5-minute median filtered ZM data seemed to be the best performing model configuration. The Bland-Altman plot and scatterplot presented in @fig-xgb_ba_cor illustrate the level of agreement between the this XGBoost model and ZM-derived sleep quality metrics that were also median-smoothed over 5 minutes. For the sleep quality metrics SPT and TST, the bias is notably close to zero, revealing a minimal average difference with the ZM. The scatterplot for SPT also suggests a moderate linear correlation between the model's predictions and the ZM-derived metrics. The TST scatterplot further indicates a slightly higher correlation, mainly due to the lack of extreme outliers. In contrast, the remaining sleep quality metrics, namely SE, LPS, and WASO, show signs of heteroscedasticity unlike SPT and TST. While a moderate positive linear correlation exists between the XGBoost model and the ZM-derived SE metrics, poorer correlations are observed for LPS and WASO.

![Comparison of sleep quality metrics derived from the XGBoost model trained on the 5-minute smoothed ZM predictions. The left column displays Bland-Altman plots. Dashed lines represent the bias (the average difference between the two measurements) and LOA, with the 95% confidence intervals represented as the grayed areas. The right column displays scatter plots of XGBoost-derived vs ZM-derived sleep quality metrics. The dashed line represents the identity line, while the full-drawn line represents the best linear fit. Pearson's correlations are annotated in the upper left corner](figures/median_5_xgboost_ba_cor.pdf){#fig-xgb_ba_cor}

# Summary and Discussion of Results

The following sections provide a summary of the main findings reported in this thesis, discusses the implication of the results, outlines directions for future research and provides an overall conclusion.

## Paper I

In this study, we introduced a methodology for manually annotating periods spent in bed using accelerometry data. The accuracy of this method was evaluated across multiple raters, within raters (test/retest) and then compared to in-bed periods as detected with the EEG-based ZM. Furthermore, the comparison of self-reported in-bed periods as obtained from prospective sleep diaries was tested against the ZM. All comparisons were made using ICC and Bland-Altman analyses. When examining the limits of the 95% confidence interval of the ICC analyses, we found several noteworthy results. First, the method exhibited good-to-excellent inter-rater agreement between human raters. Second, intra-rater agreement of the human raters also showed good to excellent agreement across all three raters between their first and second rounds of annotations. Third, when comparing human raters to the ZM, the ICC indicated excellent agreement. Fourth, comparing in-bed periods of self-reported vs ZM, we also observed excellent agreement between the two. Additionally, Bland-Altman analysis indicated that the mean bias between both the manual annotations and self-reported sleep times compared to ZM was within a range of ±6 minutes, with LOAs not exceeding ±45 minutes. Probability density distribution plots further substantiated these findings, showing comparable symmetry, spread around zero, and positioning of outliers when the manual annotations and self-reported sleep times were compared to ZM.

The high accuracy observed between th ZM and the prospective sleep diaries in this study can be attributed to the sleep diaries being synchronized with ZM. Having participants manually start and end the ZM recordings every morning and evening enhances their ability to accurately recall their times of going to bed and getting out of bed. This minimizes the usual discrepancies often seen between objectively and subjectively measured sleep durations[@aili_reliability_2017]. If participants had been instructed to log their sleep using only subjective measures without these protocol anchors, we would not expect to see less strong agreement between the manual annotations and sleep diaries, or ZM timestamps.

Compared to ZM, our study found that the manual annotation of in-bed periods was more prone to errors when estimating the time of going to bed. This was expected as the issue mainly arose from raters having difficulties in differentiating between inactive behaviors before bed time and actual bed time. Despite this limitation, the manual annotation method displayed reassuring accuracy, especially considering the limited formal instructions provided to the raters. This ease of use was aided by the specific signal features we selected for study in Audacity, as evidenced by the excellent ICC agreement scores between the raters. Interestingly, our findings suggest a learning curve for the raters, as evidenced by the narrower LOAs and the density plots in the second round of manual scoring. These indicators suggest that additional rounds of scoring could further improve result consistency or that preliminary training could be beneficial for the raters. Consequently, future research should explore methods to optimize the uniformity of these manual annotations.

While numerous tools are available for annotating time series data, including Label Studio[@label_studio], Visplore[@visplore], and others, our research determined that Audacity was sufficiently suited for our specific requirements. Both Label Studio and Visplore have their strengths and limitations. For instance, while Label Studio's capability to handle vast datasets is contingent on factors like hardware, browser restrictions, and backend configuration, it might encounter challenges with week-long accelerometer data comprising over 100 million entries. Visplore, designed for visual exploration of time series data, might have its own set of limitations and strengths, which would need consideration based on the specific requirements of a task. In contrast, Audacity demonstrates proficiency in managing and navigating these extensive datasets.

We deliberately tailored our feature selection to prevent overwhelming the raters with superfluous information, choosing a concise yet effective set of features rooted in domain expertise. This strategy could be adapted for annotating other activities, such as walking, which would, however, necessitate a different feature set. The human raters in this study gained valuable insights despite the absence of explicit guidelines for data annotation, highlighting the intuitive nature of the method. It's important to note that labeling data inherently involves a certain level of understanding of human behavior. If such labels could be accurately determined based on a set of formal rules, it raises the question of whether training an machine learning model would even be necessary. Therefore, we recommend additional research to identify the most critical features for successful manual annotations. Examining the impact of varying feature sets could yield further insights that would streamline the manual annotation of accelerometer time series data.

To date, many studies comparing actigraphy and self-report methods to PSG or EEG-based methodologies have primarily focused on evaluating aggregate sleep measures like total sleep time, wake after sleep onset, sleep latency, and sleep efficiency. These measures inherently incorporate aspects like sleep onset and wake onset times, akin to the "to-bed" and "out-of-bed" timestamps of our study. However, the precision of these specific time points has been scarcely assessed, making direct comparisons with our study difficult. Pur manual annotations for marking time in bed yielded ICC scores that are comparable or even superior to previous studies that compared actigraphy sleep parameters with PSG[@haghayegh_application_2020; @yavuz-kodat_2019]. One study reported mean absolute errors of 39.9 minutes for sleep onset time and 29.9 minutes for wake-up time, with 95% limits of agreement surpassing ±3 hours when comparing an algorithm to PSG[@van_hees_estimating_2018]. It's worth noting that determining exact timestamps of specific events is inherently more challenging than summarizing broader measures like total sleep time. The former demands high precision and subjective judgment, while the latter averages variations across longer periods, naturally minimizing potential discrepancies. Our research underscored the strengths of manual annotations. One consistent observation was their reliability across a broad age and gender spectrum, which was reflected in our diverse sample that included both children and adults from various genders. This highlights the adaptability of manual annotations, given that sleep behaviors are often influenced by developmental stages and gender-specific factors. Furthermore, despite being more labor-intensive, manual annotations seem to offer superior precision, particularly when identifying exact moments. In some cases, they outperformed automated methodologies[@van_hees_estimating_2018] which may struggle with nuances that human raters easily detect. Accuracy in these evaluations isn't universal and is often influenced by demographic and environmental factors. Our method's ability to deliver consistent results across varied groups indicates its potential for broader applicability, which is vital when generalizing findings, especially for studies targeting normal sleepers.

Identifying sleep periods as opposed to merely lying down is a critical aspect of 24-hour behavior profiling. Traditional studies on sleep detection often rely on participants to self-report their time in bed, sleep onset, and wake-up times[@littner_2003; @lockley_1999; @girschik_validation_2012]. However, the manual annotation methodology offers an alternative that not only reduces the burden on participants but also mitigates the recall bias inherent in self-reported measures. This method of manual annotations can be easily applied to free-living data, making it incredibly versatile for various applications beyond sleep detection. For instance, the manual annotations is useful for annotating non-wear time, manually synchronizing clocks across different devices, and examining the validaty of raw data and more. Its applicability also extends to multi-channel data, providing a comprehensive overview that can incorporate variables like orientation from gyroscopic data, temperature, battery voltage, and light. Audacity stands out for its capability to handle large multi-channel data effortlessly. Researchers can quickly zoom to any resolution and scroll through time without experiencing lag, which makes it an ideal tool for adding labels. This fluidity in workflow suggests that Audacity could become a standard tool for researchers working with raw data for labelling purporses and in relation to machine learning applications.

For years, the transition from raw sensor data to operational predictive models has relied on labeled data. Despite this, no previous research has offered a insights into the precision of manual annotations compared to self-report measures and EEG-based in-bed periods which allows researchers to optimally utilize their available accelerometry data. Our study demonstrates that with a careful selection of features, manual annotation for identifying in-bed periods can yield results comparable to those achieved with EEG-based sleep monitoring hardware. However, it's crucial to clarify that we are not advocating that manual annotations should replace more established techniques for sleep estimation, such as EEG or tracheal-sound-based methods, in ongoing studies. Instead, our methodology can be valuable as a post-hoc procedure to enrich existing datasets with an additional measure of sleep.

This study boasts several strengths, notably the continuous, multi-day data collection of accelerometry, sleep diary, and ZM recordings carried out in participants' homes, which provides high-quality free-living data. However, the study is not without limitations. One such limitation pertains to rater generalizability. The three manual raters were all
experienced working with accelerometry data, and as such, their proficiency might not be representative of the broader population of potential raters, possibly affecting the replicability of our findings with less experienced individuals. Nevertheless, given the minimal pre-briefing instructions for labeling raw data, we believe this methodology could be generalizable to other researchers working with accelerometer data. Another concern is the challenge of recording true free-living behavior using participant-mounted devices like ZM, as wearing such a device during sleep could affect participants' natural behavior, thereby posing a study limitation. Moreover, although the criterion measure in our study has been validated against PSG, utilizing PSG as the criterion measure would have been more optimal. Finally, the study did not consider napping behavior; its focus was solely on in-bed periods as it relates to circadian rhythms. As such, future research is required to validate the utility of this manual annotation methodology for detecting naps.

## Paper II

In our study, we evaluated various methods for classifying non-wear episodes in accelerometer data, focusing on episodes longer than 60 minutes and those shorter than 60 minutes. Our findings showed that the simplest methods, specifically cz_60 and heu_alg, excelled in identifying non-wear episodes longer than 60 minutes across all three sensor wear locations: wrist, hip, and thigh. They were closely followed in performance by decision tree models that included surface skin temperature as a predictive variable. On the other hand, the random forest model demonstrated excellent performance only on the wrist, delivering mediocre results on the hip and thigh. When we shifted our focus to short non-wear episodes lasting less than 60 minutes, we found limitations in the cz_60 and heu_alg algorithms due to their built-in minimum episode durations of 60 and 20 minutes, respectively. As a result, their performance was poor for these shorter episodes. Similarly, the deep learning model showed poor results, mainly attributable to a low sensitivity score that led to many episodes being misclassified as non-wear time. The random forest model's performance was also poor on the hip and only mediocre on the thigh and wrist. Decision tree models, both without temperature and with all predictors, showed mediocre performance as well. However, a decision tree model trained on the six most important predictors stood out as the best performer for short non-wear episodes. Our study also highlighted the value of incorporating surface skin temperature as a predictor to enhance the performance of non-wear time classification. Overall, these results provide valuable insights into the effectiveness of various methods for classifying non-wear episodes in accelerometer data, emphasizing the potential of simple algorithms like cz_60 and heu_alg, especially for longer episodes, and the benefit of including surface skin temperature as a predictive variable.

We discovered that most non-wear episodes in our ground truth datasets had a duration exceeding 60 minutes, with a noticeable peak around the 10-hour mark. This finding contrasts with previous research that typically reported shorter episodes as being more prevalent[@aadland_comparison_2018; @jaeschke_variability_2018; @hutto_identifying_2013]. Our data prominently features children and physically active adolescents, a demographic known to spend less time in sedentary activities and to frequently interrupt such periods[@cooper_objectively_2015; @kwon_breaks_2012]. This likely contributed to both the longer non-wear episodes and clarified the differentiation between sedentary behavior and non-wear time in our study. The data favored simple heuristic algorithms for classifying non-wear time, largely because the limitations imposed by minimum window lengths had a negligible impact on the proportion of non-wear time that was incorrectly classified. These algorithms achieved excellent precision scores, confirming that neither sedentary time nor sleep was misclassified as non-wear time. This is a significant finding, given that multiple previous studies have pointed out the complexities in making this very distinction[@troiano_physical_2008; @duncan_wear-time_2018; @choi_validation_2011; @doherty_large_2017; @barouni_ambulatory_2020]. Our study suggests that a consecutive zeros algorithm could be deemed best practice for capturing non-wear episodes lasting over 60 minutes in children and adolescents. This recommendation is applicable across the various wear locations that we evaluated, including the hip, thigh, and wrist. However, it's important to consider that the specific behaviors of children and adolescents may not make these findings directly transferable to older adults. Yet, certain standardized procedures like the syed_CNN model for mounting and unmounting accelerometers might have more universal applicability.

Creating a model to classify non-wear time appears to be a relatively straightforward task, likely because the decision boundary involved is close to linear. In this context, the use of complex models, as we've included in our current study, may lead to overfitting. This overfitting would capture random variations specific to the training dataset, thereby reducing the model's ability to generalize to new, unseen data. Consequently, we hypothesize that a well-optimized logistic regression model could perform just as well as the more intricate methodologies we've tested. The reason for this is that a logistic regression model would establish a separating linear hyperplane capable of distinguishing between wear and non-wear time effectively. Therefore, employing highly non-linear models for this classification task might be an unnecessary complication, particularly if the goal is to develop a machine learning model that can be applied across diverse populations and wear locations. It is also crucial to enrich the training data with multiple wear locations and various physical activity profiles to improve generalizability.

Incorporating surface skin temperature for the classification of non-wear time has been minimally explored in the realm of machine learning. One study did indicate that using acceleration data along with rate-of-change in surface skin temperature could create a robust decision tree model for detecting non-wear time[@vert_detecting_2022]. This aligns with previous heuristic studies that have shown improved predictive performance when temperature data is included[@duncan_wear-time_2018; @zhou_classification_2015]. Our own findings also corroborate this, as we observed that adding surface skin temperature as a variable significantly enhances the performance of the non-wear time model. However, a critical consideration is the precise detection of the transition between wear and non-wear periods, especially given the slow temperature step response time of sensors. Solely relying on temperature data could introduce classification delays if the sensor's response time is sluggish. Combining both temperature and acceleration data is therefore a more effective approach[@zhou_classification_2015]. During our study, we noted a 20-minute step response in the Axivity temperature sensor, which could be attributed to the design of the device's casing. The sensor's response time may also be influenced by the attachment method used. If more material is placed between the skin and the device, delays are likely to occur, suggesting that machine learning models should perhaps consider the type of sensor attachment in their algorithms. Additionally, different brands of devices have been found to have varying optimal temperature thresholds, further complicating the issue. As noted by Duncan et al. and Zhou et al., algorithmic modifications are needed for devices to function optimally in different latitudes[@duncan_wear-time_2018; @zhou_classification_2015]. Therefore, the type of device and its attachment method can be critical variables for improving the accuracy of non-wear time classification models.

In the study of accelerometry data processing, the ideal scenario is to employ a single model that performs reliably across different wear locations and populations. To evaluate the generalizability and robustness of the methods used in our study, we included a dataset from wrist-worn devices for external validation. This ensures that the performance metrics of our decision tree models are not artificially inflated due to overfitting or lack of variance between the training and testing data. External validation involves testing a model with independently sourced datasets to confirm its performance. If a predictor set has been inaccurately selected due to characteristics inherent to the training data, such as technical or sampling bias, it is likely to perform poorly during external validation[@steyerberg_prediction_2016]. The rationale behind using external validation is strong: while data from different sources may have fewer similarities, they can nonetheless capture important domain-relevant information. A model trained to identify truly informative predictors will maintain its performance even when exposed to new data. Therefore, the external validation in our study acts as a verification step, ensuring that our decision tree models that pass this criterion are not just robust but also likely to be interpretable within the domain[@altman_prognosis_2009]. While Syed et al.'s methodology for identifying non-wear time is innovative and logically coherent, we believe its performance may vary depending on the age of the population in the dataset. The approach by Syed et al. focuses on identifying the specific shape of the acceleration signal at the start and end of a non-wear episode. In contrast, methods that simply identify non-wear time based on the absence of acceleration are less dependent on the characteristics of the population, since zero movement during non-wear is a universal trait.

Our results support this idea. The Convolutional Neural Network (CNN) model developed by Syed et al. showed poor performance across all wear locations in our study. One possible reason for this could be the age differences in the populations of the datasets. The original model was trained on an older population, aged between 40-84 years (mean = 62.74, SD = 10.25), whereas our study involved datasets of younger individuals aged 8.1-17.9 years (mean = 12.14, SD = 2.40) for hip and thigh data, and 14.5-16.4 years (mean = 15.4, SD = 0.37) for wrist data. Contrastingly, the sunda_RF model showed acceptable performance in identifying non-wear episodes shorter than 60 minutes on both the thigh and wrist data. This suggests that the model by Sundararajan et al. is less affected by population characteristics, as anticipated, compared to the syed_CNN model. Another point worth noting is that the syed_CNN model was originally trained on data with a frequency of 100 Hz, while we applied it to data with frequencies of 50 Hz and 25 Hz. Although it's unclear whether this frequency difference impacted the model's performance, we believe that the 25 Hz data is sufficient for capturing true movement behavior, given that movement frequencies are generally below 5 Hz.

While it's standard to evaluate a machine learning model using a test split from the same dataset used for training, known as internal validation, this approach has its limitations. For instance, Syed et al. and Sundararajan et al. report high metrics like sensitivity, specificity, and accuracy for classifying non-wear time, but these results are derived from cross-validation without an external validation dataset[@syed_evaluating_2020; @sundararajan_sleep_2021]. The absence of external validation raises questions about the models' generalizability. Highly flexible models, without rigorous testing on independent datasets, risk overfitting or learning dataset-specific nuances rather than broader, more generalizable characteristics. This concern is particularly relevant for Syed et al.'s model. Their methodology could become more robust with training data from a more varied population and a greater number of participants. Differences in signal shapes for mounting and unmounting devices may vary with age or other population characteristics, making a diverse training set essential for improved generalizability. Given these challenges, future research should focus on validating models with independent external datasets prior to publication. While we recognize that accumulating large and diverse datasets may not always be practically feasible, the benefits in terms of model reliability and generalizability make it an important consideration for future work.

The robustness of this study is significantly enhanced by the use of external validation, which offers strong evidence of methodological generalizability. However, there are limitations to consider. One major issue is the absence of a universally accepted gold standard for ground truth datasets in this research area. This lack of a benchmark makes it challenging to compare performance metrics across different studies. Despite this, our approach remains transparent since it relies on raw accelerometer data, and no part of our data collection or analysis process is proprietary. It's important to note that our findings are based on a study population consisting of children and adolescents. Consequently, the results may not be directly applicable to older age groups. Additionally, while we chose to develop a decision tree model for its balance of complexity and interpretability, future research could explore the efficacy of other machine learning methods like logistic regression, gradient boosting, or support vector machines. These alternative algorithms may offer different insights or advantages that could improve upon our current model.

In this study, we examine the effectiveness and generalizability of both existing techniques and our newly-developed decision tree models for classifying non-wear periods in accelerometer data collected in free-living conditions. While current heuristic methods offer promising results, they come with inherent limitations. On the other hand, our findings suggest that some of the newer, more complex machine learning methods may be prone to overfitting. The quality and quantity of data are pivotal factors in training a machine learning model, especially for a straightforward binary classification problem like this, where the aim is to make the model generalizable to different datasets. To mitigate over-optimistic projections about a model's performance on unseen data, we strongly recommend the use of external validation. Additionally, given the importance of accurately detecting non-wear time as the initial step in analyzing accelerometer data, we urge researchers to carefully choose an appropriate method for this critical task.

## Paper III

In our quest to find the most effective method for estimating sleep based on thigh-worn accelerometers, we scrutinized various models designed to predict both in-bed and sleep times, as well as associated sleep quality metrics. These models were trained and assessed using both raw and median-filtered sleep estimates derived from the ZM EEG-based sleep monitor. Overall, all sequential models exhibited strong performance in predicting when subjects were in bed. However, distinguishing between wakefulness and sleep during these in-bed periods proved to be more challenging. Interestingly, while the multiclass biLSTM model excelled in terms of F1 score, precision, and NPV, it lagged behind in deriving sleep quality metrics when compared to the XGBoost model. The latter outperformed all others across every evaluation metric, including epoch-to-epoch prediction and various sleep quality indicators. Nonetheless, it's worth noting that all models struggled with low specificity values, indicating a common difficulty in accurately identifying awake epochs during time spent in bed. We also observed performance improvement in all models when 5-minute and 10-minute median filters were applied. This filtering approach resulted in increased total sleep time and sleep efficiency metrics while reducing wake after sleep onset and the number of awakenings. Of all the models, the XGBoost demonstrated the smallest bias and the highest correlation with the ZM sleep quality metrics, making it the most robust choice for this particular application.

While there is limited existing research on the epoch-to-epoch effectiveness of thigh-worn accelerometers in classifying in-bed time, Carlson and colleagues[@carlson_validity_2021] have offered valuable insights. Their study demonstrated that both a third-party algorithm called "ProcessingPal" and a proprietary algorithm named "CREA" were able to achieve high accuracies of 91% and 86%, respectively. Evaluated against self-reported measures in adolescents and adults, these algorithms yielded impressive F1 scores as high as 95% and 96%. This is consistent with our sequential models, which also managed to surpass 95% in both F1 and accuracy scores for identifying in-bed time, equated in our study with SPT. However, it's worth noting that all models in our study, with the exception of XGBoost, tended to underestimate SPT. The biLSTM model displayed the most significant underestimation, with a bias of -36 minutes. This aligns with previous research by Winkler et al.[@winkler_identifying_2016], who reported a similar trend in both young-middle-aged and older adults. Their algorithm showed a moderate correlation with diary-recorded waking times but overestimated waking wear time by more than 30 minutes, resulting in an underestimation of in-bed time. This underestimation was further validated by Inan-Eroglu et al.[@inan-eroglu_comparison_2021], who found an underestimation of 9.8 minutes when comparing Winkler et al.'s algorithm to self-reported measures in middle-aged adults. Contrastingly, another study reported only a slight underestimation of in-bed time in middle-aged and older adults[@van_der_berg_identifying_2016]. They used a unique algorithmic approach that quantified the number and duration of sedentary periods to ascertain time in bed and active periods to identify wake times. Lastly, it's essential to clarify that strong predictive performance in identifying in-bed time doesn't automatically imply accurate predictions for broader sleep quality metrics. Capturing awake periods during in-bed time, a critical factor for assessing other derived sleep quality metrics, isn't effectively handled by simply predicting in-bed time. This distinction between actual sleep and time spent in bed while awake is often overlooked but is vital for a more comprehensive understanding of sleep quality.

To our knowledge, Johansson and colleagues[@johansson_development_2023] are the only researchers who have gone beyond merely reporting "waking time" and "in-bed time" to provide epoch-to-epoch performance metrics for sleep scoring with thigh-worn accelerometers. Utilizing a single-night evaluation dataset comprising 71 adult subjects, they managed a mean sensitivity of 0.84, a specificity of 0.55, and an accuracy of 0.80. Similarly, our models achieved a high sensitivity of above 97%, but struggled, like Johansson et al.'s algorithm, in detecting in-bed awake epochs. This struggle is manifested in our low specificity scores, which ranged from 54.7% to 76.4%. This challenge is not solely confined to thigh-worn devices. Conley et al.'s[@conley_agreement_2019] meta-analysis reported issues with wrist-worn accelerometers as well, noting mean values of 0.89 for sensitivity, 0.88 for accuracy, and a low 0.53 for specificity among healthy adults. Patterson and colleagues also recently summarized various heuristic algorithms, machine learning, and deep learning models for sleep prediction, finding mean sensitivity and specificity scores of 93% and 60%, respectively. These data collectively highlight the persistent difficulty in automating the identification of periods when individuals are awake yet still in bed. Interestingly, we noted a divergence in our study regarding the overestimation of LPS and WASO by several of our models, in contrast to most previous research. This overestimation is evident in the low NPV scores, suggesting that only a small fraction of the wake predictions are accurate. This inconsistency might be attributed to the SMOTE we used to balance the dataset. If the synthetic 'wake' samples created by SMOTE don't accurately represent the actual 'wake' data, it could cause the models to misclassify certain 'sleep' epochs as 'wake'. Consequently, this could lead to inflated LPS and WASO estimates, as the models would incorrectly identify more instances of wakefulness during sleep.

The application of the SMOTE in our study likely enhanced the performance of various models by addressing class imbalance issues. However, the introduction of synthetic "wake" samples through this method posed a challenge as they might not be fully indicative of genuine wake data. This could explain why some models, including the biLSTM which was not trained on SMOTE-processed data, overestimated TST and SE. Contrarily, the XGBoost model, trained on SMOTE-processed data, managed to navigate these synthetic "wake" samples more effectively and did not overestimate TST as much as other models. Interestingly, Bland-Altman statistics for the XGBoost model trained on 5-minute median-filtered ZM predictions indicated a mean difference of -7 minutes for TST and -1.1% for SE. The limits of agreement for these metrics spanned from -95.5 to 81.4 minutes and -15.6% to 13.3% respectively. This suggests that the XGBoost model successfully maintained a balance between sensitivity and specificity without being overly swayed by the synthetic "wake" samples. The resilience of the XGBoost model to these synthetic samples could be attributed to its gradient boosting mechanism, which allows for iterative learning from prior models' errors. Such an iterative learning process likely rendered XGBoost more robust against inaccuracies that might be introduced by synthetic data, ultimately leading to a better overall model performance.

Sleep detection methods are generally used in two distinct scenarios: night-only recordings and 24-hour recordings. For night recordings, SE and LPS can be readily derived since SPT can be inferred from the length of the recording itself, as indicated by studies from Conley et al. and Patterson et al[@conley_agreement_2019; @patterson_40_2023]. In contrast, when applied to 24-hour recordings, most sleep detection methods face the challenge of inferring SPT without the aid of sleep diaries, as presented by several studies[@girschik_validation_2012; @doherty_large_2017; @anderson_assessment_2014]. This limitation prevents these methods from generating sleep quality metrics dependent on SPT. To address this issue, we designed models capable of distinguishing between in-bed awake time and in-bed asleep time, as well as out-of-bed awake time, over a full 24-hour period. This innovation allows our models to estimate a comprehensive range of commonly used sleep quality metrics. In a similar vein, Van Hees et al.[@van_hees_estimating_2018] proposed an algorithm for determining SPT using wrist-worn devices, an approach that was subsequently validated by Plekhanova and her team[@plekhanova_validation_2023]. When combined with other methods, this algorithm enables the estimation of additional sleep quality metrics based on the identified SPT. Van Hees et al. reported favorable results with low mean differences when compared to self-reported measures and PSG for SPT, a finding later corroborated by Plekhanova et al. However, both studies also highlighted challenges in achieving good agreement on metrics such as LPS and WASO, revealing low reliability with PSG. These challenges in accurately detecting wakefulness during in-bed time are similar to the issues we encountered in our own study.

In evaluating various sleep quality metrics, our study identified that LPS consistently exhibited the largest mean error in relation to the actual time allocated to it. This challenge in accurately classifying the initial periods of SPT was further corroborated by poor Pearson correlations between LPS obtained from model predictions and the ZM. Among all models assessed, the XGBoost model emerged as the most reliable, yet it overestimated LPS by an average of 26.4 minutes for models trained on unfiltered ZM predictions. This increased to 28.5 and 34.5 minutes when the training data was 5-minute and 10-minute filtered ZM predictions, respectively. This discrepancy is not unique to our study; it is on par with the mean error of 23 minutes in sleep latency reported by Johansson et al[@johansson_development_2023]. Johansson and colleagues attribute such inconsistencies to sleep state's multifaceted and complex physiological nature. Specifically, brief awakenings or transient sleep episodes may not always result in detectable thigh movements, complicating their identification and accurate classification. These observations are consistent with findings on wrist-worn devices by Conley and colleagues[@conley_agreement_2019], who reported that correlations between accelerometer data and polysomnography (PSG) sleep onset latency (equivalent to LPS) varied greatly across studies. The mean correlation was only 0.2, underscoring the challenges in leveraging accelerometry alone for estimating LPS.

Moreover, when compared to other models like the Van Hees algorithm[@hees_novel_2015], Oakley rsc[@palotti_benchmark_2019], and LSTM-50[@palotti_benchmark_2019] as evaluated in the Patterson et al. study[@patterson_40_2023], our XGBoost model displayed narrower LOAs for TST, SE, and WASO. Interestingly, the LOAs were also narrower when pitted against the algorithm tailored for thigh-worn devices by Johansson et al[@johansson_development_2023], albeit not for SPT. Despite these promising facets, it is important to note that all methods, both from this study and the literature, showcase wide LOAs. This implies a high level of variability in sleep quality metrics derived from accelerometry, cautioning against its use as a stand-alone alternative to EEG-based ZM or PSG for individual-level sleep assessments. The presence of extreme outliers in our study appeared to exacerbate the width of LOAs, suggesting that current methods are better suited for group-level sleep quality metrics. As a result, there is a pressing need for further refinement to enhance the reliability and validity of these models for individual sleep assessments.

In our study, we opted to use the ZM as the reference method for sleep measurement, as opposed to the generally accepted gold standard, PSG. While this choice could contribute to the observed discrepancies between our models and ZM outcomes, we argue that the use of ZM has distinct advantages. For one, ZM facilitates multiple consecutive nights of recording in a free-living environment[@pedersen_self-administered_2021], thereby capturing intra-individual variations in sleep patterns. This is an aspect often impractical to achieve with PSG. Additionally, the use of ZM allowed us to incorporate more nights into our study than is typically possible with PSG-based studies. This is evident when comparing our data set to the more limited Newcastle dataset, which consists of only 28 participants[@hees_novel_2015]. Despite its benefits, we found that the raw ZM outputs were not ideally suited for developing machine learning models, primarily due to a low signal-to-noise ratio, as indicated in Figure ZM Median (@fig-paper3_raw_filt). The ZM device itself employs certain filtering processes to mitigate this issue when generating sleep quality metrics. For example, WASO is determined using contiguous epochs of 3 minutes, and sleep only contributes to sleep quality metrics if 10 out of 12 minutes are categorized as sleep. To enhance the effectiveness of our machine learning algorithms, we applied median filters to the raw ZM predictions, which had a notable impact on the derived sleep quality metrics. The application of these filters led to several changes. Specifically, the mean WASO dropped from 39 minutes in the raw predictions to 30.6 minutes when using a 5-minute median filter, and further decreased to 22.3 minutes with a 10-minute median filter. Likewise, TST, SE, and LPS all increased upon the application of the 5-minute and 10-minute median filters. These shifts suggest that the median filters could potentially reclassify brief instances of wakefulness as sleep, and similarly, eliminate short awakenings. Despite these alterations, the sleep quality metrics derived from the median-filtered predictions remained largely consistent with those from the raw predictions, validating the approach we took in this study.

Our current study offers significant contributions to the field of sleep estimation methods, particularly in the use of thigh-worn accelerometers. One of the primary strengths of the research lies in its ability to distinguish between in-bed awake time and asleep time, as well as out-of-bed time. This distinction is crucial for extracting essential sleep quality metrics. Additionally, by evaluating multiple nights per subject, the study offers valuable insights into intra-subject sleep variability, another important factor for sleep assessment. However, there are limitations to our approach. Most notably, we utilized the ZM as our reference method, which is not considered the gold standard for sleep measurement. This choice might have impacted the validity of our findings. For future work, it would be beneficial to employ PSG as a reference, despite its own set of limitations, to provide a more accurate comparison and to easier extend to comparisons of other studies utilizing PSG as the gold standard. Another limitation is the lack of external validation for our models, which confines the applicability of our findings primarily to the children studied.

In terms of model performance, we tested a variety of machine learning and deep learning models to predict in-bed and sleep times as well as corresponding sleep quality metrics. The sequential models performed particularly well in predicting in-bed time, although they encountered difficulties in accurately discerning sleep from wake epochs during that period. Among all the models evaluated, the XGBoost model stood out for its superior performance in epoch-to-epoch predictions and sleep quality metrics.The study also brings attention to the existing limitations of current sleep detection methods. Specifically, there are challenges in effectively identifying wake periods during in-bed time, and improvements are needed to enhance the accuracy of individual sleep assessments. Overall, we believe our work serves as a foundational step for future research aimed at refining these models. The ultimate goal is to offer a more precise and accurate evaluation of sleep patterns and quality through the use of thigh-worn accelerometers.

## Overall Conclusions

### Conclusions

In summing up, our study demonstrates that the use of Audacity for manually annotating in-bed periods based on thigh- and hip-worn accelerometer data aligns well with objective EEG-based sleep device estimates and prospective sleep diaries. Our findings reveal minimal mean bias and acceptable limits of agreement when comparing time to bed and time out of bed across these different methods. Additionally, the manual annotation process proved highly reliable, exhibiting excellent inter- and intra-rater agreement. Its accuracy in relation to EEG-based assessments was also comparable to that of sleep diaries. Importantly, the manual annotation method can be applied to pre-existing raw data that may not have accompanying sleep records. This offers a significant advantage for making better use of free-living data resources. The increased availability of such annotated data can be particularly beneficial when training data-intensive machine learning algorithms, potentially enhancing their generalizability in objectively assessing human behavior. \### Conclusions

\### Conclusions

In this study, we examine the effectiveness and generalizability of both existing techniques and our newly-developed decision tree models for classifying non-wear periods in accelerometer data collected in free-living conditions. While current heuristic methods offer promising results, they come with inherent limitations. On the other hand, our findings suggest that some of the newer, more complex machine learning methods may be prone to overfitting. The quality and quantity of data are pivotal factors in training a machine learning model, especially for a straightforward binary classification problem like this, where the aim is to make the model generalizable to different datasets. To mitigate over-optimistic projections about a model's performance on unseen data, we strongly recommend the use of external validation. Additionally, given the importance of accurately detecting non-wear time as the initial step in analyzing accelerometer data, we urge researchers to carefully choose an appropriate method for this critical task.

In terms of model performance, we tested a variety of machine learning and deep learning models to predict in-bed and sleep times as well as corresponding sleep quality metrics. The sequential models performed particularly well in predicting in-bed time, although they encountered difficulties in accurately discerning sleep from wake epochs during that period. Among all the models evaluated, the XGBoost model stood out for its superior performance in epoch-to-epoch predictions and sleep quality metrics.The study also brings attention to the existing limitations of current sleep detection methods. Specifically, there are challenges in effectively identifying wake periods during in-bed time, and improvements are needed to enhance the accuracy of individual sleep assessments. Overall, we believe our work serves as a foundational step for future research aimed at refining these models. The ultimate goal is to offer a more precise and accurate evaluation of sleep patterns and quality through the use of thigh-worn accelerometers.

# Thesis Perspectives

The importance of sleep health and the limitations of conventional PSG examinations have highlighted the potential of wearable sensor systems as complementary measurement methods. In the previous sections of this thesis, it has been shown that accelerometers as a platform for long-term activity monitoring can be used to assess quantitative and qualitative aspects of sleep. Specifically, the findings of this thesis offer a deep dive into the methods and tools employed in the manual annotation of in-bed periods, the classification of non-wear periods in accelerometer data, and the application of various machine learning models to detect sleep.

The findings regarding in-bed period annotations using Audacity, based on thigh- and hip-worn accelerometer data, showcased notable alignment with EEG-based sleep devices and sleep diaries. The approach exhibited minimal bias, high reliability, and favorable inter-rater agreement. Particularly, the methodology's adaptability to pre-existing raw data without sleep records stands out, paving the way for leveraging free-living data sources. This accessible annotated data holds promise for honing machine learning algorithms in assessing human behaviors.

In assessing non-wear period classification in accelerometer data, we assessed established techniques with our novel decision tree models. Traditional methods, though effective, have their constraints. Notably, some advanced machine learning models risked overfitting. The integrity and volume of data remain central to machine learning efficacy, emphasizing the need for a balanced model generalizable across various datasets. To ensure pragmatic outcomes, we champion external validation and underscore the imperative of method precision for initial non-wear time detection.

Evaluating various models for predicting in-bed and sleep times, sequential models emerged as adept in predicting in-bed times but faltered in distinguishing sleep from wake periods. Notably, the XGBoost model excelled in its predictive capabilities and sleep quality assessments. Yet, challenges persist in current sleep detection methods, especially pinpointing wake intervals during in-bed time. Our efforts underline an urgent need for refining these detection models to achieve nuanced evaluations of sleep patterns using thigh-worn accelerometers.

Regarding the objectives set in the beginning of the thesis, it can be stated that:

-   A method was successfully introduced for the manual annotation of individual bedtime and wake-up times employing raw, unprocessed accelerometry data. The annotations' accuracy was validated, demonstrating alignment when compared with results from a single-channel EEG-based sleep staging system and a conventional sleep diary. Both inter-rater and intra-rater reliability evaluations were conducted, reinforcing the robustness and consistency of the manual annotation approach.

-   Decision tree models were effectively evaluated, using data from both thigh and hip-worn accelerometers to discern non-wear time in accelerometry data. The role of surface skin temperature in this context was identified as an important predictor of non-wear. A comprehensive comparison was undertaken between machine-learned models and heuristic algorithms, analyzing datasets obtained from accelerometers positioned on the hip, thigh, and wrist. This comparison facilitated a deeper understanding of each method's efficacy across different wearable placements.

-   A rigorous assessment was conducted on a range of machine learning and deep learning models with the intent of estimating in-bed and sleep time. All the models included in this analysis were benchmarked against an EEG-based sleep tracking monitor to ensure accuracy and reliability. The models developed were evaluated for their capability to quantify important sleep quality metrics. This evaluation was once again benchmarked against the EEG-based sleep tracking device, ensuring a robust validation process.

A logical next step in this research would be to create and validate a standardized procedure for manual sleep annotation, akin to what is available for EEG-based sleep annotation in the AASM Scoring Manual[@aasm]. This would make the methodology more accessible to individuals with limited experience in the field of accelerometry.

# Code Availability

All code associated with Paper II, used for data processing and analysis, and for producing figures, tables, and results, is available at <https://github.com/esbenlykke/nonwear_project>. This specific codebase requires substantial refactoring and commenting to adhere to best coding practices. However, upon request, the corresponding author can assist in utilizing the code if needed.

For Paper III, all code used for data processing, analysis, figure and table generation, results production, and manuscript pdf document creation is available at <https://github.com/esbenlykke/sleep_study>. This codebase largely adheres to good coding practices and should be deployable 'out of the box', provided all necessary dependencies are installed. To facilitate this, I plan to provide a conda/mamba environment description in the repository to encompass all required dependencies. Note that the repository currently contains some redundant scripts; however, I intend to streamline the content in the future. Additionally, plans are underway to introduce a Snakemake file to automate the entire process.

I've also developed a tool based on the findings from Paper III. This tool leverages the best-performing models (specifically, the XGBoost models trained on 5-minute median-filtered ZM sleep predictions) and is available at <https://github.com/esbenlykke/get_sleep_stats>. When given the path to a folder with .wav or .cwa raw accelerometer files, the tool first extracts all relevant features from the raw data. It then predicts and extracts in-bed periods. Lastly, it predicts sleep time based on the extracted in-bed periods. The final output includes timestamps for going to bed and leaving bed, SPT, TST, and SE for each accelerometer recording.

No code is available for Paper I.

# References

::: {#refs}
:::

# List of Appendices

-   **Appendix I**: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

-   **Appendix II**: Generalizability and performance of methods to detect non‑wear with free‑living accelerometer recordings

-   **Appendix III**: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

-   **Appendix IV**: Supplementary Material for Paper III

\newpage

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix I}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix I}

\vspace{2cm}

\textsf{\Huge Manual Annotation of Time in Bed Using Free-Living Accelerometry Data}

\vspace{5cm}

This paper was published in \textbf{Sensors} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.3390/s21248442}{https://doi.org/10.3390/s21248442}

\end{center}
```
\includepdf[pages=-]{my_papers/paper1.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix II}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix II}

\vspace{2cm}

\textsf{\Huge Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings}

\vspace{5cm}

This paper was published in \textbf{Scientific Reports} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.1038/s41598-023-29666-x}{https://doi.org/10.1038/s41598-023-29666-x}

\end{center}
```
\includepdf[pages=-]{my_papers/paper2.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix III}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix III}

\vspace{2cm}

\textsf{\Huge Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking}

\vspace{5cm}

This manuscript is under preparation for submission to \href{https://academic.oup.com/sleep}{\textbf{SLEEP}}, the official journal of the Sleep Research Society (SRS).

\vspace{1cm}

\end{center}
```
\includepdf[pages=-]{my_papers/paper3.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix IV}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix IV}

\vspace{1cm}

\textsf{\Huge Supplementary Material for Paper III}

\end{center}
```
\includepdf[pages=-]{paper3_supp.pdf}
