---
format:
  pdf:
      # - paperwidth=17cm
      # - paperheight=24cm
    documentclass: scrbook
    toc: true
    lof: true
    lot: true
    
    # toccolor: BrickRed
    # biblio-style: biblatex
    csl: nature.csl
    css: my_styles.css
    # classoption: nottoc
    # biblio-title: "References"
    mainfont: EB Garamond
    sansfont: Montserrat
    fontsize: 9pt
    geometry:
      - margin=20mm
      - paperwidth=17cm
      - paperheight=24cm
    colorlinks: true
    linkcolor: DarkSlateBlue
    urlcolor: DarkRed
    citecolor: DarkSlateBlue
    link-citations: true
    number-sections: false
    pdf-engine: lualatex
    keep-tex: true
    template-partials: 
      - before-body.tex
    include-in-header: 
      text: |
       % Page setup and typography
        \usepackage[margin=20mm, paperwidth=17cm, paperheight=24cm]{geometry}
        \pagestyle{plain}
        \usepackage{sectsty}
        \allsectionsfont{\sffamily}
        \usepackage{multicol}
        
        % Landscape handling
        \usepackage{lscape}
        \newcommand{\blandscape}{\begin{landscape}}
        \newcommand{\elandscape}{\end{landscape}}
        
        % Captions and listings
        \usepackage[font=small,labelfont=bf]{caption}
        \captionsetup{font=footnotesize}
        \usepackage{longtable}
      
        % Other utilities
        \usepackage{pdfpages}
        \usepackage{hyperref}
        \usepackage{afterpage}
        \usepackage[nottoc,numbib]{tocbibind}
        \newcommand{\aftertocpagenum}{
          \cleardoublepage
          \pagenumbering{arabic}
        }
        
        % Continuous numbering for figures/tables
        \usepackage{chngcntr}
        \counterwithout{figure}{chapter}
        \counterwithout{table}{chapter}
bibliography: refs.bib
editor: source
editor_options: 
  chunk_output_type: console
---

\aftertocpagenum

# English Summary

**Introduction**

Sleep is an important element in promoting health, and the quantification of sleep has been improved with modern technology. Polysomnography, considered the gold standard, provides in-depth insight into sleep but is costly. In contrast, accelerometry is a cheaper and less invasive method, especially for longer home-based recordings. Machine learning is a tool that has the potential to automate and facilitate the estimation of sleep from accelerometer data. However, there are three challenges: producing reliable training data, ensuring data integrity through accurate removal of non-wear, and effectively using data to estimate sleep. Firstly, it is necessary to have sufficient and accurate annotations in the data for effective machine learning, emphasizing the importance of methods for manual annotations based on accelerometer data. Secondly, it is essential to detect and remove periods when the device is not worn to perform accurate analyses. Identifying periods of non-wear is challenging, as traditional methods like logbooks can be prone to bias. Existing algorithms removes bias, but their accuracy is still debated. Finally, once data is correctly collected and processed, it is crucial to apply it effectively. Current methods for estimating sleep using accelerometers are based on data from wrist-worn and hip-worn devices, while data from thigh-worn accelerometers remains largely untapped for sleep estimation.

**Aims**

This thesis has the following objectives. Firstly, we will assess the accuracy of manual annotation of bedtime in raw accelerometer data compared to EEG-based bedtime and sleep diaries. Secondly, we will assess heuristic algorithms and machine learning models for detecting non-wear. Finally, we will develop machine learning models for sleep classification and the estimation of sleep quality metrics using data from thigh-worn accelerometers and compare them with EEG-based sleep recordings. Overall, this thesis aims to understand the potential and challenges of using machine learning to estimate sleep via accelerometry.

**Methods**

The data for the papers in this thesis are sourced from the SCREENS pilot experiment (paper I), the PHASAR study, and an internal validation study (paper II), as well as the SCREENS experiment (paper III). Accelerometer data, sleep recordings, and diaries of bedtimes are used from the SCREENS experiments, while PHASAR and the internal validation study provide accelerometer data. All accelerometer data were collected using Axivity AX3 triaxial accelerometers, and sleep was recorded using the EEG-based Zmachine® Insight+ system.

For paper I, accelerometer data from the hip and thigh of 14 children and 19 adults were used. Using Audacity, an open-source audio editing program, three evaluators annotated each accelerometer recording by marking the times when the person went to bed and when they got out of bed. Two rounds of annotations were performed to test reliability. The 'ground truth' was based on EEG sleep recordings. Concordance and agreement was evaluated using the intraclass correlation coefficient and Bland-Altman analyses.

Paper II used accelerometer data from sensors placed on the wrist, thigh, and hip. In data from 64 PHASAR participants and 42 participants in the internal validation study, periods of non-wear were manually annotated in the same way as described in paper I. Three variants of decision trees were trained on 79.2% data from the hip and thigh, and the remaining data were used for testing. Hyperparameters were optimized through five-fold cross-validation. External validation was performed on wrist data from all 42 participants from the internal validation study. All included algorithms and models were evaluated using metrics derived from confusion matrices.

For paper III, accelerometry and EEG-based sleep recordings from children aged 4-17 years were used. Prediction classes from the sleep recording were reduced to "awake" and "sleep", as information about sleep stages is not relevant for generating the sleep quality metrics of interest for this thesis. Data preprocessing included a low-pass Butterworth filter, removal of periods non-wear using the method described paper II, and a set of 64 predictors were constructed. Sleep recordings were median filtered in 5 and 10-minute windows before models were trained to better capture true awakenings. Two model strategies were used, a sequential approach with four types of binary classification models, and the other strategy used a multi-class model. Data for the four pairs of sequential models were divided into training and test sets, and hyperparameter optimization was performed using ten-fold Monte Carlo cross-validation. An imbalance in training data was addressed using the synthetic minority oversampling technique. Data for training the multi-class model was split in a ratio of 50/25/25 for training, validation, and testing. For both strategies, the F1 score was used as an optimization target. To evaluate the performance of all models, metrics derived from confusion matrices were used, and to understand the effectiveness of our models in estimating sleep quality measures, Bland-Altman plots and Pearson correlations were used. The following sleep quality measures were evaluated: time in bed, total sleep time, sleep efficiency, time to first sleep, and wake time after first sleep.

**Results**

In paper 1, we compared manual annotations with the ZM and sleep diaries. The results indicated excellent inter- and intra-rater agreement. Furthermore, the Bland--Altman limits of agreement were approximately ±30 min, showcasing only a minimal mean bias.

In paper 2, our focus was on non-wear detection. For non-wear periods longer than 60 minutes, the established consecutive zeros algorithms were the most effective, registering F1-scores above 0.96. However, for durations shorter than 60 minutes, decision trees stood out, achieving F1-scores of over 0.74 across all sensor locations. Notably, the newly developed deep learning and random forests models couldn't match these performances.

Paper 3 examined sleep classification and the estimation of sleep quality metrics using thigh-worn accelerometers. Here, the XGBoost model excelled, especially when analyzing 5-minute filtered data. The model demonstrated small discrepancies in several sleepquality metrics: sleep period time (0.2 minutes), total sleep time (-7.0 minutes), sleep efficiency (-1.1%), and wake after sleep onset (-0.9 minutes). Additionally, this model exhibited a moderate correlation of 0.66 with total sleep time. It's worth noting that the limits of agreement in our findings mirrored those in previous studies on hip and wrist devices. Specifically, total sleep time exhibited LoAs of (95%CI): -95.5 (-105.2 to -88) minutes to 81.4 (72.4 to 92.5) minutes.

**Conclusions**

Overall, the findings of this thesis underscore the reliability and precision of emerging technological methods in sleep and non-wear detection research. Paper 1 validated the credibility of manual annotation techniques, reinforcing their alignment with traditional benchmarks. Paper 2 emphasized the nuances of non-wear detection, revealing clear strengths in certain algorithms for specific durations and highlighting areas where newer models need enhancement. Paper 3 highlights the XGBoost model for sleep assessment with thigh-worn accelerometers, situating it as a valid alternative compared to methods machine learning models employed on thigh and wrist accelerometer data. However,challenges remain in identifying in-bed awake periods and in assessing individual sleep quality metrics, consistent with previous findings from wrist and hip-worn devices.

# Dansk Resume

**Introduktion**

Søvn er et vigtigt element i sundhedsfremme og kvantificeringen af søvn er blevet forbedret med moderne teknologi. Polysomnografi betragtes som guldstandarden, og giver en dybdegående indsigt i søvn, men er omkostningsfuld. Omvendt er accelerometri en billigere og mindre invasiv metode, især til længere optagelser i hjemmet. Maskinlæring er et værktøj, der har potentialet til at automatisere og lette arbejdet med at estimere søvn fra accelerometridata. Dog er der tre udfordringer: at producere pålidelig træningsdata, sikre integriteten af data og effektivt bruge data til at estimere søvn. For det første er det nødvendigt at have tilstrækkeligt med nøjagtige annotationer i data for effektiv maskinlæring, hvilket understreger vigtigheden af metoder til manuelle annotationer baseret på accelerometridata. For det andet, for at udføre korrekte analyser, er det essentielt at detektere og fjerne perioder, hvor sensoren ikke er båret. Det kan være udfordrende at identificere perioder, hvor sensorene ikke bæres, da traditionelle metoder som logbøger kan være fejlbehæftede. Eksisterende algoritmer kan forbedre denne detektering, men deres nøjagtighed er stadig genstand for debat. Endelig, når data er blevet korrekt indsamlet og bearbejdet, er det afgørende at anvende det effektivt. Nuværende metoder til at estimere søvn ved brug accelerometre er baseret på data fra håndleds- og hoftebårne sensorer, mens data fra accelerometre, der bæres på låret, stort set er uudnyttede i forhold til at estimere søvn.

**Formål**

Denne afhandling har følgende formål. For det første vurderes præcisionen af manuel annotation af sengetider i accelerometridata sammenlignet med EEG-baserede sengetider og søvndagbøger. For det andet undersøges eksisternede og nye algoritmer og maskinlæringsmodeller til at detektere perioder, hvor accelerometeret ikke er båret. Endeligt udvikles maskinelæringsmodeller til søvnklassifikation og estimering af søvnkvalitetsmål ved brug af data fra accelerometre, der bæres på låret og sammenligner med EEG-baserede søvnoptagelser. Samlet set søger denne afhandling at forstå potentialet og udfordringerne ved at anvende maskinlæring til at estimere søvn via accelerometri.

**Metoder**

Data til artiklerne i denne afhandling stammer fra SCREENS piloteksperimentet (artikel I), PHASAR-studiet og en intern valideringsundersøgelse (artikel II) samt SCREENS-eksperimentet (artikel III). Fra SCREENS-eksperiemterne gøres brug af accelerometerdata, søvnoptagelser og dagbøger over sengetider, mens PHASAR og det interne valideringsstudie leverer accelerometerdata. Al accelerometerdata blev indsamlet ved hjælp af Axivity AX3 triaksiale accelerometre, og søvnen blev registreret ved hjælp af det EEG-baserede Zmachine® Insight+-system.

Til artikel I benyttedes accelerometerdata fra hofte og lår fra 14 børn og 19 voksne. Ved hjælp af Audacity, et open-source lydredigeringsprogram, annoterede tre bedømmere hver accelerometeroptagelserne ved at markere tidspunkter for, hvornår personen gik i sengen, og hvornår de stod ud af sengen. Der blev udført to runder med annotationer for at teste pålideligheden. 'Ground truth' baseredes på EEG-søvnoptagelserne. Overensstemmelse blev målt ved hjælp af intraklassekorrelationskoefficienten og Bland-Altman-analyser.

Artikel II anvendte accelerometerdata fra sensorer placeret på håndleddet, låret og hoften. I data fra 64 PHASAR-deltagere og 42 deltagere i den interne valideringsundersøgelse annoteredes manuelt perioder hvor sensorerne ikke blev båret på samme måde som metoden beskrevet i artikel I. Tre varianter af decision trees blev trænet på 79,2% data fra hofte og lår og det resterende data blev brugt til test. Hyperparametre blev optimeret gennem en fem-foldig krydsvalidering. Ekstern validering blev udført på håndledsdata fra alle 42 deltagere fra den interne valideringsundersøgelse. Alle inkluderede algoritmer og modeller blev evalueret ved hjælp af mål afledt af confusion matricer.

Til artikel III benyttedes accelerometri og EEG-baserede søvnoptagelser fra børn i alderen 4-17 år. Prædiktionsklasserne fra søvnoptagelsen blev reduceret til "vågen" og "sove", da information om søvnstadier ikke er relevant for generere søvnkvalitetsmål af interesse for denne afhandling. Dataforarbejdningen omfattede et lowpass Butterworth-filter, fjernelse af perioder, hvor sensorerne ikke blev båret via metode fra artikel II og et sæt på 64 prædiktorer blev konstrueret. Søvnoptagelserne blev medianfiltreret i 5 og 10 minutters vinduer inden modellerne blev trænet, for at fange sande opvågninger bedre. To model-strategier blev anvendt, en sekventiel tilgang med fire typer af binære klassifikationsmodeller og den anden strategi anvendte en multiklasse model. Data til de fire par sekventielle modeller blev delt op i trænings- og testsæt og hyperparameteroptimering blev udført ved hjælp af ti-fold Monte Carlo krydsvalidering. En ubalance i træningsdata blev løst ved hjælp af synthetic minority oversampling technique. Data til træning af multiklasse-modellen blev opdelt i et forhold på 50/25/25 for træning, validering og test. For begge strategier blev F1 score anvendt som optimeringsmål. For at vurdere præstationen på alle modeller blev der anvendt mål afledt af confusion matricer og for at forstå effektiviteten af vores modeller til at estimere søvnkvalitetsmål blev Bland-Altman-plots og Pearson-korrelationer anvendt. Følgende søvnkvalitetsmål blev evalueret: tid i sengen, total sovetid, søvneffektivitet, tid til første søvn og vågentid efter første søvn.

**Resultater**

I den første artikel sammenlignes manuelle annotationer med ZM og søvndagbøger. Resultaterne viste fremragende enighed både mellem bedømmere og inden for samme bedømmer. Derudover var Bland-Altman limits of agreement cirka ±30 minutter samtidig med en minimal gennemsnitsbias.

I artikel II undersøges detekteringen af perioder, hvor accelerometrene ikke bliver båret. For perioder af denne type længere end 60 minutter var de etablerede algoritmer, som på forskellig vis detekterer perioder uden acceleration, de mest effektive og opnåede F1-score over 0,96. Decision trees viste sig at præstere bedst på perioder kortere end 60 minutter og opnåede en F1-score på over 0,74 på tværs af alle sensorplaceringer. De nyligt udviklede deep learning- og random forest-modeller kunne ikke matche disse resultater.

Den tredje og sidste artikel beskæftiger sig med søvnklassifikation og søvnkvalitetsmål ved hjælp af accelerometre, der var båret på låret. Her udmærkede XGBoost-modellen sig, især når den analyserede data filtreret i 5 minutter. Modellen viste små afvigelser i flere søvnkvalitetsmål: tid i sengen (0,2 minutter), total sovetid (-7,0 minutter), søvneffektivitet (-1,1%) og vågen efter først søvn (-0,9 minutter). Derudover viste denne model en moderat korrelation på 0,66 med total søvntid. Det er værd at bemærke, at limits of agreements i vores resultater var sammenlignelige med tidligere studier på hofte- og håndledssensorer. Specifikt viste total søvntid limits of agreements på -95,5 minutter til 81,4 minutter.

**Konklusion**

Samlet set undersøger denne afhandling pålideligheden og præcisionen af metoder inden for bearbejdning af accelerometerdata og søvndetektering. Artikel I understreger at manuel annotatering stemmer overens med EEG-baserede og selvrapporterede sengetider. Artikel II fremhævede nuancerne ved detektering af perioder, hvor sensorerne ikke bæres og viste at visse metoder præsterer bedst for specifikke varigheder af perioderne. Artikel III fremhæver XGBoost-modellen som bedst til at klassificere søvn på data fra accelerometre på låret og viser sammenligelige resultater i forhold til metoder, der anvender maskinlæringsmodeller på data fra hofter og håndled. Dog er der stadig udfordringer med at identificere perioder, hvor man er vågen i sengen. Derudover understreger limits of agreements udfordringer i forhold til at vurdere individuelle søvnkvalitetsmål, hvilket er i tråd med tidligere fund fra sensorer, der bæres på håndleddet og hoften.

# Introduction

The rapidly growing field of wearable technology presents unparalleled opportunities for acquiring accurate and objective data on human behavior, particularly in the context of free-living accelerometer recordings. This encompasses sleep patterns and physical activity among other metrics. However, several challenges exist, especially when the data is collected in free-living conditions as opposed to controlled settings. Issues related to data annotation, the detection of periods when the device is not being worn (non-wear time), and the optimal sensor location for accurate assessments are all more complex in free-living scenarios. This thesis aims to tackle these challenges through a series of studies that employ machine learning techniques and robust analytical methods. The primary focus is on the development and validation of innovative approaches for manually annotating sleep data, identifying non-wear time, and optimizing the use of thigh-worn accelerometers specifically for sleep assessment in free-living conditions.

## Why Track Sleep in Health Research

Physical behaviors throughout a day encompass a range of activities, including sleep, physical activity, and sedentary behavior. A plethora of research over the past decade has strongly emphasized the health benefits of optimal sleep, high pysical activity levels, especially moderate-to-vigorous physical activity[@kraus_physical_2019; @lee_effect_2012], minimal sedentary periods[@wilmot_sedentary_2012], and adequate sleep[@cappuccio_sleep_2010]. These findings have informed public health guidelines, such as the American recommendation of 150 minutes of MVPA per week[@kl_physical_2018], and the Danish guideline suggesting 30 minutes of MVPA daily for adults[@el-zine_fysisk_nodate-1] and 60 minutes for children[@el-zine_fysisk_nodate]. The integration of sleep tracking in health research is increasingly crucial for a comprehensive understanding of individual well-being. Sleep and physical activity are instrumental in influencing a broad spectrum of health aspects, ranging from mental health[@biddle_physical_2011] to physical fitness[@warburton_health_2017], and even extending to disease prevention[@strath_guide_2013; @arem_leisure_2015]. With advancements in wearable technology and tracking systems, we now have tools that enable in-depth investigations into the complex interplay between sleep, physical activity, and general health[@rollo_whole_2020]. Given the significance of sleep and physical activity in overall health, as backed by extensive research and public health guidelines, the need for focused studies on sleep becomes even more compelling. The incorporation of sleep tracking into broader health research allows us to delve deeper into how sleep quality and duration impact various aspects of our well-being, such as mental health, physical fitness, and disease prevention. Advanced wearable technologies and tracking systems now provide us with unprecedented capabilities to examine the intricate relationships between sleep, physical activity, and general health. Hence, an intensive study of sleep is not just beneficial but essential for a holistic understanding of what makes for a healthy life.

We spend approximately one-third of our lives asleep, yet much about this state of consciousness remains elusive, including its biological function and what defines its quality[@ma_sleep_2017]. However, what is clear is the essential role of adequate sleep in maintaining both physical and psychological well-being, consolidating memories, and regulating emotions[@worley_2018; @matricciani_2019; @scott_2021]. On the flip side, insufficient sleep has been linked to a range of negative health outcomes such as weight gain, obesity, heart disease, stroke, impaired immune function, and even an elevated risk of death[@consensus_conference_panel_recommended_2015; @ji_2020; @hale_2020]. The impact of sleep deprivation is felt not just in the long term but also immediately. Short-term consequences include reduced alertness, heightened stress levels, diminished concentration, delayed reaction times, and a propensity for risk-taking behavior[@shochat_2014; @kecklund_2016; @obrien_2005; @bonnet_1985]. When poor sleep becomes a chronic issue, it can severely affect one's quality of life. For example, daytime sleepiness increases the risk of accidents in occupational settings and while driving, as well as negatively impacts academic and work performance, with broader social and economic repercussions like school dropouts or job loss[@connor_2002; @dewald_2010; @roth_1996]. The concern over daytime sleepiness becomes particularly acute given that it is widespread, affecting around 10-20% of society[@wang_2019]. Various factors contribute to this, ranging from lifestyle choices like irregular bedtime and shift work to medical conditions that affect the central nervous system and certain medications[@roth_1996].

## The Gold Standard for Measuring Sleep

The challenge of studying sleep has become significantly more manageable due to advancements in technology and our understanding of neuroscience. It wasn't until the 1950s that sleep study became scientifically feasible, thanks to the pioneering work of Nathaniel Kleitman and Eugene Aserinsky[@aserinsky_1953], who demonstrated the brain's active involvement in sleep. Utilizing electroencephalography (EEG), Kleitman and Aserinsky were able to measure brain activity and identified that it synchronizes over multiple regions, predominantly within specific frequency bands. This groundbreaking discovery enabled them to define distinct "sleep stages" that the brain cycles through, fundamentally transforming the way sleep is measured and understood. A visual example of these sleep stage cycles can be seen in @fig-hypno. Therefore, measuring sleep, given its complexities, not only is critical but also possible thanks to these technological and scientific milestones.

In a relaxed wakeful state, the EEG predominantly displays alpha activity within the frequency band of 8-10 Hz and amplitudes usually ranging from 10-50 μV. As an individual begins to fall asleep, they enter the non-rapid eye movement (NREM) sleep stage 1 (N1). During this drowsy phase, the brain's EEG activity transitions to the theta range frequencies of 4-6 Hz. Simultaneously, muscle relaxation is evident, respiratory rates decelerate, and there's a drop in both distal body temperature and heart rate. This initial stage is followed by NREM sleep stage 2 (N2), where the EEG spectra frequencies are further reduced. This stage introduces sleep spindles---periodic high-frequency waves oscillating between 12-14 Hz lasting for 0.5-1.5 seconds---and K-complexes, which are characteristic reactive EEG elements of N2 sleep.

Progressing deeper into sleep, one enters NREM sleep stage 3 (N3), often termed "deep sleep". Here, the EEG mainly exhibits high-amplitude oscillations within the delta band of 1-4 Hz. Following these NREM stages, the sleep cycle culminates in REM sleep. Interestingly, the EEG patterns during REM sleep closely resemble the alpha activity seen in wakeful states. This stage is marked by evident rhythmic eye movements, while the respiration and heart rate display enhanced amplitudes and variability. The brain stem suppresses most voluntary body movements at this time, making it a period of relative physical inactivity. Dreaming tends to be more frequent during REM sleep. A single REM episode can span a few minutes and tends to extend in duration during the latter part of the night. On average, an entire sleep cycle, from N1 to REM, spans 90-110 minutes and is typically repeated multiple times throughout the night.

![Sample hypnogram showing the sleep stage cycles of an eight-hour polysomnography recording. The sleep stages (REM, NREM 1-3) and arousals are shown.](figures/hypnogram.pdf){#fig-hypno}

Polysomnography (PSG) is a gold-standard technique in sleep research, allowing simultaneous assessment of various physiological signals influenced during sleep[@sadeh_2015]. PSG gathers electrophysiological data from the brain using a 6-channel EEG, specifically from locations F3, F4, C3, C4, O1, and O2, contrasted against the contralateral mastoids (M1, M2). In addition, it uses electrooculography (EOG) to gauge eye movements, electromyography (EMG) to track chin muscle tone and occasional arm and leg movements, and electrocardiography (ECG) to monitor heart rate. The study is augmented by methods that evaluate respiratory airflow, effort indicators, and peripheral pulse oximetry (PPG)[@ibáñez_2018]. For enhanced data interpretation, an infrared-equipped video camera captures the sleeping subject. Typically, PSG is carried out overnight in a specialized clinical sleep laboratory, assisting in diagnosing various sleep disorders. This technique furnishes detailed insights into an individual's sleep architecture, revealing sleep and wake durations as well as aiding in the classification of sleep stages[@sadeh_2015]. Sleep, as per standard PSG scoring, is classified into four distinct stages: three stages of non-REM (NREM) sleep and one stage of rapid-eye-movement (REM) sleep[@roebuck_2014]. Such detailed data enables accurate clinical research and the diagnosis of various sleep disorders, such as sleep apnea and periodic movements during sleep[@sadeh_2015]. Moreover, PSG proves instrumental in examining excessive daytime sleepiness. Following a PSG-monitored night, individuals undergo a multiple sleep latency test where they're presented with multiple chances to fall asleep throughout the day. Both the time taken to fall asleep (latency) and the characteristics of ensuing sleep stages, as captured by PSG, are evaluated[@sadeh_2015]. While PSG offers an unparalleled depth of sleep data, essential for diagnosing an array of sleep disorders, it comes with its own set of limitations. The procedure can be costly, often restricted to one or two nights in a specialized setting under a technician's supervision. This controlled environment may not truly mirror natural sleep conditions[@sadeh_2015]. Moreover, PSG necessitates specialized personnel to oversee, score, and interpret the data, making it less feasible for expansive or free-living studies[@girschik_validation_2012]. Hence, while invaluable, PSG is predominantly reserved for individuals presenting sleep-related complaints and for the conclusive diagnosis of sleep disorders.

## Sleep Questionnaires and Diaries

Sleep is often initially assessed using a sleep questionnaire. These are cost-effective and quick, making them suitable for first-line diagnosis. They quantify a patient's subjective perception of their sleep quality. While these questionnaires are inherently subjective, they've been validated as accurate in numerous studies[@silva_2011; @el-sayed_2012; @firat_2012; @luo_2014; @pataka_2014]. Typically, medical professionals are not needed to administer these questionnaires; they can be self-completed, even at home. For example, several apps exist that instantly provides a report after questionnaire completion, assisting those with potential sleep issues to seek specialist care. It's vital to understand that not all questionnaires measure the same aspect of sleep. While some gauge sleep quality, others like the FOSQ-10 evaluate sleepiness[@chasens_2009] whereas instruments like the Pittsburgh Sleep Quality Index offers insights into an individual's overall satisfaction with their sleep over a defined time-frame, often a month[@sadeh_2015]. In population studies, self-report sleep assessments are common but flawed. They can exaggerate sleep duration and miss subtle sleep quality details. Their design, summarizing sleep data over weeks, risks recall biases, especially when remembering older sleep patterns[@sadeh_2015]. Factors like weight, ethnicity, and regular sleep duration can influence these self-reports' accuracy[@lauderdale_2008].

Stepping away from these broad self-reports, sleep diaries stand out as a more detailed and structured tool. Often framed as the "gold standard" in subjective sleep assessment, they dive deep into various sleep parameters, like total sleep duration, efficiency, onset latency, and wake periods post sleep onset (WASO). Their strength lies in offering a day-by-day account, making it easier to spot disturbances, ascertain precise sleep timings, and decipher the rhythm of daily sleep-wake patterns over an extended duration[@ibáñez_2018]. However, like all tools, they aren't perfect. Their accuracy hinges on participants' memory retention and commitment to regular, detailed diary entries. From the researchers' standpoint, sifting through these extensive diaries can be time-intensive and, for participants, the process can sometimes be seen as taxing, potentially affecting their consistency in logging entries[@thurman_2018].

## Accelerometry as an Alternative

To bypass the challenges associated with PSG and sleep diaries, accelerometer devices have emerged. These devices offer a practical alternative by capturing sleep patterns based on movement. In healthy individuals, wrist-worn accelerometers have shown good alignment with PSG, especially in estimating total sleep time[@van_de_water_objective_2011]. However, its dependence on movement can occasionally lead to inaccuracies. For example, the device may overestimate sleep in metrics such as LPS, SE, and WASO[@van_de_water_objective_2011]. Its accuracy can vary depending on the accelerometer brand, software, and specific settings used, and may not capture sleep accurately for people who remain motionless while awake, like insomnia sufferers[@lichstein_2006; @taibi_2013], or those with restricted mobility due to illnesses or treatments[@ancoli-israel_1997; @beecroft_2008].

The data from accelerometer devices is typically interpreted through automated algorithms, determining sleep and wake states based on activity. The first such algorithm was crafted in 1982, grounded in linear regression and validated against PSG[@webster_activity-based_1982]. Its 1992 successor, Cole-Kripke, became widely adopted[@cole_automatic_1992]. Due to early technological limits, these devices converted raw motion data into simplified 'activity counts' for storage[@neishabouri_2022]. Traditional algorithms, including Cole-Kripke, then used these counts to analyze sleep[@cole_automatic_1992], examining activity data around particular time-frames and predicting sleep states[@sazonov_activity-based_2004].

The adoption of accelerometer devices has led to an influx of sleep data. Traditional methods of data interpretation, however, may not sufficiently harness the full potential of this vast information. Consequently, the transition towards machine learning becomes imperative, as it offers enhanced capabilities for analyzing and interpreting extensive datasets.

## Why Machine Learning

The availability of large datasets from wearable devices has set the stage for the incorporation of machine learning techniques into health research. These algorithms have the capability to analyze vast and complex data sets, delivering insights that were previously difficult or impossible to discern. Whether it's recognizing subtle patterns indicative of sleep apnea for timely intervention[@cappuccio_sleep_2010], or evaluating the effectiveness of antidepressant medications in patients with sleep disturbances[@paruthi_consensus_2016], machine learning adds a layer of precision and depth to data analysis. These methods have also been integrated into sleep studies due to the growing volume of sleep data. For instance, deep learning models showed enhanced precision over traditional models in the MESA sleep dataset[@lutsey_objectively_2015; @palotti_benchmark_2019]. Technological advancements allowed manufacturers to provide raw acceleration data, facilitating nuanced analyses. Notably, van Hees et al. in 2015[@hees_novel_2015] utilized this raw data to formulate a sleep-wake classification based on the forearm's angle, later improving accuracy with a random forest model that outperformed both its predecessor and established algorithms like Cole-Kripke and Sadeh[@sundararajan_sleep_2021]. In the context of preventive healthcare, which is becoming increasingly focal in today's medical landscape, machine learning can serve as a robust tool for early warning systems. By analyzing regular tracking data of sleep and physical activity, machine learning algorithms can identify potential health risks long before they evolve into severe conditions. For example, sustained inactivity patterns could be predictive of future obesity or heart disease risks[@tremblay_sedentary_2017]. Early detection through such sophisticated data analysis enables timely interventions and preventive measures, further substantiating the role of machine learning in modern health research[@liguori_evolving_2023].

Machine learning emerges from the confluence of statistics, focused on understanding relationships within data, and computer science, which is devoted to the development of efficient computing algorithms[@hastie01statisticallearning]. This interdisciplinary field is particularly fueled by the computational demands of constructing statistical models from large-scale data sets, sometimes comprising billions or even trillions of data points. Within machine learning, various learning types exist, notably supervised and unsupervised learning. However, when examining the impact of machine learning on medicine, it can be helpful to divide tasks into two broad categories: those that medical professionals already excel at, and those where success has been limited. Understanding machine learning through this lens allows us to explore its potential contributions to different areas of medicine, pinpointing where these advanced computational methods can offer significant benefits. While machine learning holds immense promise, the success of these
models, also in sleep research, is intrinsically linked to the
quality of the data they are trained on. This leads us to the pivotal
role of data annotation.

### Importance of Accurate Data Annotation

The utilization of wearable technology in sleep research and other behavioral studies largely depends on the accuracy of the annotated data. Inaccurate or imprecise annotations could significantly impact the effectiveness of machine learning models trained on these datasets. Hence, the need for innovative and reliable annotation methods is more pressing than ever.

Human-in-the-loop

### Challenges in Non-Wear Detection

For wearable physical activity sensors to provide meaningful data, it is crucial to accurately distinguish between wear and non-wear times. Traditional heuristic algorithms have limitations, and thus there is a need to explore machine learning solutions that offer better performance and generalizability.

ensure integrity of the data

### Sensor Placement for Sleep Assessment

While wrist and hip-worn devices are commonly used in sleep research, alternative placements like thigh-worn devices remain relatively unexplored despite their potential advantages. Additionally, traditional methods like polysomnography are impractical for large-scale studies, necessitating the investigation of cost-effective alternatives.

-   Nonwear
-   Sleep

#### Scope and Relevance

-   The need for cost-effective, reliable, and practical alternatives for large-scale studies.
-   The potential of free-living accelerometers, and why they are a compelling subject of study.

#### Existing Challenges

-   Discuss the challenges with existing methods, such as identifying non-wear time, annotating in-bed periods, and classifying awake periods during in-bed time.
-   Address the lack of exploration of certain sensor locations, like the thigh.

#### Thesis Goals and Objectives

-   Clearly state the aim and objectives of your thesis.

    create data -\> ensure integrity of data -\> build models

-   Explain how your thesis will address the identified challenges, including improving the manual annotation of in-bed periods, enhancing non-wear detection, and estimating sleep quality metrics.

#### Overview of the Papers

-   Briefly introduce each paper, highlighting the key research question, methods, and findings.

-   Explain how each paper contributes to your thesis goals and objectives.

### Thesis Structure

Provide an outline of the subsequent chapters of your thesis.

\newpage

# Paper I: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

This segment of the thesis encompasses the methods, results, and discussion for Paper I. The study underscores the importance of effective machine learning algorithms for sleep/wake cycles, which ideally necessitate correct data annotations over a span of 7-10 days. Although sleep diaries or EEG recordings can annotate 'time in bed', many researches predominantly rely on accelerometry. This emphasizes the imperative for enhanced annotation techniques and their validity. Our objective is to introduce a manual annotation method, gauge its precision, and determine its consistency. Some of the details presented here were previously mentioned in the published version of Paper I[@skovgaard_manual_2021].

## Methods

### Study Population

The data for this study was sourced from the SCREENS pilot trial (www.clinicaltrials.gov, NCT03788525), a two-arm parallel-group cluster-randomized trial with two intervention groups, conducted between October 2018 and March 2019[@rasmussen_feasibility_2021; @rasmussen_short-term_2020]. There was no control group in this trial.

Families from the Middelfart municipality in Denmark were approached for participation if they had a child aged between 6 to 10 years living with them, out of a total of 1686 families. To qualify, the parent's screen media usage had to exceed the median of 2.7 hours per day, based on survey responses from 394 respondents. Additionally, all children in the household needed to be older than 3.9 years to ensure that sleep measurements weren't disrupted by the nocturnal awakenings typical of infants or toddlers. For a comprehensive list of inclusion and exclusion criteria, refer to Pedersen et al.[@pedersen_self-administered_2021].

The study ultimately included data from 14 children and 19 adults. These participants weren't advised to alter their sleep or bedtime routines for the interventions. While the study focused on nightly sleep time as recorded by the EEG-based sleep staging system, any napping behavior of the participants was deemed irrelevant.

All data collection procedures were reported to the local data protection department, SDU RIO (ID: 10.391), in compliance with the Danish Data Protection Agency's regulations.

### Actigraphy

Both adults and children participated in 24-hour accelerometry recordings using two triaxial accelerometers, Axivity AX3 (Axivity Ltd., Newcastle upon Tyne, UK). The Axivity AX3 is a compact device, measuring 23 mm × 32.5 mm × 7.6 mm and weighing just 11 g. It was set with a sensitivity of ±8 g and a sampling frequency of 50 Hz.

Participants wore the accelerometers at two specific anatomical locations. The first was positioned on the right hip, secured in a pocket attached to a belt around the waist, ensuring the USB connector faced outward from the body's right side. The second accelerometer was placed midway between the hip and knee on the right thigh, housed in a pocket on a belt, with the USB connector also facing away from the body.

For both the baseline and follow-up, the devices were worn for a duration of one week (seven consecutive days). This duration aligns with the recommended number of days to reliably gauge habitual physical activity[@jaeschke_variability_2018].

### Zmachine® Insight+ Sleep Assessments

Both adults and children were assessed for their sleep patterns using the Zmachine® (ZM) Insight+ model DT-200 (General Sleep Corporation, Cleveland, OH, USA), Firmware version 5.1.0. This assessment was concurrent with the accelerometer recordings. At the baseline, the sleep assessment spanned 3--4 nights, while during the follow-up, it was conducted over 3 nights.

The ZM device operates by measuring sleep through a single-channel EEG, specifically from the differential mastoid (A1--A2) EEG location, evaluated on a 30-second epoch basis. Designed for use in everyday settings, the ZM provides an objective measurement of various sleep parameters, including sleep duration, sleep stage classification, and latency to different sleep stages.

The ZM's algorithm has been benchmarked against polysomnography (PSG) in laboratory settings for both adults with and without chronic sleep issues[@wang_evaluation_2015; @kaplan_performance_2014]. Our findings indicate that the ZM is effectively applicable to both children and adults for multi-day measurements in real-world settings[@hees_novel_2015]. Notably, the device showcased a high accuracy in distinguishing between sleep and wakefulness, with sensitivity, specificity, positive predictive value, and negative predictive values being 95.5%, 92.5%, 98%, and 84.2%, respectively[@kaplan_performance_2014].

For the assessment, three electrodes (Ambu A/S, Ballerup, Denmark, type: N-00-S/25) are positioned on the mastoids (for signal) and the nape (as ground). About half an hour before their intended sleep time, participants' skin areas are cleaned with alcohol swabs, after which the electrodes are affixed. An EEG cable connects these electrodes to the ZM device. A preliminary sensor check ensures all electrodes are correctly mounted; any issues are promptly addressed by replacing the problematic electrodes. Additionally, participants, or parents on behalf of their children, recorded their sleep and wake times daily in a dedicated diary.

### Audacity

Audacity®️ is a distinguished free audio editing software[@audacity]. The genesis of Audacity can be traced back to the fall of 1999, when it emerged as an innovative project led by Dominic Mazzoni and Roger Dannenberg at Carnegie Mellon University. By May 2000, it was unveiled to the world as an open-source audio editor. Since its inception, Audacity has undergone significant evolution. The software, developed collaboratively by the community, now boasts of hundreds of unique features, offers complete support for professional-grade 24-bit and 32-bit audio, has a comprehensive manual available in multiple languages, and has witnessed distribution in the millions. Today, a dedicated team of volunteers from various corners of the globe continues to maintain and enhance Audacity. It is disseminated under the GNU General Public License, granting everyone the freedom to utilize the software for personal, educational, or commercial endeavors.

In the realm of accelerometer data analysis, Audacity stands out. It furnishes researchers with the capability to meticulously scrutinize high-resolution raw accelerometer data with unparalleled precision. Users can quickly zoom in to delve deeper into specific segments of the recording, like certain patterns around bedtime, or zoom out for a broader perspective, such as data spanning a week. Furthermore, Audacity's sophisticated labeling function is pivotal for annotating the accelerometry data. Any generated labels can be preserved in an individual file and later integrated into machine learning algorithms. This level of detailed manual inspection of high-resolution accelerometer data offered by Audacity is, based on our knowledge, unparalleled by any other software.

Within the Audacity interface, there's the possibility of amalgamating over 100 channels of data. This aids in the merging of distinct signal features derived from acceleration. The integration of multiple signal features is intriguing as it might enhance the visual comprehension and classification of inherent behaviors. Nevertheless, an excessive conglomeration of signal features might obscure the precise identification of targeted behaviors. For our study, we incorporated a total of seven distinct signal features. The criteria for classifying "lying" in the first feature are explicit: if the inclination of the hip accelerometer surpasses 65 degrees and the thigh accelerometer simultaneously identifies as "sitting" based on Skotte et al.'s activity type classification algorithm[@skotte_detection_2014]. The other signal features, barring "time", are directly procured from Skotte et al.'s algorithm. These features, delineated in @tbl-man_signal_features, concern the longitudinal axis of the body. Data derived from accelerometry undergoes processing using a window length of two seconds (60 samples) and has a 50% overlap (30 samples), ensuring a resolution of one second. The methodologies from Skotte et al. and those generating the first feature rely exclusively on the accelerometer's inclination(s). Hence, while they can determine time in bed and participant's posture, they aren't precise indicators for pinpointing exact in-bed and out-of-bed moments.

To provide a visual perspective, @fig-screen_full and @fig-screen_night depict the Audacity interface displaying all seven signal features as cataloged in @tbl-man_signal_features. @fig-screen_full offers a glimpse of a week's data, whereas @fig-screen_night zooms into an approximate 24-hour span, showcasing a single annotated night.

\newpage

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_signal_features
#| tbl-cap: Summary of the specific signal features utilized in Audacity for the accurate detection and analysis of in-bed periods.
#| tbl-cap-location: bottom

source("code/tables.R")

tbl_signal_features %>% as_latex()
```

```{=tex}
\endgroup
```
![Screenshot of the Audacity interface showing the seven horizontal panels representing the included signal features. See @tbl-man_signal_features for a detailed description of the features.](figures/audacity_full_view.png){#fig-screen_full}

![Screenshot of the Audacity interface when zoomed in on a single night for the labeling of the in-bed period. The seven horizontal panels represent the included signal features. See @tbl-man_signal_features for a detailed description of features.](figures/audacity_single_night.png){#fig-screen_night}

### Annotation Process Conducted by Expert Raters

Three experienced researchers, well-versed in working with accelerometer data, were chosen as raters. Their proficiency ensured that they had the requisite knowledge to accurately interpret the various data channels presented to them. Each rater meticulously reviewed and labeled each wav-file, marking specific timestamps that indicated in-bed and out-of-bed activities. These annotations were then saved as individual text files. For ensuring consistency and reliability in the annotations, each wav-file underwent two rounds of labeling. Importantly, at no point during this process were the raters privy to any prior annotations, either made by themselves or their colleagues.

### Establishing the Ground Truth Using ZM

The definitive ground truth for in-bed and out-of-bed time frames was gleaned from the sleep staging data derived from the ZM. This was established by identifying the first and last events at night that did not present any sensor-related issues. Nights where the ZM detected sensor problems, either at the onset or conclusion of the recording, were excluded from further consideration. Such sensor issues typically arise due to inadequate attachment of electrodes. To maintain accuracy in data collection, all participants were meticulously instructed to affix the ZM and activate it precisely at their bedtime and to detach it upon waking. These crucial timestamps were then utilized as the ground truth for the study.

### Statistical Analysis

The statistical interpretations for this study were achieved using the R statistical software (version 4.0.2, released on 22 June 2020) and its complementary interface, RStudio (version 1.1.456). For continuous variables, the descriptive attributes were gauged using medians and interquartile ranges. Meanwhile, categorical variables were assessed based on their proportions. To offer a clear distinction, the characteristics for children and adults were presented separately.

The core of the statistical analysis encompassed agreement studies which were executed using the intraclass correlation coefficient (ICC) and the Bland--Altman analysis. Additionally, to provide a comprehensive visual representation of the agreement and symmetry across methodologies, probability density distribution plots were integrated. The ICC, as a metric, goes beyond merely correlating two techniques; it evaluates if they align in magnitude. The scale for interpretation is as follows:

$ICC < 0.5$ signifies poor agreement

$0.5 < ICC > 0.75$ indicates moderate agreement

$0.75 < ICC > 0.9$ represents good agreement

$ICC > 0.90$ underscores excellent agreemen

In this research, the ICC values were interpreted based on their 95% confidence intervals, adhering to recommended guidelines[@koo_guideline_2016]. The Bland--Altman analysis, on the other hand, is a tool to measure the concurrence between two measuring techniques[@bland_measuring_1999]. It calculates the average of the differences (representing bias) between the two methods, and also establishes the limits of this agreement. A positive mean difference suggests an earlier underestimation of the in-bed or out-of-bed timestamp relative to the ZM, while a negative difference indicates a later overestimation.

## Results

Descriptive characteristics of the included subjects of the current study are reported in @tbl-man_describe.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_describe
#| tbl-cap: "Descriptive characteristics of the study participants. ISCE: International Standard Classification of Education"

tbl_man_describe
```

```{=tex}
\endgroup
```
### Intraclass Correlation Coefficient Analyses

The analyses of Intra-Class Correlation (ICC) underscored an exemplary consistency when comparing ZM's automatic in-bed annotations with manual annotations. This was evident in both metrics assessed: time to bed and time out of bed. The high agreement was consistent across both evaluation phases---Round 1 and Round 2---and was observed in the initial baseline as well as the subsequent follow-up assessments. Reinforcing the robustness of these findings, the lower bounds of the confidence intervals consistently remained above 0.9, suggesting a high degree of reliability in the agreement. For a detailed breakdown of these results, please refer to @tbl-man_icc_zm_man.

\newpage

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_man
#| tbl-cap: Intraclass correlation coefficients between ZM and the average of the manual annotations between the three raters.

tbl_icc_zm_man
```

```{=tex}
\endgroup
```
In our analysis, we also identified a remarkable consistency between the data from self-reports and the ZM measurements. This high level of agreement was evident in both the initial baseline data as well as the subsequent follow-up data. The strength of this agreement is underscored by the fact that the lower limit of the 95% confidence interval never fell below a value of 0.94. For a detailed representation of this, please refer to @tbl-man_icc_zm_self.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_self
#| tbl-cap: Intraclass correlation coefficients between self-report and ZM.

tbl_icc_self_zm
```

```{=tex}
\endgroup
```
Our analysis evaluated the agreement among three manual raters in annotating timestamps for both 'to bed' and 'out of bed' events. The Intraclass Correlation Coefficients (ICCs) from this evaluation demonstrated strong consensus among the raters. Specifically, the lower bounds of the 95% confidence intervals were consistently strong, never dropping below 0.88, highlighting both good and excellent agreement levels. However, when analyzing the ICCs more closely, subtle variations appear between the 'to bed' and 'out of bed' timestamps. This nuanced observation is detailed in @tbl-man_icc_man_man.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_man_man
#| tbl-cap: Intraclass correlation coefficients between manual raters.

tbl_icc_man_man
```

```{=tex}
\endgroup
```
The test-retest reliability exhibited good to excellent ICC agreement for each rater between the first and second rounds, applicable to both baseline and follow-up data (refer to @tbl-man_icc_test_retest). This robust reliability is further evidenced by the 95% confidence intervals having lower limits not less than 0.86. While the ICCs across raters are broadly consistent, there's a distinction: Raters 1 and 3 had slightly reduced agreement in their baseline to-bed annotations relative to subsequent ones. In contrast, Rater 2's ICC scores remained stable and didn't reflect this trend.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_test_retest
#| tbl-cap: Test–retest intraclass correlation coefficients between the first and second round of manual annotations.

tbl_icc_test_retest
```

```{=tex}
\endgroup
```
### Bland-Altman Analyses

@tbl-7 presents the bias and its corresponding confidence intervals, as well as the upper and lower limits of agreement, comparing manual annotation and self-report to ZM. The bias for manual annotation relative to ZM ranges from -6 minutes to 5 minutes. In contrast, the self-report exhibits a slightly smaller bias when compared to ZM. Notably, the magnitude of the limits of agreement appears consistent across both methods of comparison.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-7
#| tbl-cap: Bland–Altman analysis was conducted to assess the inter-method agreement. This analysis compared manual annotation to ZM and also compared self-report to ZM. All the measurements in the analysis are presented in minutes.

tbl_7

```

```{=tex}
\endgroup
```
### Density Plots

@fig-ridge_plot presents the probability density distribution of the differences between the "to-bed" and "out-of-bed" scorings, comparing manual annotations and self-reports to ZM. These plots offer a visual illustration of the bias and spread arond zero, showcasing how manual annotations and self-reports diverge from ZM, as previously highlighted in[@van_hees_estimating_2018].

![Probability density distributions for differences between manual in-bed annotations and self-report compared to ZM.](figures/paper1_ridge_plot.pdf){#fig-ridge_plot}

## Discussion

In this study, we introduced a methodology for manually annotating periods spent in bed using accelerometry data. The accuracy of this method was rigorously evaluated through multiple raters and then compared to sleep assessments made with EEG-based Zmachine Insight+® (ZM) and self-reported sleep data. When examining the lower limit of the 95% confidence interval in Intraclass Correlation Coefficient (ICC) analyses, we found several noteworthy results. First, the method exhibited good-to-excellent interrater reliability. Second, intra-rater reliability also showed good to excellent agreement across all three raters between their first and second rounds of annotations. Third, when compared to ZM, the average manual in-bed annotations from all raters ranged from good to excellent in terms of agreement. Fourth, the self-reported in-bed timestamps were also in good to excellent agreement with ZM. Additionally, Bland-Altman analysis indicated that the mean bias between both the manual annotations and self-reported sleep times compared to ZM was within a range of ±6 minutes, with Limits of Agreement (LOA) not exceeding ±45 minutes. Probability density distribution plots further substantiated these findings, showing comparable symmetry, spread around zero, and positioning of outliers when the manual annotations and self-reported sleep times were compared to ZM.

The high accuracy observed in the prospective sleep diaries in this study can be attributed to their being synchronized with Zmachine Insight+® (ZM). Having participants manually start and end ZM recordings every morning and evening enhances their ability to accurately recall their times of going to bed and getting out of bed. This minimizes the usual discrepancies often seen between objectively and subjectively measured sleep durations[@aili_reliability_2017]. If participants had been instructed to log their sleep using only subjective measures without these protocol anchors, we would not expect to see such strong agreement between the manual annotations, sleep diaries, and ZM data.

Compared to ZM, our study found that the manual annotation of in-bed periods was more prone to errors when estimating the time of going to bed. This issue mainly arose from raters' difficulties in differentiating between inactive behaviors before sleep and actual sleep time. Despite this significant limitation, the manual annotation method displayed reassuring accuracy, especially considering the limited formal training provided to the raters. This ease of use was aided by the specific signal features we selected for study in Audacity, as evidenced by the excellent ICC scores among raters.

Interestingly, our findings suggest a learning curve for the raters, as evidenced by the narrower LOAs and the density plots in the second round of manual scoring. These indicators suggest that additional rounds of scoring could further improve result consistency or that preliminary training could be beneficial for the raters. Consequently, future research should explore methods to optimize the uniformity of these manual annotations.

While there are various tools available for annotating time series data, such as Label Studio[@label_studio] and Visplore[@visplore], our study found that Audacity was particularly well-suited for the task at hand. Label Studio, for example, may struggle with handling week-long accelerometer data consisting of over 100 million entries, whereas Audacity excels in managing and navigating such large data sets. Our feature selection was intentionally designed to avoid overwhelming the raters with redundant information, opting for a limited but effective combination of features based on domain knowledge. This approach could be adapted for annotating other behaviors, like walking, though that would necessitate a different set of features.

The raters in this study gained valuable insights despite the absence of explicit guidelines for data annotation, highlighting the intuitive nature of the method. It's important to note that labeling data inherently involves a certain level of understanding of human behavior. If such labels could be accurately determined based on a set of formal rules, it raises the question of whether training an AI model would even be necessary. Therefore, we recommend additional research to identify the most critical features for successful manual annotations. Examining the impact of varying feature sets could yield further insights that would streamline the manual annotation of accelerometer time series data.

To date, many studies comparing actigraphy and self-report methods to polysomnography (PSG) or EEG-based methodologies have focused primarily on evaluating aggregate sleep parameters. These include total sleep time, wake after sleep onset, sleep latency, and sleep efficiency. These aggregate measures incorporate both sleep onset and wake onset times, analogous to the "to-bed" and "out-of-bed" timestamps used in our current study. However, the precision of these specific time points is rarely assessed, making direct comparisons with our study's measurements challenging.

Our novel method for annotating time in bed has produced Intraclass Correlation Coefficient (ICC) values that are on par with, or even better than, previous studies that compared actigraphy sleep parameters to PSG[@haghayegh_application_2020; @yavuz-kodat_2019]. One such study cited mean absolute errors of 39.9 minutes for sleep onset time and 29.9 minutes for wake-up time, with 95% limits of agreement exceeding ±3 hours when comparing an algorithm to PSG[@van_hees_estimating_2018]. In contrast, our methodology allows for a more precise estimation of specific timestamps rather than durations of behaviors, resulting in less room for error and better agreements.

Moreover, our manual annotation method demonstrated robust performance across various age and gender groups. The study sample included both children and adults of both genders, suggesting that our approach is accurate regardless of the specific behaviors associated with different developmental age groups and genders. This finding also indicates that our manual annotation method may offer higher precision in estimating exact time points compared to existing automated methodologies. Given that the accuracy of sleep parameter assessments is often highly contingent on the target population, the results of our study have promising implications for their broad generalizability, particularly among populations of normal sleepers.

Identifying sleep periods as opposed to merely lying down is a critical aspect of 24-hour behavior profiling. Traditional studies on sleep detection often rely on participants to self-report their time in bed, sleep onset, and wake-up times[@littner_2003; @lockley_1999; @girschik_validation_2012]. However, the manual annotation methodology using Audacity offers an alternative that not only reduces the burden on participants but also mitigates the recall bias inherent in self-reported measures. This method can be easily applied to free-living data, making it incredibly versatile for various applications beyond sleep detection.

For instance, the methodology is useful for annotating non-wear time, manually synchronizing clocks across different devices, and validating raw data. Its applicability also extends to multi-channel data, providing a comprehensive overview that can incorporate variables like orientation from gyroscopic data, temperature, battery voltage, and light. Audacity stands out for its capability to handle large multi-channel data files effortlessly. Researchers can quickly zoom to any resolution and scroll through time without experiencing lag, which makes it an ideal tool for adding labels. This fluidity in workflow suggests that Audacity could become a standard tool for researchers working with raw data and machine learning applications. Thus, the incorporation of this Audacity-based methodology into raw accelerometer data annotation is poised to significantly contribute to future human behavior research.

For years, the transition from raw sensor data to operational predictive models has relied on labeled data. Despite this, no previous research has offered a methodology that allows researchers to optimally utilize their available accelerometry data. Our study demonstrates that with a judicious selection of features, manual annotation for identifying sleep periods can yield results comparable to those achieved with EEG-based sleep classification hardware. However, it's crucial to clarify that we are not advocating for our manual annotation method to replace more established techniques for sleep estimation, such as EEG or tracheal-sound-based methods, in ongoing studies. Instead, our methodology can be valuable as a post-hoc procedure to enrich existing datasets with an additional measure of sleep.

This study boasts several strengths, notably the continuous, multi-day data collection of accelerometry, sleep diary, and ZM recordings carried out in participants' homes, which provides high-quality free-living data. However, the study is not without limitations. One such limitation pertains to rater generalizability. The three manual raters were fixed, not selected randomly from a broader pool of eligible raters with varying characteristics. Nevertheless, given the minimal pre-briefing instructions for labeling raw data, we believe this methodology could be generalizable to other researchers working with accelerometer data.

A logical next step in this research would be to create and validate a standardized procedure for manual sleep annotation, akin to what is available for EEG-based sleep annotation in the AASM Scoring Manual[@aasm]. This would make the methodology more accessible to individuals with limited experience in the field of accelerometry. Another concern is the challenge of recording true free-living behavior using participant-mounted devices like ZM, as wearing such a device during sleep could affect participants' natural behavior, thereby posing a study limitation.

Moreover, although the criterion measure in our study has been validated against PSG, utilizing PSG as the criterion measure would have been more optimal. Finally, the study did not consider napping behavior; its focus was solely on sleep as it relates to circadian rhythms. As such, future research is required to validate the utility of this manual annotation methodology for detecting naps.

### Conclusions

In summing up, our study demonstrates that the use of Audacity for manually annotating in-bed periods based on thigh- and hip-worn accelerometer data aligns well with objective EEG-based sleep device estimates and prospective sleep diaries. Our findings reveal minimal mean bias and acceptable limits of agreement when comparing time to bed and time out of bed across these different methods. Additionally, the manual annotation process proved highly reliable, exhibiting excellent inter- and intra-rater agreement. Its accuracy in relation to EEG-based assessments was also comparable to that of sleep diaries. Importantly, the manual annotation method can be applied to pre-existing raw data that may not have accompanying sleep records. This offers a significant advantage for making better use of free-living data resources. The increased availability of such annotated data can be particularly beneficial when training data-intensive machine learning algorithms, potentially enhancing their generalizability in objectively assessing human behavior.

\newpage

# Paper II: Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings

This segment of the thesis encompasses the methods, results, and discussion for Paper II. Wearable sensors, commonly used to track physical activity, face challenges in detecting "non-wear" times. While traditional methods use fixed interval heuristic algorithms, this study explored decision trees that use raw acceleration and skin temperature data. By training on thigh- and hip-worn devices from 64 children and validating with wrist-worn data from 42 adolescents, results showed traditional methods excel for non-wear durations over 60 minutes. However, for durations below 60 minutes, decision tree models, especially those with the top six predictors, were superior. The study emphasizes method selection's significance and promotes external validation for machine learning models in this area. Some of the details presented here were previously mentioned in the published version of Paper I[@skovgaard_generalizability_2023].

## Methods

The classification and generalizability of non-wear classification methods were evaluated using free-living data collected from accelerometers positioned on the wrist, thigh, and hip. This data amalgamated findings from the Physical Activity in Schools After the Reform (PHASAR)[@pedersen_protocol_2018] study, which provided hip and thigh data, with an in-house validation study that used wrist-worn accelerometers. @fig-paper2_flowchart illustrates the data utilization process. By leveraging these datasets, we ensured that established non-wear classification methods underwent assessment on an external dataset. Moreover, our decision tree models were tested on an independent external dataset, including wear locations not initially considered during model development. Therefore, all machine-learned models underwent evaluation using test data from various anatomical positions, regardless of their inclusion in the model's initial development.

### Reference Methods

To thoroughly evaluate and compare the performance of our three newly developed algorithms, we incorporated four additional non-wear classification methods. The selection of these existing methods was grounded in our aim to span a broad range of methodological flexibility. We specifically targeted techniques ranging from the most simplistic and commonly used to the most recent and sophisticated.

1.  \textsf{\textbf{Consecutive Zeros-Algorithm (cz\_60):}} Over the years, there have been various consecutive zero-algorithms designed for accelerometer data, with the aim of identifying non-wear periods within stipulated timeframes, such as 30-, 60-, or 90-minute intervals[@hecht_methodology_2009; @troiano_physical_2008; @choi_validation_2011]. In addition, van Hees and colleagues have developed non-wear algorithms for raw acceleration using a 30-minute interval[@van_hees_estimation_2011]. They later expanded this approach to include a 60-minute interval[@rasmussen_short-term_2020]. Another method utilizes a 135-minute interval with adjusted hyperparameters, as introduced by Syed et al.[@syed_evaluating_2020]. In our study, we adopted a straightforward approach to this concept. Using Actigraphy counts, we identified periods of no movement that registered zero counts for at least 60 continuous minutes. Notably, these Actigraphy counts operate with a deadband set at 68 mg, which denotes the minimum detectable acceleration threshold.

2.  \textsf{\textbf{Heuristic Algorithm (heu\_alg):}} As detailed by Rasmussen and colleagues[@rasmussen_short-term_2020], this algorithm merges raw acceleration data with surface skin temperature measurements. Non-wear time is determined for periods surpassing 120 minutes with accelerations less than 20 mg. For durations between 45 to 120 minutes, non-wear is identified if the temperature falls below a personalized non-moving temperature threshold. Additionally, the algorithm can spot non-wear periods ranging from 10 to 45 minutes, but only if these intervals end within the anticipated awake hours.

3.  \textsf{\textbf{Random Forests Model(sunda\_RF):}} Sundararajan et al.[@sundararajan_sleep_2021] delineated a non-wear classification technique grounded in a random forest ensemble model. This model was informed by raw accelerometer data derived from 134 participants aged between 20 to 70 years. These subjects were fitted with an accelerometer on their wrist for a singular overnight PSG session. The verifiable labels for non-wear periods were anchored in the assumption that the accelerometer was donned only during the PSG. Any epoch with a standard deviation in the acceleration signal exceeding 13.0 mg outside the PSG was classified as wear time. The model utilized 36 predictors, and a nested cross-validation method was employed both to ascertain the model's generalization capability and to refine its hyperparameters.

4.  \textsf{\textbf{Deep Convolutional Neural Network (syed\_CNN):}} This method, introduced by Syed et al.[@syed_novel_2021], employs a unique approach. It's built upon a deep convolutional neural network (CNN) that diverges from traditional techniques. Initially, all potential non-wear episodes are discerned using a standard deviation threshold. However, instead of scrutinizing the acceleration within these intervals, the focus shifts to the signal shape of the raw acceleration immediately before and after a non-wear episode. Through the CNN, the method discerns non-wear periods by detecting the moments when the accelerometer is removed and reattached. For our study's purposes, we chose a window length of 10 seconds on each side of the identified non-wear episode, as this yielded the most accurate results. The training dataset that informed the CNN consisted of data from hip-mounted accelerometers worn by 583 participants. These individuals ranged in age from 40 to 84 years, with an average age of 62.74 and a standard deviation of 10.25.

### Data Collection and Devices Used

Both the PHASAR study and the in-house validation research utilized the Axivity AX3 accelerometer (Axivity Ltd., Newcastle upon Tyne, UK) to record raw acceleration data along with surface skin temperature. The device, weighing a mere 11 g and with dimensions of 23 mm × 32.5 mm × 7.6 mm, measures acceleration in gravity units (g) across three axes (vertical, mediolateral, and anteroposterior). The sampling frequencies were set at 50 Hz for the PHASAR study and 25 Hz for the in-house study. However, all recorded data from both studies were uniformly resampled to 30 Hz.

### Description of the Studies

*PHASAR Study:* The PHASAR study involved a representative sample of over 2000 school-aged children from 31 public schools in Denmark. The study, conducted between 2017 and 2018, captured data from 1,315 boys (49%) and 1,358 girls (51%), aged between 8.1 to 17.9 years (mean age = 12.14, SD = 2.40). Accelerometers were placed at two specific anatomical sites: the right hip and midway on the right thigh. They were worn for a recommended seven consecutive days to reliably estimate habitual physical activity. For this analysis, data from 64 randomly selected participants from the PHASAR cohort were used. A dataset indicating genuine non-wear time was created via manual annotation, a method elaborated in another publication. Essentially, non-wear periods were determined by visually examining raw accelerations coupled with skin temperature readings. True non-wear episodes with specific start and end times were manually labelled in each dataset and were utilized as reference labels in subsequent analyses.

*In-House Validation Study:* This study consisted of accelerometer data from 42 youth athletes, evenly split between boys and girls, aged 14.5 to 16.4 years (mean age = 15.4, SD = 0.37 years). These athletes, part of a specialized talent program in the Region of Southern Denmark, wore the Axivity accelerometer on their non-dominant wrist for 14 consecutive days. This study was initiated in the spring of 2021. A dataset mirroring the one from the PHASAR study was created, including all 42 participants.

### Ethical Considerations

The PHASAR study was reviewed by the Regional Committee on Health Research Ethics for Southern Denmark (ID: S-20170031) and was determined not to require an ethics review, as per Danish regulations, which mandate only biomedical research or risk-involved studies to undergo a formal ethics review. Documentation regarding this decision is available upon request from the principal author. Conversely, the in-house validation study received an ethical approval waiver from the Research & Innovation Organization and the legal department of the University of Southern Denmark. All participants, or their legal guardians, provided written informed consent for both studies, which adhered to the Danish Data Protection Agency (2015-57-0008) standards and globally recognized guidelines like the Declaration of Helsinki.

![The flowchart depicts the division of the PHASAR dataset into training and testing segments. On the left, boxes signify 79.2% of the PHASAR data designated for training across five-fold resamples. On the right, the yellow and blue boxes collectively represent 20.2% of the PHASAR data, specifically delineating the hip and thigh data for testing. The green box represents our separate in-house test dataset, which was gathered from wrist-worn devices.](figures/paper2_flowchart.pdf){#fig-paper2_flowchart}

### Development of Decision Tree Models:

For our decision tree models, we sourced 12 predictors from the raw PHASAR accelerometer data, which encompassed elements like temperature, time of day, indicators for device placement, day of the week, and moving average statistics (detailed in @tbl-8). These moving average metrics were collated in 10-second increments. To train the model, we utilized 79.2% of the PHASAR data, incorporating data from both hip- and thigh-worn devices (as shown in @fig-paper2_flowchart). A critical aspect of our methodology was the data partitioning: we made certain that data from individual participants was exclusively allocated to either the training or test datasets. This strategy was crucial in ensuring that the model could effectively generalize to unfamiliar data, rather than overfitting to specific participant data. During the tuning phase, to boost model accuracy and avoid overfitting, we opted for a five-fold cross-validation approach. This process entailed refining several hyperparameters, such as the tree's depth, its cost-complexity, and the minimum amount of data points necessary in a node for it to split further. To effectively explore the hyperparameter space, we employed Latin hypercube sampling. This method systematically divides the parameter range into segments, randomly drawing a value from each segment, resulting in a well-distributed set of parameter combinations. In our case, we established a 10-level parameter grid, guaranteeing a comprehensive exploration of the hyperparameter space.

Following this procedure, we introduced three distinct model variations:

1.  A full-scope model (*tree_full*) incorporating every predictor.

2.  A refined model (*tree_imp6*) centered on the six most crucial predictors, as established by permutation predictor importance (@fig-importance).

3.  A model excluding surface skin temperature data (*tree_no_temp*).

In sum, our methodology generated 50 distinct models for each decision tree variant. Given our data's distribution - 55.8% wear time compared to 44.2% non-wear time - there was no need to adopt synthetic minority oversampling methods like SMOTE or other balancing techniques.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-8
#| tbl-cap: Predictors derived from the raw sensor signals.

tbl_8

```

```{=tex}
\endgroup
```
![Permutation importance plot depicting the significance of predictors in the decision tree. The top six predictors informed the tree_imp6 model, while a third model, tree_no_temp, was trained using all predictors except temperature.](figures/paper2_vip.pdf){#fig-importance}

\newpage

### Statistics

We assessed the classification performance to each ground truth test dataset, which combined consisted of over 7 million 10-second epochs from 104 distinct subjects. True positives (TP) represent correctly identified non-wear time, and true negatives (TN) denote correctly identified wear time. Both TPs and TNs are vital for the algorithm's accuracy. These correct classifications are vital for the high accuracy of our non-wear time algorithm. Misclassifications, where non-wear time is labeled as wear and vice-versa, resulted in False Negatives (FN) and False Positives (FP). To determine these classifications, we analyzed the acceleration data in 10-second intervals, comparing inferred results with ground truth labels. This comparison allowed us to construct a confusion matrix. We then computed the overall accuracy, sensitivity, precision, and F1-score to evaluate the effectiveness of each non-wear detection method. Notably, a high F1-score, which is the harmonic mean of precision and sensitivity, indicates superior classification performance. We further delved into the permutation predictor importance to discern the factors behind the enhanced performance of our decision tree models. All our analyses and model developments were conducted using R (version 4.1.2, Bird Hippie) and RStudio (version 2021.9.1.372, Ghost Orchid), with Tidymodels for machine learning and the rpart package serving as the decision tree algorithm engine.

$$accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$ $$sensitivity = \frac{TP}{TP+FN}$$ $$precision = \frac{TP}{TP+FP}$$ $$F_1 = 2 \cdot \frac{precision \cdot sensitivity}{precision + sensitivity}$$

The F1-score, which is the harmonic mean of precision and sensitivity, provides an indicator of the classification performance. A high F1-score suggests commendable classification prowess.

Additionally, we delved into the permutation predictor importance to discern what factors contributed to the superior performance of certain decision tree models.

For all analytical processes and model development, we utilized R (version 4.1.2, Bird Hippie) and RStudio (version 2021.9.1.372, Ghost Orchid). The machine learning tasks were primarily facilitated by the Tidymodels[@kuhn_tidymodels_2020] suite of packages, and we used the rpart[@rpart] package as the engine for our decision tree algorithms.

## Results

In our gold standard datasets spanning three wear locations, there were 1,598 non-wear time episodes. Of these, 1,148 episodes (or 71.8%) lasted 60 minutes or more, with an average duration of about 13 hours (794 minutes with a standard deviation of 1,142 minutes). In contrast, episodes lasting 60 minutes or less made up 28.2% (450 episodes) with an average duration of 26.4 minutes (SD = 16.4). Interestingly, the briefest episodes (less than 60 minutes) made up just 1.3% of the total non-wear time across all wear locations (refer to @tbl-9). @fig-paper2_nw_dists depicts the frequency distribution for episodes shorter than 60 minutes and those 60 minutes or longer. The PHASAR dataset showed a bimodal distribution for shorter episodes, with longer episodes peaking around 10 hours. For the in-house wrist-worn dataset, shorter episodes displayed a uniform distribution, while longer episodes were significantly right-skewed.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-9
#| tbl-cap: Overview of non-wear episodes grouped in short and long non-wear episodes. ¹ Aggregated in minutes. ² Proportion of total non-wear time by wear location.

tbl_9
```

```{=tex}
\endgroup
```
![Distribution of the length of the non-wear episodes across hip, thigh, and wrist data. Distributions are shown for episodes shorter than 60 min (binwidth = 1 minute) and longer than 60 min (binwidth = 1 hour).](figures/paper2_plot_nw_dists.pdf){#fig-paper2_nw_dists}

### Classification Performance

In assessing classification performance, @fig-paper2_preds_ex visually contrasts the results from machine-learned models and rule-based algorithms against the ground truth non-wear time, which is highlighted with a light blue background. This visualization underscores that while tree-based models tend to be precise, they can also be unpredictable. On the other hand, threshold-based methods, such as Syed_CNN, heu_alg, and cz_60, offer more consistency. Notably, both cz_60 and heu_alg algorithms fall short in identifying shorter non-wear episodes.

Detailing further, @fig-paper2_performance_all compiles performance metrics from all methods evaluated in this study. The CNN model by Syed et al. demonstrated consistency across three datasets, achieving an overall accuracy between 75% and 80%. It stood out with a sensitivity score between 93% and 96%. However, its F1 scores, which hovered between 82% and 84%, were hampered by only average precision. Conversely, the random forests model by Sundararajan et al. shone with wrist data, boasting an F1 score of 94% and accuracy of 93%. Still, its performance dwindled with hip and thigh data, marking an overall accuracy of 56% and precision of 59%. This drop suggests a significant number of false positives.

Among the decision tree models, the variant excluding surface skin temperature as a predictor fared the poorest for wrist data, achieving a mere 72% accuracy. While it secured a high sensitivity score of 98%, its subpar precision dragged its F1 score down to 81%. The other two decision tree models---one incorporating the six most critical predictors and the other using all predictors---consistently performed well across metrics and datasets. Remarkably, both the heu_alg and cz_60 algorithms approached perfection across evaluations.

Further, @fig-paper2_performance_short zeroes in on performance metrics for episodes 60 minutes or shorter. The consecutive zeros algorithm was unable to detect any non-wear, a result absent from the figure. Syed et al.'s deep learning model underperformed, detecting a mere 1--2% of all non-wear time, leading to F1 scores below 5%. Although the heu_alg algorithm boasted high precision, its lackluster sensitivity resulted in F1 scores spanning from 12% to 16% across wear locations. The random forest model displayed average results for thigh and wrist data but faltered with hip data, recording F1 scores of 46%, 57%, and 8%, respectively. Among the trio of decision tree models, the one leveraging the six pivotal predictors outshone the rest, with F1 scores between 72% and 79%. Meanwhile, the decision tree model encompassing all predictors faced challenges with hip data due to a 23% sensitivity score. Excluding the surface skin temperature, another decision tree model exhibited commendable precision; however, its low sensitivity culminated in F1 scores ranging from 45% to 57%.

![Visual example of the output of non-wear detection models and algorithms for a random person from the in-house wrist dataset (14 consecutive days). The grey shade is ground-truth non-wear time. Syed_CNN, cz_60, and tree_full are vertically offset for easier interpretation.](figures/paper2_plot_preds_example.pdf){#fig-paper2_preds_ex}

![Classification performance metrics on all non-wear episodes for the seven included methods for classifying non-wear time. Metrics are shown for the three different ground-truth dataset including hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_all.pdf){#fig-paper2_performance_all}

![Classification performance for episodes no longer than 60 min in length. Metrics are shown for the three different gold-standard dataset including hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_short.pdf){#fig-paper2_performance_short}

## Discussion

In our study, we evaluated various methods for classifying non-wear episodes in accelerometer data, focusing on episodes longer than 60 minutes and those shorter than 60 minutes. Our findings showed that the simplest methods, specifically cz_60 and heu_alg, excelled in identifying non-wear episodes longer than 60 minutes across all three sensor wear locations: wrist, hip, and thigh. They were closely followed in performance by decision tree models that included surface skin temperature as a predictive variable. On the other hand, the random forest model demonstrated excellent performance only on the wrist, delivering mediocre results on the hip and thigh. When we shifted our focus to short non-wear episodes lasting less than 60 minutes, we found limitations in the cz_60 and heu_alg algorithms due to their built-in minimum episode durations of 60 and 20 minutes, respectively. As a result, their performance was poor for these shorter episodes. Similarly, the deep learning model showed poor results, mainly attributable to a low sensitivity score that led to many episodes being misclassified as non-wear time. The random forest model's performance was also poor on the hip and only mediocre on the thigh and wrist. Decision tree models, both without temperature and with all predictors, showed mediocre performance as well. However, a decision tree model trained on the six most important predictors stood out as the best performer for short non-wear episodes. Our study also highlighted the value of incorporating surface skin temperature as a predictor to enhance the performance of non-wear time classification. Overall, these results provide valuable insights into the effectiveness of various methods for classifying non-wear episodes in accelerometer data, emphasizing the potential of simple algorithms like cz_60 and heu_alg, especially for longer episodes, and the benefit of including surface skin temperature as a predictive variable.

We discovered that most non-wear episodes in our ground truth datasets had a duration exceeding 60 minutes, with a noticeable peak around the 10-hour mark. This finding contrasts with previous research that typically reported shorter episodes as being more prevalent[@aadland_comparison_2018; @jaeschke_variability_2018; @hutto_identifying_2013]. Our data prominently features children and physically active adolescents, a demographic known to spend less time in sedentary activities and to frequently interrupt such periods[@cooper_objectively_2015; @kwon_breaks_2012]. This likely contributed to both the longer non-wear episodes and clarified the differentiation between sedentary behavior and non-wear time in our study. The data favored simple heuristic algorithms for classifying non-wear time, largely because the limitations imposed by minimum window lengths had a negligible impact on the proportion of non-wear time that was incorrectly classified. These algorithms achieved excellent precision scores, confirming that neither sedentary time nor sleep was misclassified as non-wear time. This is a significant finding, given that multiple previous studies have pointed out the complexities in making this very distinction[@troiano_physical_2008; @duncan_wear-time_2018; @choi_validation_2011; @doherty_large_2017; @barouni_ambulatory_2020]. Our study suggests that a consecutive zeros algorithm could be deemed best practice for capturing non-wear episodes lasting over 60 minutes in children and adolescents. This recommendation is applicable across the various wear locations that we evaluated, including the hip, thigh, and wrist. However, it's important to consider that the specific behaviors of children and adolescents may not make these findings directly transferable to older adults. Yet, certain standardized procedures like the syed_CNN model for mounting and unmounting accelerometers might have more universal applicability.

Creating a model to classify non-wear time appears to be a relatively straightforward task, likely because the decision boundary involved is close to linear. In this context, the use of complex models, as we've included in our current study, may lead to overfitting. This overfitting would capture random variations specific to the training dataset, thereby reducing the model's ability to generalize to new, unseen data. Consequently, we hypothesize that a well-optimized logistic regression model could perform just as well as the more intricate methodologies we've tested. The reason for this is that a logistic regression model would establish a separating linear hyperplane capable of distinguishing between wear and non-wear time effectively. Therefore, employing highly non-linear models for this classification task might be an unnecessary complication, particularly if the goal is to develop a machine learning model that can be applied across diverse populations and wear locations. It is also crucial to enrich the training data with multiple wear locations and various physical activity profiles to improve generalizability.

Incorporating surface skin temperature for the classification of non-wear time has been minimally explored in the realm of machine learning. One study did indicate that using acceleration data along with rate-of-change in surface skin temperature could create a robust decision tree model for detecting non-wear time[@vert_detecting_2022]. This aligns with previous heuristic studies that have shown improved predictive performance when temperature data is included[@duncan_wear-time_2018; @zhou_classification_2015]. Our own findings also corroborate this, as we observed that adding surface skin temperature as a variable significantly enhances the performance of the non-wear time model. However, a critical consideration is the precise detection of the transition between wear and non-wear periods, especially given the slow temperature step response time of sensors. Solely relying on temperature data could introduce classification delays if the sensor's response time is sluggish. Combining both temperature and acceleration data is therefore a more effective approach[@zhou_classification_2015]. During our study, we noted a 20-minute step response in the Axivity temperature sensor, which could be attributed to the design of the device's casing. The sensor's response time may also be influenced by the attachment method used. If more material is placed between the skin and the device, delays are likely to occur, suggesting that machine learning models should perhaps consider the type of sensor attachment in their algorithms. Additionally, different brands of devices have been found to have varying optimal temperature thresholds, further complicating the issue. As noted by Duncan et al. and Zhou et al., algorithmic modifications are needed for devices to function optimally in different latitudes[@duncan_wear-time_2018; @zhou_classification_2015]. Therefore, the type of device and its attachment method can be critical variables for improving the accuracy of non-wear time classification models.

In the study of accelerometry data processing, the ideal scenario is to employ a single model that performs reliably across different wear locations and populations. To evaluate the generalizability and robustness of the methods used in our study, we included a dataset from wrist-worn devices for external validation. This ensures that the performance metrics of our decision tree models are not artificially inflated due to overfitting or lack of variance between the training and testing data. External validation involves testing a model with independently sourced datasets to confirm its performance. If a predictor set has been inaccurately selected due to characteristics inherent to the training data, such as technical or sampling bias, it is likely to perform poorly during external validation[@steyerberg_prediction_2016]. The rationale behind using external validation is strong: while data from different sources may have fewer similarities, they can nonetheless capture important domain-relevant information. A model trained to identify truly informative predictors will maintain its performance even when exposed to new data. Therefore, the external validation in our study acts as a verification step, ensuring that our decision tree models that pass this criterion are not just robust but also likely to be interpretable within the domain[@altman_prognosis_2009]. While Syed et al.'s methodology for identifying non-wear time is innovative and logically coherent, we believe its performance may vary depending on the age of the population in the dataset. The approach by Syed et al. focuses on identifying the specific shape of the acceleration signal at the start and end of a non-wear episode. In contrast, methods that simply identify non-wear time based on the absence of acceleration are less dependent on the characteristics of the population, since zero movement during non-wear is a universal trait.

Our results support this idea. The Convolutional Neural Network (CNN) model developed by Syed et al. showed poor performance across all wear locations in our study. One possible reason for this could be the age differences in the populations of the datasets. The original model was trained on an older population, aged between 40-84 years (mean = 62.74, SD = 10.25), whereas our study involved datasets of younger individuals aged 8.1-17.9 years (mean = 12.14, SD = 2.40) for hip and thigh data, and 14.5-16.4 years (mean = 15.4, SD = 0.37) for wrist data. Contrastingly, the sunda_RF model showed acceptable performance in identifying non-wear episodes shorter than 60 minutes on both the thigh and wrist data. This suggests that the model by Sundararajan et al. is less affected by population characteristics, as anticipated, compared to the syed_CNN model. Another point worth noting is that the syed_CNN model was originally trained on data with a frequency of 100 Hz, while we applied it to data with frequencies of 50 Hz and 25 Hz. Although it's unclear whether this frequency difference impacted the model's performance, we believe that the 25 Hz data is sufficient for capturing true movement behavior, given that movement frequencies are generally below 5 Hz.

While it's standard to evaluate a machine learning model using a test split from the same dataset used for training, known as internal validation, this approach has its limitations. For instance, Syed et al. and Sundararajan et al. report high metrics like sensitivity, specificity, and accuracy for classifying non-wear time, but these results are derived from cross-validation without an external validation dataset[@syed_evaluating_2020; @sundararajan_sleep_2021]. The absence of external validation raises questions about the models' generalizability. Highly flexible models, without rigorous testing on independent datasets, risk overfitting or learning dataset-specific nuances rather than broader, more generalizable characteristics. This concern is particularly relevant for Syed et al.'s model. Their methodology could become more robust with training data from a more varied population and a greater number of participants. Differences in signal shapes for mounting and unmounting devices may vary with age or other population characteristics, making a diverse training set essential for improved generalizability. Given these challenges, future research should focus on validating models with independent external datasets prior to publication. While we recognize that accumulating large and diverse datasets may not always be practically feasible, the benefits in terms of model reliability and generalizability make it an important consideration for future work.

The robustness of this study is significantly enhanced by the use of external validation, which offers strong evidence of methodological generalizability. However, there are limitations to consider. One major issue is the absence of a universally accepted gold standard for ground truth datasets in this research area. This lack of a benchmark makes it challenging to compare performance metrics across different studies. Despite this, our approach remains transparent since it relies on raw accelerometer data, and no part of our data collection or analysis process is proprietary. It's important to note that our findings are based on a study population consisting of children and adolescents. Consequently, the results may not be directly applicable to older age groups. Additionally, while we chose to develop a decision tree model for its balance of complexity and interpretability, future research could explore the efficacy of other machine learning methods like logistic regression, gradient boosting, or support vector machines. These alternative algorithms may offer different insights or advantages that could improve upon our current model.

### Conclusions

In this study, we examine the effectiveness and generalizability of both existing techniques and our newly-developed decision tree models for classifying non-wear periods in accelerometer data collected in free-living conditions. While current heuristic methods offer promising results, they come with inherent limitations. On the other hand, our findings suggest that some of the newer, more complex machine learning methods may be prone to overfitting. The quality and quantity of data are pivotal factors in training a machine learning model, especially for a straightforward binary classification problem like this, where the aim is to make the model generalizable to different datasets. To mitigate over-optimistic projections about a model's performance on unseen data, we strongly recommend the use of external validation. Additionally, given the importance of accurately detecting non-wear time as the initial step in analyzing accelerometer data, we urge researchers to carefully choose an appropriate method for this critical task.

\newpage

# Paper III: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

This segment of the thesis encompasses the methods, results, and discussion for Paper III. Polysomnography, the premier method for sleep evaluation, is not always feasible for extensive research due to its high costs and impracticality. Wearable accelerometers present an affordable solution. While wrist and hip-worn devices dominate sleep studies, the potential of thigh-worn accelerometers remains largely untapped. This paper delves into the use of machine learning and deep learning models that leverage data from thigh-worn accelerometers to gauge sleep and its quality. By comparing these models with an EEG-based sleep monitor and utilizing data from 585 days and nights of children aged 4-17 years, we discerned that the XGBoost model, particularly when applied to 5-minute median filtered data, showcased the most promise. This model demonstrated minimal biases and a robust correlation with total sleep time, highlighting its applicability. Nevertheless, our findings also exposed certain limitations, especially in determining awake intervals during in-bed periods. Thus, while our results are encouraging for group-level sleep quality estimation using machine learning, further refinement is essential for precise individual assessments due to observed limits of agreement in thigh-worn accelerometry data.

## Methods

### Dataset and Participants

The current study uses data from the SCREENS trial, which took place from June 2019 to March 2021 in the Region of Southern Denmark. Conducted by Rasmussen and Pedersen[@rasmussen_short-term_2020; @pedersen_effects_2022], the trial aimed to evaluate the impact of limiting screen media usage among Danish families. We specifically analyzed data from children between the ages of 4 and 17, with a mean age of 9.1 years, who were part of the SCREENS cohort. To gather our primary data, we utilized accelerometer readings from Axivity AX3 devices attached to the children's thighs and sleep metrics derived from EEG readings using the ZM device by General Sleep Corporation.

The Axivity AX3 is a 3-axis accelerometer positioned midway between the hip and knee on the right anterior thigh. This device records movement data, and it is unobtrusive, allowing for natural behavior from the children. On the other hand, the ZM device uses advanced EEG hardware and signal processing algorithms to gather sleep data. It features three self-adhesive, disposable sensors placed outside the hairline, ensuring reliable EEG signal acquisition. Participants were instructed to attach the device at bedtime and remove it upon leaving bed. To process the sleep data, the ZM device uses two proprietary algorithms, Z-ALG and Z-PLUS. The former is known for its accurate sleep detection capabilities, which make it suitable for in-home monitoring as supported by Kaplan et al.[@kaplan_performance_2014], while the latter differentiates between sleep stages and aligns well with expert evaluations using PSG data, as demonstrated by Wang et al.[@wang_evaluation_2015].

In this study, we didn't focus on different sleep stages like light sleep (N1 & N2), deep sleep (N3), and REM sleep; instead, we categorized the output of the ZM into "awake" and "asleep." This simplification was made to streamline the data for the machine learning algorithms and because distinguishing between sleep stages was not crucial for the sleep quality metrics of interest. As illustrated in @fig-paper3_flow, we only considered recordings that had complete accelerometer data from the Axivity AX3 and ZM readings lasting between 7 and 14 hours. Recordings with sensor issues reported by the ZM were excluded. As a result, our study included a total of 585 nights from 151 children, with an average of 3.87 nights per child (SD = 1.86). The mean age of these children was 9.4 years, with a standard deviation of 2.1. Our study encompassed 696,779 epochs, each lasting 30 seconds, and about 84% of the ZM recordings were classified as sleep.

Lastly, the study adhered to ethical guidelines, receiving approval from the Regional Scientific Committee of Southern Denmark. All data handling processes were in compliance with the General Data Protection Regulation (GDPR), ensuring the secure and ethical management of participant information.

![Flowchart of eligible ZM recording nights included in the study.](figures/paper3_flowchart.pdf){#fig-paper3_flow}

### Data preprocessing and Feature Extraction

In this study, we began by processing raw accelerometer data through a low-pass filtration step, utilizing a 4th order Butterworth filter with a 5 Hz cut-off frequency to remove high-frequency noise as described by Skotte and colleagues[@skotte_detection_2014]. Non-wear data was identified and eliminated using the methods outlined in Paper 2[@skovgaard_generalizability_2023], and the remaining data was resampled into 30-second epochs to align with ZM recordings. We then conducted feature extraction, generating 64 features that offered a comprehensive characterization of the data. These features were derived from both accelerometer and temperature signals and included temporal elements, which utilized both lag and lead values to capture dynamic data trends. Additionally, we took inspiration from Walch et al.[@walch_sleep_2019] to include sensor-independent features that encapsulate circadian rhythms, offering unique insights that are not directly discernible from sensor outputs (see @fig-paper3_sensor_independent). We further enriched the feature set by incorporating signal characteristics such as vector magnitude, mean crossing rate, skewness, and kurtosis for each of the x, y, and z dimensions. The ZM recordings and the corresponding accelerometer data were then merged. Any time overlap between these two sets of data was categorized as 'in-bed' time, while the remaining time was considered 'out-of-bed.' This process yielded a comprehensive dataset that provided a 24-hour view of each participant's activity and sleep patterns.

Upon examining the raw ZM predictions, we observed that the device appeared to overestimate the number of awakenings among the children studied. Although the ZM software addresses many of these awakenings by counting only three consecutive awake epochs towards wake time, this approach renders the raw predictions less suitable as training data for machine learning algorithms. In fact, many of these awakenings, labeled by the ZM, would be more aptly described as arousals rather than actual awakenings. Separately, the ZM device's sleep efficiency rating for our sample was 83%, which is below recognized standards. An efficiency of 85% is considered good, and over 90% is seen as ideal. This contrasts with prior research on similar child cohorts that reported a sleep efficiency of 88.3%[@galland_2018]. Recommendations from an expert panel by the National Sleep Foundation emphasize that fewer than 2 awakenings lasting more than 5 minutes each night qualify as good sleep across all age groups[@ohayon_2017]. Additionally, it's widely recognized that children typically experience between five to eight sleep cycles every night, with awakenings most likely occurring at the conclusion of each cycle[@galland_normal_2012]. However, definitions of a "waking bout" vary across studies. Some demand at least 5 continuous minutes of wakefulness for it to be counted as one bout, while others find a 1-minute duration adequate. Of particular note, many short arousal epochs labeled as awake by the ZM did not show significant shifts in the accelerometer signal. This misalignment might distort underlying patterns for machine learning algorithms. While this might not be outright mislabeling, categorizing all such epochs as true awakenings could introduce noise, jeopardizing model accuracy. In light of these observations, we opted to process both the raw ZM output and versions with 5-minute and 10-minute median filtering for our model training and evaluation. This approach minimized noise and offered an awakening count more aligned with typical patterns in children's sleep (see @tbl-10 for details).

![Sensor-independent features of circadian rhythms across two consecutive nights. A) cosinus feature, B) linear feature.](/home/esbenlykke/projects/thesis/figures/paper3_sensor_independent.pdf){#fig-paper3_sensor_independent}

![The difference in number of awakenings between the raw ZM predictions vs. 5-minute, and 10-minute median filtered predictions for a random night (boy, 9 years). Grey line is the raw predictions, black line is the median filtered predictions. A: 5-minute median filter on raw ZM predictions, B: 10-minute median filter on raw ZM predictions.](/home/esbenlykke/projects/thesis/figures/paper3_zm_raw_vs_filtered.pdf){#fig-paper3_raw_filt}

### Algorithms

In our study to assess sleep patterns, we utilized thigh-mounted accelerometer data and employed two distinct modeling strategies. The first approach involved a sequential strategy using a series of binary classifiers, aiming to simplify the task by breaking down the multiclass problem into more manageable parts. Initially, we predicted 'in-bed' times, which were then subjected to a 5-minute median filter to eliminate transient blips. This allowed us to identify a single continuous time interval, termed the Sleep Period Time (SPT), which represents the total time spent in bed attempting to sleep. The SPT served as the input for a second set of binary classifiers focused on predicting 'sleep' time, thereby improving their predictive accuracy.

Four machine learning algorithms were applied in this sequential strategy. Logistic regression acted as a fast and straightforward baseline model, although its linear nature limited its ability to capture complex, non-linear patterns. Decision trees, capable of handling non-linear patterns, were implemented with a maximum tree depth of 8 to mitigate overfitting and maintain easy interpretability. Single-layer feed-forward neural networks, while challenging to interpret, were effective in capturing non-linear relationships. Careful tuning was required to avoid overfitting. Lastly, XGBoost was used for its high accuracy and built-in overfitting prevention techniques, despite its computational intensity and interpretational challenges.

Simultaneously, we also explored a second modeling strategy using a multiclass algorithm based on a bidirectional Long Short-Term Memory (biLSTM) neural network[@hochreiter_long_1997]. This model was designed to predict three distinct sleep states: 'out-of-bed-awake,' 'in-bed-awake,' and 'in-bed-asleep.' It featured four layers and 128 hidden units per layer, balancing model complexity and training efficiency. The bidirectional architecture doubled the hidden units at each time step, enhancing data interpretation and reducing the risk of overfitting. The model accepted sequences of tensors spanning 10 minutes with a step size of one epoch. This approach is supported by previous research such as studies by Sano et al. (2019)[@sano_multimodal_2019]\] and Chen et al. (2021)[@chen_attention_2021]\], which have demonstrated the efficacy of LSTM models in sleep detection by capturing complex temporal patterns in accelerometer data.

### Model Training

We trained a total of four pairs of models sequentially to distinguish between two sets of states: in-bed/out-of-bed and asleep/awake. The dataset was randomly split into a training and a testing set, each containing approximately half of the subjects. To ensure the robustness of the results, we made sure that data from the same night was not distributed across both sets. To optimize our models, we used a specific set of hyperparameters for each type of machine learning algorithm. For the Decision Tree, we tuned the cost complexity, tree depth, and minimum number of samples required at a leaf node. The decision tree model was set up using the `rpart`[@rpart] engine for classification, with tree depth ranging from 3 to 7. For Logistic Regression, implemented using the `glmnet`[@friedman_glmnet_2010] engine, we considered tuning the penalty and mixture parameters. The feed-forward neural network was implemented with a single-layer feed-forward architecture using the `nnet`[@nnet] engine, with the maximum number of allowable weights set to 7000 as a form of regularization. The hyperparameters we tuned for this model were the number of hidden units, the penalty, and the number of epochs. The range for the number of hidden units was between 3 and 27. Lastly, the XGBoost model was configured with the `xgboost`[@xgboost] engine. The hyperparameters subjected to tuning included tree depth, learning rate, loss reduction, minimum number of samples required at a leaf node, sample size, and number of trees. For this algorithm, the number of trees was specifically tuned within a range of 200 to 800. These hyperparameters were optimized using a 10-fold Monte Carlo cross-validation, carried out on a regular grid comprising different combinations of these parameters. By providing the range and the specific hyperparameters considered for each model, we ensured the most robust and optimal model fitting.

After identifying the best-performing hyperparameters, we proceeded to fit the models to the full training dataset. This approach made it possible to use all available data for model parameter estimation, thus maximizing performance. However, an imbalance issue was noted after the initial step of our sequential model strategy. This imbalance in the resulting dataset - the awake in-bed class was underrepresented, making up only about 15% of the training data - could induce biases during model training, as models tend to favor the majority class. To correct this, we employed the Synthetic Minority Over-sampling Technique (SMOTE) as outlined by Chawla et al. [@chawla_smote_2002]. Using the themis R package [@themis], we implemented SMOTE to achieve a balanced distribution of training samples across both classes. The F1 score served as the optimization metric because it balances both precision and recall, and therefore is more robust to class imbalance.

In parallel to these sequential models, we also trained a bidirectional Long Short-Term Memory (biLSTM) model to classify three distinct states: out-of-bed-awake, in-bed-awake, and in-bed-asleep. The data for this model was divided into training, validation, and test sets, adhering to a 50/25/25 split ratio. Again, caution was exercised to avoid having data from the same night across different sets. For efficient and adaptive learning, the Adam optimizer was used during the training process. Given that we were dealing with a multiclass classification task with mutually exclusive classes, the cross-entropy loss function was employed. At the output layer, a softmax activation function was applied to obtain a probability distribution over the classes. We monitored the model's performance using the F1 score for both the training and validation sets and employed early stopping with a patience of 3 epochs, ceasing training if no improvement in the validation loss was observed over three consecutive epochs.

### Model Validation

In our study, we utilized standard evaluation metrics to assess the performance of each model on an epoch-to-epoch basis. These include $$accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$ $$sensitivity = \frac{TP}{TP+FN}$$ $$specificity = \frac{TN}{TN+FP}$$ $$precision = \frac{TP}{TP+FP}$$ $$NPV = \frac{TN}{TN + FN}$$ $$F_1 = 2 \cdot \frac{precision \cdot sensitivity}{precision + sensitivity}$$

where $NPV$ is negative predictive value, $F_1$ is the F1 score, $TP$ is true positives, $FP$ is false positives, $TN$ is true negatives, and $FN$ is false negatives.

In our sequential model strategy, we initially focused on models that carried out binary classification tasks distinguishing between in-bed and out-of-bed states. We gauged these models' performance through various metrics, including the F1-score, accuracy, sensitivity, specificity, and precision. Subsequently, we shifted our focus to models that could identify the binary state of being asleep or awake, using the same metrics as well as the negative predictive rate. Due to the class imbalance, we calculated the F1 score as an unweighted macro-average. We also scrutinized a multiclass biLSTM classifier using the same metrics, interpreting its multiclass output as two separate binary classifications: out-of-bed versus all other states, and in-bed-awake versus in-bed-asleep. To give a comprehensive view of model performance, we offered confusion matrices for the entire dataset, covering both in-bed and out-of-bed data. These matrices report relative counts, column percentages for accurate predictions of the true class, and row percentages for correctly classified predictions. Both in-bed/out-of-bed and awake/asleep classification tasks were treated as binary, designating 'in-bed' and 'asleep' as positive labels and 'out-of-bed' and 'awake' as negative labels, in line with prior studies [@hjorth_measure_2012; @kushida_comparison_2001].

To evaluate how well our models performed in generating sleep quality metrics, we employed Bland-Altman plots and Pearson correlations. Specifically, the Bland-Altman approach was used to gauge the level of agreement between two different measurement techniques. Since our dataset included multiple but uneven observations per subject, we used a bootstrap procedure to account for extra variability. We initially calculated the mean difference or bias, and then determined the limits of agreement (LOA) as the bias ± 1.96 times the standard deviation of these differences. Given the possibility of non-normal distribution and skewness in our data, we opted for a bias-corrected and accelerated (BCa) bootstrap method \[\@diciccio_bootstrap_1996\]. This allowed for more accurate estimation, taking into account intra-subject variability. Using 10,000 bootstrap replicates, we confirmed 95% confidence intervals for both the bias and LOA, thereby ensuring robust measurements. Our sleep quality metrics conformed to ZM definitions and included the following:

1.  Sleep Period Time (SPT) - This refers to the total duration of time in bed with the intention to sleep, which is defined as the time from the start to the end of the ZM recording.
2.  Total Sleep Time (TST) - This is the time spent asleep within the SPT.
3.  Sleep Efficiency (SE) - This is the ratio between TST and SPT, representing the proportion of the sleep period that was actually spent asleep.
4.  Latency Until Persistent Sleep (LPS) - This metric represents the time it takes to transition from wakefulness to sustained sleep. It is calculated as the time from the beginning of the ZM recording until the first period when 10 out of 12 minutes are scored as sleep.
5.  Wake After Sleep Onset (WASO) - This refers to the time spent awake after initially falling asleep and before the final awakening. In our analysis, a period is counted as 'awake' only if it consists of 3 or more contiguous 30-second epochs which is also how the ZM summarizes WASO.

The technical frameworks used for model development and analyses were R version 4.3.0 [@rcoreteam_2023] along with the Tidymodels[@kuhn_tidymodels_2020] and Tidyverse[@wickham_tidyverse_2019] package suites. For the biLSTM model, we used Python version 3.10.6 [@vanrossum_python_2009] and PyTorch [@paszke_pytorch_2019].

## Results

As indicated in @tbl-10, the application of 5-minute and 10-minute median filters led to modifications in the sleep quality metrics derived from ZM predictions. SPT remained consistent between raw and filtered data sets, with a mean duration of 9.2 ± 2.1 hours, which aligns with the length of the ZM recording. Interestingly, bothTST and SE saw increases in the filtered datasets, suggesting that the filters may be classifying some periods of wakefulness as sleep. Specifically, the mean TST rose from 7.7 ± 1.9 hours in the raw data to 8.1 ± 2.0 hours with a 5-minute filter and 8.2 ± 2.1 hours with a 10-minute filter. Similarly, SE increased from an initial mean of 82.6 ± 12.0% to 86.4 ± 12.7% and 87.5 ± 12.9% for the 5-minute and 10-minute filters, respectively.

Furthermore, the LPS also saw an increase, implying that the filters may be removing brief awakenings at the onset of sleep, thereby lengthening the time it takes to achieve persistent sleep. On the other hand, the WASO metric decreased from a raw average of 39.0 ± 33.6 minutes to 30.6 ± 46.8 minutes and 22.3 ± 55.4 minutes in the 5-minute and 10-minute filtered data, respectively. Notably, the application of these filters also led to a significant reduction in the average number of awakenings per night. In the unfiltered data, the mean number of awakenings stood at 34.46 ± 11.33, which dramatically dropped to 4.43 ± 3.26 and 1.95 ± 2.01 in the 5-minute and 10-minute filtered datasets, respectively.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-10
#| tbl-cap: "Overview of characteristics of the ZM sleep quality summaries per night (585 nights from 151 children). Values are represented as mean (SD). Hrs: hours, min: minutes."

tbl_10
```

```{=tex}
\endgroup
```
### Performance on Epoch-to-Epoch Basis

As delineated in @tbl-11, the epoch-to-epoch evaluation for predicting in-bed time shows virtually identical performance across various model types. The F1 score fluctuates slightly, ranging from 94.4% in the Decision Tree model to 95.4% in the XGBoost model. Likewise, accuracy varies minimally from 95.3% for the Decision Tree model to 96.1% for the XGBoost model. Other metrics such as Sensitivity, Precision, and Specificity also exhibit uniform performance across the different models. While the XGBoost model does exhibit the highest performance with an F1 score of 95.4% and an accuracy of 96.1%, it only marginally surpasses the other models in these metrics.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-11
#| tbl-cap: Performance metrics of the classification of in-bed/out-of-bed time of the included models.

tbl_11
```

```{=tex}
\endgroup
```
As noted in @tbl-12, the performance metrics for all types of sequential models in raw and median-filtered (5 and 10 minute) ZM predictions for sleep/wake classification are summarized. In the raw ZM predictions, the F1 scores, calculated as unweighted macro averages, vary from 65.6% for the biLSTM model to 76.2% for the XGBoost model. While all models demonstrate similar performance, low specificity values ranging from 62.5% to 70.9% indicate challenges in accurately classifying awake epochs. When a 5-minute median filter is applied, there is a noticeable improvement in performance metrics, with the XGBoost model achieving the highest F1 score of 79.2% and an NPV of 74.0%. Nonetheless, specificity continues to be low across all models, registering values between 54.7% for XGBoost and 74.8% for Logistic Regression. Upon application of a 10-minute median filter, the metrics experience further improvement. The XGBoost model retains its leading position with an F1 score of 80.9% and an NPV of 75.9%. Despite these improvements, specificity remains a challenge, showing a range from 57.5% for the Decision Tree model to 76.4% for Logistic Regression across all model types.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-12
#| tbl-cap: Performance metrics of the sleep/wake classification of the included models.

tbl_12
```

```{=tex}
\endgroup
```
@fig-bin_conf_mat and @fig-mul_conf_mat presents a comprehensive set of confusion matrices generated from data that includes both out-of-bed and in-bed times. These matrices offer insights into the epoch-to-epoch performance of all sequential models when differentiating between 'awake' and 'asleep' states, irrespective of whether the subject is in bed or out of bed. However, it's crucial to acknowledge that the sequential models, owing to their binary nature, are not equipped to directly classify the 'in-bed-awake' state. In contrast, the biLSTM model, which does identify the 'in-bed-awake' state as a separate class, seems to be less successful in classifying this specific state.

![Confusion matrices for the binary predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage. i) decision tree, ii) logistic regression, iii) feed-forward neural net, iv) XGBoost](figures/all_binary_conf_mats.pdf){#fig-bin_conf_mat}

![Confusion matrices for the biLSTM predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage.](figures/all_multiclass_conf_mats.pdf){#fig-mul_conf_mat}

### Evaluation of Sleep Quality Metrics

The comparative analysis of the models used for predicting various sleep quality metrics such as SPT, TST, SE, LPS, and WASO is provided in @tbl-13. The complete table, which also includes models developed from raw ZM predictions and 10-minute median-filtered ZM predictions, can be found in table 1 of the supplementary materials. Concerning bias, the Decision Tree model tends to underestimate SPT, TST, and SE while overestimating LPS and WASO when compared to ZM. Similar trends are evident in the Logistic Regression model, although the underestimation in TST and overestimation in LPS are more pronounced. The Feed-forward Neural Network shows a similar bias pattern as the Decision Tree and Logistic Regression models but has higher overestimation in WASO. In contrast, the XGBoost model exhibits the least bias, particularly in its 5-minute median predictions.

When examining the Limits of Agreement (LOA), the Decision Tree model displays higher variability in the differences across the sleep quality metrics and filtering techniques, particularly for LPS and WASO, suggesting lower agreement with ZM. Other models exhibit comparable LOA but with some noteworthy exceptions; for instance, the LOA for TST in the Logistic Regression model is particularly wide when 5-minute median predictions are considered. In terms of correlation, the Pearson coefficient indicates that the XGBoost model consistently shows the highest correlation with ZM across all sleep quality metrics and filtering methods. Notably, the strongest correlation (0.66) for TST is observed in the XGBoost model's 5-minute median predictions among all models and filtering techniques.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-13
#| tbl-cap: Summary of bias, limits of agreement, and Pearson correlation for various sleep parameter predictions (SPT, TST, SE, LPS, WASO) using different machine learning and deep learning models (decision tree, logistic regression, feed-forward neural network, XGBoost) on raw ZM predictions, 5-minute and 10-minute median predictions. Each value is provided with its 95% confidence interval (CI).

tbl_13

```

```{=tex}
\endgroup
```
The Bland-Altman plot and scatterplot presented in @fig-xgb_ba_cor demonstrate the level of agreement between the XGBoost model, trained on 5-minute median filtered ZM predictions, and ZM-derived sleep quality metrics that were also median-smoothed over 5 minutes. For the sleep quality metrics SPT and TST, the bias is notably close to zero, revealing a minimal average difference with the ZM. The scatterplot for SPT also suggests a moderate linear correlation between the model's predictions and the ZM-derived metrics. Similarly, the bias and LOA for TST align closely with those for SPT, reflecting a consistent agreement between the XGBoost model and ZM. The TST scatterplot further indicates a slightly higher correlation, mainly due to the lack of extreme outliers. In contrast, the remaining sleep quality metrics, namely SE, LPS, and WASO, show signs of heteroscedasticity unlike SPT and TST. While a moderate positive linear correlation exists between the XGBoost model and the ZM-derived SE metrics, poorer correlations are observed for LPS and WASO.

![Comparison of sleep quality metrics derived from the XGBoost model trained on the 5-minute smoothed ZM predictions. The left column displays Bland-Altman plots. Dashed lines represent the bias (the average difference between the two measurements) and LOA, with the 95% confidence intervals represented as the grayed areas. The right column displays scatter plots of XGBoost-derived vs ZM-derived sleep quality metrics. The dashed line represents the identity line, while the full-drawn line represents the best linear fit. Pearson's correlations are annotated in the upper left corner](figures/median_5_xgboost_ba_cor.pdf){#fig-xgb_ba_cor}

## Discussion

In our quest to find the most effective method for estimating sleep based on thigh-worn accelerometers, we scrutinized various models designed to predict both in-bed and sleep times, as well as associated sleep quality metrics. These models were trained and assessed using both raw and median-filtered sleep estimates derived from the ZM EEG-based sleep monitor. Overall, all sequential models exhibited strong performance in predicting when subjects were in bed. However, distinguishing between wakefulness and sleep during these in-bed periods proved to be more challenging. Interestingly, while the multiclass biLSTM model excelled in terms of F1 score, precision, and NPV, it lagged behind in deriving sleep quality metrics when compared to the XGBoost model. The latter outperformed all others across every evaluation metric, including epoch-to-epoch prediction and various sleep quality indicators. Nonetheless, it's worth noting that all models struggled with low specificity values, indicating a common difficulty in accurately identifying awake epochs during time spent in bed. We also observed performance improvement in all models when 5-minute and 10-minute median filters were applied. This filtering approach resulted in increased total sleep time and sleep efficiency metrics while reducing wake after sleep onset and the number of awakenings. Of all the models, the XGBoost demonstrated the smallest bias and the highest correlation with the ZM sleep quality metrics, making it the most robust choice for this particular application.

While there is limited existing research on the epoch-to-epoch effectiveness of thigh-worn accelerometers in classifying in-bed time, Carlson and colleagues[@carlson_validity_2021] have offered valuable insights. Their study demonstrated that both a third-party algorithm called "ProcessingPal" and a proprietary algorithm named "CREA" were able to achieve high accuracies of 91% and 86%, respectively. Evaluated against self-reported measures in adolescents and adults, these algorithms yielded impressive F1 scores as high as 95% and 96%. This is consistent with our sequential models, which also managed to surpass 95% in both F1 and accuracy scores for identifying in-bed time, equated in our study with SPT. However, it's worth noting that all models in our study, with the exception of XGBoost, tended to underestimate SPT. The biLSTM model displayed the most significant underestimation, with a bias of -36 minutes. This aligns with previous research by Winkler et al.[@winkler_identifying_2016], who reported a similar trend in both young-middle-aged and older adults. Their algorithm showed a moderate correlation with diary-recorded waking times but overestimated waking wear time by more than 30 minutes, resulting in an underestimation of in-bed time. This underestimation was further validated by Inan-Eroglu et al.[@inan-eroglu_comparison_2021], who found an underestimation of 9.8 minutes when comparing Winkler et al.'s algorithm to self-reported measures in middle-aged adults. Contrastingly, another study reported only a slight underestimation of in-bed time in middle-aged and older adults[@van_der_berg_identifying_2016]. They used a unique algorithmic approach that quantified the number and duration of sedentary periods to ascertain time in bed and active periods to identify wake times. Lastly, it's essential to clarify that strong predictive performance in identifying in-bed time doesn't automatically imply accurate predictions for broader sleep quality metrics. Capturing awake periods during in-bed time, a critical factor for assessing other derived sleep quality metrics, isn't effectively handled by simply predicting in-bed time. This distinction between actual sleep and time spent in bed while awake is often overlooked but is vital for a more comprehensive understanding of sleep quality.

To our knowledge, Johansson and colleagues[@johansson_development_2023] are the only researchers who have gone beyond merely reporting "waking time" and "in-bed time" to provide epoch-to-epoch performance metrics for sleep scoring with thigh-worn accelerometers. Utilizing a single-night evaluation dataset comprising 71 adult subjects, they managed a mean sensitivity of 0.84, a specificity of 0.55, and an accuracy of 0.80. Similarly, our models achieved a high sensitivity of above 97%, but struggled, like Johansson et al.'s algorithm, in detecting in-bed awake epochs. This struggle is manifested in our low specificity scores, which ranged from 54.7% to 76.4%. This challenge is not solely confined to thigh-worn devices. Conley et al.'s[@conley_agreement_2019] meta-analysis reported issues with wrist-worn accelerometers as well, noting mean values of 0.89 for sensitivity, 0.88 for accuracy, and a low 0.53 for specificity among healthy adults. Patterson and colleagues also recently summarized various heuristic algorithms, machine learning, and deep learning models for sleep prediction, finding mean sensitivity and specificity scores of 93% and 60%, respectively. These data collectively highlight the persistent difficulty in automating the identification of periods when individuals are awake yet still in bed. Interestingly, we noted a divergence in our study regarding the overestimation of LPS and WASO by several of our models, in contrast to most previous research. This overestimation is evident in the low NPV scores, suggesting that only a small fraction of the wake predictions are accurate. This inconsistency might be attributed to the SMOTE we used to balance the dataset. If the synthetic 'wake' samples created by SMOTE don't accurately represent the actual 'wake' data, it could cause the models to misclassify certain 'sleep' epochs as 'wake'. Consequently, this could lead to inflated LPS and WASO estimates, as the models would incorrectly identify more instances of wakefulness during sleep.

The application of the SMOTE in our study likely enhanced the performance of various models by addressing class imbalance issues. However, the introduction of synthetic "wake" samples through this method posed a challenge as they might not be fully indicative of genuine wake data. This could explain why some models, including the biLSTM which was not trained on SMOTE-processed data, overestimated TST and SE. Contrarily, the XGBoost model, trained on SMOTE-processed data, managed to navigate these synthetic "wake" samples more effectively and did not overestimate TST as much as other models. Interestingly, Bland-Altman statistics for the XGBoost model trained on 5-minute median-filtered ZM predictions indicated a mean difference of -7 minutes for TST and -1.1% for SE. The limits of agreement for these metrics spanned from -95.5 to 81.4 minutes and -15.6% to 13.3% respectively. This suggests that the XGBoost model successfully maintained a balance between sensitivity and specificity without being overly swayed by the synthetic "wake" samples. The resilience of the XGBoost model to these synthetic samples could be attributed to its gradient boosting mechanism, which allows for iterative learning from prior models' errors. Such an iterative learning process likely rendered XGBoost more robust against inaccuracies that might be introduced by synthetic data, ultimately leading to a better overall model performance.

Sleep detection methods are generally used in two distinct scenarios: night-only recordings and 24-hour recordings. For night recordings, SE and LPS can be readily derived since SPT can be inferred from the length of the recording itself, as indicated by studies from Conley et al. and Patterson et al[@conley_agreement_2019; @patterson_40_2023]. In contrast, when applied to 24-hour recordings, most sleep detection methods face the challenge of inferring SPT without the aid of sleep diaries, as presented by several studies[@girschik_validation_2012; @doherty_large_2017; @anderson_assessment_2014]. This limitation prevents these methods from generating sleep quality metrics dependent on SPT. To address this issue, we designed models capable of distinguishing between in-bed awake time and in-bed asleep time, as well as out-of-bed awake time, over a full 24-hour period. This innovation allows our models to estimate a comprehensive range of commonly used sleep quality metrics. In a similar vein, Van Hees et al.[@van_hees_estimating_2018] proposed an algorithm for determining SPT using wrist-worn devices, an approach that was subsequently validated by Plekhanova and her team[@plekhanova_validation_2023]. When combined with other methods, this algorithm enables the estimation of additional sleep quality metrics based on the identified SPT. Van Hees et al. reported favorable results with low mean differences when compared to self-reported measures and PSG for SPT, a finding later corroborated by Plekhanova et al. However, both studies also highlighted challenges in achieving good agreement on metrics such as LPS and WASO, revealing low reliability with PSG. These challenges in accurately detecting wakefulness during in-bed time are similar to the issues we encountered in our own study.

In evaluating various sleep quality metrics, our study identified that LPS consistently exhibited the largest mean error in relation to the actual time allocated to it. This challenge in accurately classifying the initial periods of SPT was further corroborated by poor Pearson correlations between LPS obtained from model predictions and the ZM. Among all models assessed, the XGBoost model emerged as the most reliable, yet it overestimated LPS by an average of 26.4 minutes for models trained on unfiltered ZM predictions. This increased to 28.5 and 34.5 minutes when the training data was 5-minute and 10-minute filtered ZM predictions, respectively. This discrepancy is not unique to our study; it is on par with the mean error of 23 minutes in sleep latency reported by Johansson et al[@johansson_development_2023]. Johansson and colleagues attribute such inconsistencies to sleep state's multifaceted and complex physiological nature. Specifically, brief awakenings or transient sleep episodes may not always result in detectable thigh movements, complicating their identification and accurate classification. These observations are consistent with findings on wrist-worn devices by Conley and colleagues[@conley_agreement_2019], who reported that correlations between accelerometer data and polysomnography (PSG) sleep onset latency (equivalent to LPS) varied greatly across studies. The mean correlation was only 0.2, underscoring the challenges in leveraging accelerometry alone for estimating LPS.

Moreover, when compared to other models like the Van Hees algorithm[@hees_novel_2015], Oakley rsc[@palotti_benchmark_2019], and LSTM-50[@palotti_benchmark_2019] as evaluated in the Patterson et al. study[@patterson_40_2023], our XGBoost model displayed narrower LOAs for TST, SE, and WASO. Interestingly, the LOAs were also narrower when pitted against the algorithm tailored for thigh-worn devices by Johansson et al[@johansson_development_2023], albeit not for SPT. Despite these promising facets, it is important to note that all methods, both from this study and the literature, showcase wide LOAs. This implies a high level of variability in sleep quality metrics derived from accelerometry, cautioning against its use as a stand-alone alternative to EEG-based ZM or PSG for individual-level sleep assessments. The presence of extreme outliers in our study appeared to exacerbate the width of LOAs, suggesting that current methods are better suited for group-level sleep quality metrics. As a result, there is a pressing need for further refinement to enhance the reliability and validity of these models for individual sleep assessments.

In our study, we opted to use the ZM as the reference method for sleep measurement, as opposed to the generally accepted gold standard, PSG. While this choice could contribute to the observed discrepancies between our models and ZM outcomes, we argue that the use of ZM has distinct advantages. For one, ZM facilitates multiple consecutive nights of recording in a free-living environment[@pedersen_self-administered_2021], thereby capturing intra-individual variations in sleep patterns. This is an aspect often impractical to achieve with PSG. Additionally, the use of ZM allowed us to incorporate more nights into our study than is typically possible with PSG-based studies. This is evident when comparing our data set to the more limited Newcastle dataset, which consists of only 28 participants[@hees_novel_2015]. Despite its benefits, we found that the raw ZM outputs were not ideally suited for developing machine learning models, primarily due to a low signal-to-noise ratio, as indicated in Figure ZM Median (@fig-paper3_raw_filt). The ZM device itself employs certain filtering processes to mitigate this issue when generating sleep quality metrics. For example, WASO is determined using contiguous epochs of 3 minutes, and sleep only contributes to sleep quality metrics if 10 out of 12 minutes are categorized as sleep. To enhance the effectiveness of our machine learning algorithms, we applied median filters to the raw ZM predictions, which had a notable impact on the derived sleep quality metrics. The application of these filters led to several changes. Specifically, the mean WASO dropped from 39 minutes in the raw predictions to 30.6 minutes when using a 5-minute median filter, and further decreased to 22.3 minutes with a 10-minute median filter. Likewise, TST, SE, and LPS all increased upon the application of the 5-minute and 10-minute median filters. These shifts suggest that the median filters could potentially reclassify brief instances of wakefulness as sleep, and similarly, eliminate short awakenings. Despite these alterations, the sleep quality metrics derived from the median-filtered predictions remained largely consistent with those from the raw predictions, validating the approach we took in this study.

Our current study offers significant contributions to the field of sleep estimation methods, particularly in the use of thigh-worn accelerometers. One of the primary strengths of the research lies in its ability to distinguish between in-bed awake time and asleep time, as well as out-of-bed time. This distinction is crucial for extracting essential sleep quality metrics. Additionally, by evaluating multiple nights per subject, the study offers valuable insights into intra-subject sleep variability, another important factor for sleep assessment.However, there are limitations to our approach. Most notably, we utilized the ZM as our reference method, which is not considered the gold standard for sleep measurement. This choice might have impacted the validity of our findings. For future work, it might be beneficial to employ PSG as a reference, despite its own set of limitations, to provide a more accurate comparison. Another limitation is the lack of external validation for our models, which confines the applicability of our findings primarily to the children studied.

### Conclusions

In terms of model performance, we tested a variety of machine learning and deep learning models to predict in-bed and sleep times as well as corresponding sleep quality metrics. The sequential models performed particularly well in predicting in-bed time, although they encountered difficulties in accurately discerning sleep from wake epochs during that period. Among all the models evaluated, the XGBoost model stood out for its superior performance in epoch-to-epoch predictions and sleep quality metrics.The study also brings attention to the existing limitations of current sleep detection methods. Specifically, there are challenges in effectively identifying wake periods during in-bed time, and improvements are needed to enhance the accuracy of individual sleep assessments. Overall, we believe our work serves as a foundational step for future research aimed at refining these models. The ultimate goal is to offer a more precise and accurate evaluation of sleep patterns and quality through the use of thigh-worn accelerometers.

# Overall Discussion

bla

# References

::: {#refs}
:::

# List of Appendices

-   **Appendix I**: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

-   **Appendix II**: Generalizability and performance of methods to detect non‑wear with free‑living accelerometer recordings

-   **Appendix III**: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

-   **Appendix IV**: Supplementary Material for Paper III

\newpage

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix I}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix I}

\vspace{2cm}

\textsf{\Huge Manual Annotation of Time in Bed Using Free-Living Accelerometry Data}

\vspace{5cm}

This paper was published in \textbf{Sensors} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.3390/s21248442}{https://doi.org/10.3390/s21248442}

\end{center}
```
\includepdf[pages=-]{my_papers/paper1.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix II}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix II}

\vspace{2cm}

\textsf{\Huge Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings}

\vspace{5cm}

This paper was published in \textbf{Scientific Reports} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.1038/s41598-023-29666-x}{https://doi.org/10.1038/s41598-023-29666-x}

\end{center}
```
\includepdf[pages=-]{my_papers/paper2.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix III}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix III}

\vspace{2cm}

\textsf{\Huge Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking}

\vspace{5cm}

This manuscript is under preparation for submission to \href{https://academic.oup.com/sleep}{\textbf{SLEEP}}, the official journal of the Sleep Research Society (SRS).

\vspace{1cm}

\end{center}
```
\includepdf[pages=-]{my_papers/paper3.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix IV}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix IV}

\vspace{1cm}

\textsf{\Huge Supplementary Material for Paper III}

\end{center}
```
\includepdf[pages=-]{paper3_supp.pdf}
