---
format:
  pdf:
      # - paperwidth=17cm
      # - paperheight=24cm
    documentclass: scrbook
    toc: true
    lof: true
    lot: true
    
    # toccolor: BrickRed
    # biblio-style: biblatex
    csl: nature.csl
    css: my_styles.css
    # classoption: nottoc
    # biblio-title: "References"
    mainfont: Zilla Slab Light
    sansfont: Montserrat
    fontsize: 10pt
    geometry:
      - margin=20mm
      - paperwidth=17cm
      - paperheight=24cm
    indent: false
    colorlinks: true
    linkcolor: DarkSlateBlue
    urlcolor: DarkRed
    citecolor: DarkSlateBlue
    link-citations: true
    tbl-cap-location: top
    number-sections: false
    pdf-engine: lualatex
    keep-tex: true
    template-partials: 
      - before-body.tex
    include-in-header: 
      text: |
       % Page setup and typography
        \usepackage[margin=20mm, paperwidth=17cm, paperheight=24cm]{geometry}
        \pagestyle{plain}
        \usepackage{sectsty}
        \usepackage{color}
        \definecolor{color1}{RGB}{80, 80, 80}
        \allsectionsfont{\sffamily \color{color1}}
        \usepackage{multicol}
        \usepackage{unicode-math}
        \setmathfont{Zilla Slab Light}
        
        \let\originaltextbf\textbf
        \renewcommand{\textbf}[1]{\textcolor{color1}{\textsf{\originaltextbf{#1}}}}
    
        \let\originalbfseries\bfseries
        \renewcommand{\bfseries}{\originalbfseries\color{color1}\sffamily}
        
        % Landscape handling
        \usepackage{lscape}
        \newcommand{\blandscape}{\begin{landscape}}
        \newcommand{\elandscape}{\end{landscape}}
        
        
        % Captions and listings
        \usepackage[font=small,labelfont=bf]{caption}
        \captionsetup{font=footnotesize}
      
        % Other utilities
        \usepackage{siunitx}
        \usepackage{pdfpages}
        \usepackage{eso-pic}
        \usepackage{hyperref}
        \usepackage{afterpage}
        \usepackage[nottoc,numbib]{tocbibind}
        \newcommand{\aftertocpagenum}{
          \cleardoublepage
          \pagenumbering{arabic}
        }
        
        % Continuous numbering for figures/tables
        \usepackage{chngcntr}
        \counterwithout{figure}{chapter}
        \counterwithout{table}{chapter}
bibliography: refs.bib
editor: source
editor_options: 
  chunk_output_type: console
---

\aftertocpagenum

# English Summary

**Introduction:** Sleep is an important element in promoting health, and the quantification of sleep has been improved with modern technology. Polysomnography, considered the gold standard, provides in-depth insight into sleep but is costly. In contrast, accelerometry is a cheaper and less invasive method, especially for longer home-based recordings. Machine learning is a tool that has the potential to automate and facilitate the estimation of sleep from accelerometer data. However, there are three challenges: producing reliable training data, ensuring data integrity through accurate removal of non-wear, and effectively using data to estimate sleep. Firstly, it is necessary to have sufficient and accurate annotations in the data for effective supervised machine learning, emphasizing the importance of methods for manual annotations based on accelerometer data. Secondly, it is essential to detect and remove periods when the device is not worn to perform accurate analyses. Identifying periods of non-wear is challenging, as traditional methods like logbooks can be prone to bias. Existing algorithms removes bias, but their accuracy is still debated. Finally, once data is correctly collected and processed, it is crucial to apply it effectively. Current methods for estimating sleep using accelerometers are based on data from wrist-worn and hip-worn devices, while data from thigh-worn accelerometers remains largely untapped for sleep estimation.

**Objectives:** Firstly, we will assess the accuracy of manual annotations for in-bed and out-of-bed timestamps in raw accelerometer data, comparing them to the timestamps determined by sleep diaries and an EEG-based sleep monitor. Secondly, we will assess heuristic algorithms and machine learning models for detecting non-wear. Finally, we will develop machine learning models for sleep classification and the estimation of sleep quality metrics using data from thigh-worn accelerometers and compare them with EEG-based sleep recordings.

**Methods:** For Paper I, accelerometer data from the hip and thigh of 14 children and 19 adults were used. Using Audacity, an open-source audio editing program, three raters annotated each accelerometer recording by marking the times when the person went to bed and when they got out of bed. Two rounds of annotations were performed to test reliability. The manual annotations were evaluated against both sleep diaries and EG-based sleep recordings. Concordance and agreement was evaluated using the intraclass correlation coefficient and Bland-Altman analyses.

Paper II used accelerometer data from sensors placed on the wrist, thigh, and hip. In hip and thigh data from 64 persons and wrist data from 42 participants, periods of non-wear were manually annotated in the same way as described in paper I. Three variants of decision trees were trained on 79.2% data from the hip and thigh and were evaluated against a selection of heuristic algorithms and recently developed machine learning models. The remaining data were used as test data for all included algorithms and models. Decision tree hyperparameters were optimized through five-fold cross-validation. External validation was performed on all wrist data. All included algorithms and models were evaluated using metrics derived from confusion matrices.

For Paper III, accelerometry and EEG-based sleep recordings from children aged 4-17 years were used. Data preprocessing included a low-pass Butterworth filter, removal of non-wear periods using the method described paper II, and a set of 64 predictors were extracted. Sleep recordings were median filtered in 5 and 10-minute windows before models were trained to better capture true awakenings. Two model strategies were used, a sequential approach with four pairs of binary classification models, and the other strategy used a multi-class model. Hyperparameter optimization was performed using ten-fold Monte Carlo cross-validation on the binary classifiers. Class imbalance was addressed using the synthetic minority oversampling technique. Data for training the multi-class model was split in a ratio of 50/25/25 for training, validation, and testing. For both strategies, the F1 score was used as an optimization target. Confusion matrix derivatives were used to assess epoch-to-epoch performance, and agreements on sleep quality metrics were assessed using Bland-Altman plots and Pearson correlations.

**Results:** The results of Paper I indicated excellent inter- and intra-rater agreement. Furthermore, the Bland--Altman limits of agreement were approximately ±30 min, showcasing only a minimal mean bias of manual annotation compared to EEG-based and sleep diary in-bed timestamps.

In Paper II, for detecting non-wear periods longer than 60 minutes, the established consecutive zeros algorithms were the most effective, registering F1-scores above 0.96. However, for durations shorter than 60 minutes, decision trees stood out, achieving F1-scores of over 0.74 across all sensor locations. Notably, the recently published deep learning and random forests models could not match this performance.

In Paper III, the XGBoost model performed the best when compared to an EEG-based sleep monitor in detecting sleep. The model demonstrated small biases in sleep period time (0.2 minutes), total sleep time (-7.0 minutes), sleep efficiency (-1.1%), and wake after sleep onset (-0.9 minutes). The model showed a moderate 0.66 correlation with total sleep time. Our limits of agreement for total sleep time, ranging from -95.5 to 81.4 minutes, were consistent with previous studies on hip and wrist devices.

**Conclusions:** Overall, the findings of this thesis underscore the reliability and precision of emerging technological methods in sleep and non-wear detection research. Paper 1 examined the agreement of manual annotations of in-bed time against traditional benchmarks and found it to be good to excellent across all comparisons. Paper 2 emphasized the nuances of non-wear detection, revealing clear strengths in certain algorithms for specific durations and highlighting areas where newer models need enhancement. Paper 3 highlights the XGBoost model for sleep assessment with thigh-worn accelerometers, situating it as a valid alternative compared to methods employed on hip and wrist accelerometer data. However,challenges remain in identifying in-bed awake periods and in assessing sleep quality metrics on an individual-basis, consistent with previous findings from wrist and hip-worn devices.

# Dansk Resume

**Introduktion:** Søvn er et vigtigt element i sundhedsfremme og kvantificeringen af søvn er blevet forbedret med moderne teknologi. Polysomnografi betragtes som guldstandarden, og giver en dybdegående indsigt i søvn, men er omkostningsfuld. Omvendt er accelerometri en billigere og mindre invasiv metode, især til længere optagelser i hjemmet. Maskinlæring er et værktøj, der har potentialet til at automatisere og lette arbejdet med at estimere søvn fra accelerometridata. Dog er der tre udfordringer: at producere pålidelig træningsdata, sikre integriteten af data og effektivt bruge data til at estimere søvn. For det første er det nødvendigt at have tilstrækkeligt med nøjagtige annotationer i data for superviseret effektiv maskinlæring, hvilket understreger vigtigheden af metoder til manuelle annotationer baseret på accelerometridata. For det andet, for at udføre korrekte analyser, er det essentielt at detektere og fjerne perioder, hvor sensoren ikke er båret. Det kan være udfordrende at identificere perioder, hvor sensorene ikke bæres, da traditionelle metoder som logbøger kan være fejlbehæftede. Eksisterende algoritmer kan forbedre denne detektering, men deres nøjagtighed er stadig genstand for debat. Endelig, når data er blevet korrekt indsamlet og bearbejdet, er det afgørende at anvende det effektivt. Nuværende metoder til at estimere søvn ved brug accelerometre er baseret på data fra håndleds- og hoftebårne sensorer, mens data fra accelerometre, der bæres på låret, stort set er uudnyttede i forhold til at estimere søvn.

**Formål:** Denne afhandling har følgende formål. For det første vurderes præcisionen af manuel annotation af sengetider i accelerometridata sammenlignet med EEG-baserede sengetider og søvndagbøger. For det andet undersøges eksisternede og nye algoritmer og maskinlæringsmodeller til at detektere perioder, hvor accelerometeret ikke er båret. Endeligt udvikles maskinelæringsmodeller til søvnklassifikation og estimering af søvnkvalitetsmål ved brug af data fra accelerometre, der bæres på låret og sammenligner med EEG-baserede søvnoptagelser. Samlet set søger denne afhandling at forstå potentialet og udfordringerne ved at anvende maskinlæring til at estimere søvn via accelerometri.

**Metoder:** Til artikel I benyttedes accelerometerdata fra hofte og lår fra 14 børn og 19 voksne. Ved hjælp af Audacity, et open-source lydredigeringsprogram, annoterede tre bedømmere hver accelerometeroptagelserne ved at markere tidspunkter for, hvornår personen gik i sengen, og hvornår de stod ud af sengen. Der blev udført to runder med annotationer for at teste pålideligheden. 'Ground truth' baseredes på EEG-søvnoptagelserne. Overensstemmelse blev målt ved hjælp af intraklassekorrelationskoefficienten og Bland-Altman-analyser.

Artikel II anvendte accelerometerdata fra sensorer placeret på håndleddet, låret og hoften. På samme måde som beskrevet i artikel I, annoteredes perioder hvor sensorerne ikke blev båret i lår- og hoftedata fra 64 personer og håndledsdata fra 42 personer. Tre varianter af decision trees blev trænet på 79.2% data fra hofte og lår og det resterende data blev brugt til test. Hyperparametre blev optimeret gennem en fem-foldig krydsvalidering. Ekstern validering blev udført på al håndledsdata. Alle inkluderede algoritmer og modeller blev evalueret ved hjælp af mål afledt af confusion matricer.

Til artikel III benyttedes accelerometri og EEG-baserede søvnoptagelser fra børn i alderen 4-17 år. Dataforarbejdningen omfattede et lowpass Butterworth-filter, fjernelse af perioder, hvor sensorerne ikke blev båret via metode fra artikel II og et sæt på 64 prædiktorer blev konstrueret. Søvnoptagelserne blev medianfiltreret i 5 og 10 minutters vinduer inden modellerne blev trænet, for at fange sande opvågninger bedre. To model-strategier blev anvendt, en sekventiel tilgang med fire par af binære klassifikationsmodeller og den anden strategi anvendte en multiklasse model. Hyperparameteroptimering blev udført ved hjælp af ti-fold Monte Carlo krydsvalidering på de binære klssifikationsmodeller. En ubalance i træningsdata blev afhjulpet ved hjælp af synthetic minority oversampling technique. Data til træning af multiklasse-modellen blev opdelt i et forhold på 50/25/25 for træning, validering og test. For begge strategier blev F1 score anvendt som optimeringsmål. For at vurdere præstationen på alle modeller blev der anvendt mål afledt af confusion matricer og for at forstå effektiviteten af vores modeller til at estimere søvnkvalitetsmål blev Bland-Altman-plots og Pearson-korrelationer anvendt.

**Resultater:** Resultaterne fra artikel I viste fremragende enighed både mellem bedømmere og inden for samme bedømmer. Derudover var Bland-Altman limits of agreement cirka ±30 minutter samtidig med en minimal gennemsnitsbias for manuelle annotationer sammenlignet med EEG-baserede tidspunkter for tid i sengen og søvndøgbøger.

I artikel II, for perioder længere end 60 minutter var de etablerede algoritmer, som på forskellig vis detekterer perioder uden acceleration, de mest effektive og opnåede F1-score over 0,96. Decision trees viste sig at præstere bedst på perioder kortere end 60 minutter og opnåede en F1-score på over 0,74 på tværs af alle sensorplaceringer. De nyligt udviklede deep learning- og random forest-modeller kunne ikke matche disse resultater.

I artikel III vist XGBoost-modellen sig bedst til at bestemme søvn sammenlignet med EEG søvnoptagelser. Modellen viste små afvigelser i tid i sengen (0,2 minutter), total sovetid (-7,0 minutter), søvneffektivitet (-1,1%) og vågen efter først søvn (-0,9 minutter). Derudover viste denne model en moderat korrelation på 0,66 med total søvntid. Vores resultater vist limits of agreements som var sammenlignelige med tidligere studier på hofte- og håndledssensorer. Specifikt viste total søvntid limits of agreements på -95,5 minutter til 81,4 minutter.

**Konklusion:** Samlet set undersøger denne afhandling pålideligheden og præcisionen af metoder inden for bearbejdning af accelerometerdata og søvndetektering. Artikel I understreger at manuel annotatering stemmer overens med EEG-baserede og selvrapporterede sengetider. Artikel II fremhævede nuancerne ved detektering af perioder, hvor sensorerne ikke bæres og viste at visse metoder præsterer bedst for specifikke varigheder af perioderne. Artikel III fremhæver XGBoost-modellen som bedst til at klassificere søvn på data fra accelerometre på låret og viser sammenligelige resultater i forhold til metoder, der anvender maskinlæringsmodeller på data fra hofter og håndled. Dog er der stadig udfordringer med at identificere perioder, hvor man er vågen i sengen. Derudover understreger limits of agreements udfordringer i forhold til at vurdere individuelle søvnkvalitetsmål, hvilket er i tråd med tidligere fund fra sensorer, der bæres på håndleddet og hoften.

# Introduction

## Why Track Sleep in Health Research

Physical behaviors throughout a day encompass activities such as sleep, physical activity, and sedentary behavior. A plethora of research has emphasized the health benefits of optimal sleep, high levels of physical activity, minimal sedentary periods, and adequate sleep across all age groups[@kraus_physical_2019; @lee_effect_2012; @wilmot_sedentary_2012; @cappuccio_sleep_2010; @jennum_søvn_sundhed_2015]. These insights have informed public health guidelines on physical activity[@piercy_physical_2018; @Sundhedsstyrelsen2023_voksne; @Sundhedsstyrelsen2023_unge] and sleep duration[@hirshkowitz_2015; @paruthi_consensus_2016; @watson_2015].

Despite spending approximately a third of our lives asleep, many facets of sleep remain elusive[@ma_sleep_2017]. What is clear is sleep's vital role in maintaining physical and psychological well-being, consolidating memories, and regulating emotions[@worley_2018; @matricciani_2019; @scott_2021]. In contrast, insufficient sleep is associated with numerous negative health outcomes, from weight gain and heart disease to impaired immunity and elevated mortality risk[@consensus_conference_panel_recommended_2015; @ji_2020; @hale_2020]. Short-term consequences of poor sleep encompass reduced alertness, heightened stress, diminished concentration, and risk-taking behavior[@shochat_2014; @kecklund_2016; @obrien_2005; @bonnet_1985]. Chronic sleep deprivation can drastically reduce one's quality of life, increase accident risks, and have broader socio-economic repercussions[@connor_2002; @dewald_2010; @roth_1996]. Such concerns are intensified considering daytime sleepiness affects 10-20% of the population[@wang_2019], attributed to factors like irregular sleep patterns, shift work, certain medical conditions, and medications[@roth_1996].

Advancements in wearable technology now provide tools for in-depth insights into the intricate relationship between sleep, physical activity, and health[@rollo_whole_2020]. Given the well-established importance of sleep and physical activity in overall well-being, integrating sleep tracking in broader health research is paramount. Modern wearable devices empower us to deeply understand the dynamics between sleep, physical activity, and overall health. Thus, comprehensive sleep research is crucial for a holistic understanding of a healthy life.

## Determinants of Sleep in Relation to Health

Sleep is multifaceted, defined by its structure, duration, quality, and timing throughout the day. Human sleep consists of two primary phases: non-REM (Rapid Eye Movement) and REM sleep. Typically, healthy adults begin their sleep cycle in the non-REM phase, which itself is subdivided into three stages. As one transitions from stage to stage, the depth of sleep intensifies[@roebuck_2014]. The third stage of non-REM sleep, often referred to as slow-wave or deep sleep, is particularly restorative and primarily occurs early in the night. Contrarily, REM sleep is characterized by increased brain activity and becomes more prolonged as the night advances[@roebuck_2014]. Throughout the night, these sleep stages cycle, generally rotating between four to six times in intervals of roughly 90-120 minutes [@roebuck_2014].

Disturbances in these sleep stages, such as interruptions from alarms, can have adverse health implications. Specifically, obstructing slow-wave sleep---even without changing the overall sleep duration---can lead to reduced insulin sensitivity, poor glucose tolerance, increased sympathetic activity, and elevated morning cortisol levels[@stamatakis_2010; @herzog_2013].

The intricate relationship between sleep duration and health is evident in numerous epidemiological and experimental studies. Cross-sectional research has unveiled a "U"-shaped association where both shorter (typically less than 6 hours) and extended sleep durations (more than 8 hours) are linked with increased risks of obesity, mental health issues, coronary heart disease, stroke, and diabetes[@cappuccio_sleep_2010; @reutrakul_2018; @cappuccio_2011; @joão_2018; @cappuccio_2008]. Furthermore, controlled experiments with sleep-deprived healthy adults have shown detrimental effects on their endocrine functions, leading to unfavorable metabolic and inflammatory responses[@banks_2007; @vancauter_2008].

Besides its duration, sleep quality is an essential component to consider. Factors determining quality include sleep onset latency, often known as latency until persistent sleep (SOL or LPS); wake after sleep onset (WASO); sleep efficiency (SE); and nocturnal awakenings[@buysse_2014]. Subpar sleep quality is associated with elevated risks of chronic diseases in adults, encompassing conditions like obesity, diabetes, and cardiovascular disease[@basnet_2016].

The growing body of research underscores the importance of understanding sleep's complexities and its pivotal role in health, reinforcing the importance of studying both its quantity and quality.

## The Gold Standard for Measuring Sleep

The challenge of studying sleep has become significantly more manageable due to advancements in technology and our understanding of neuroscience. It wasn't until the 1950s that sleep study became scientifically feasible, thanks to the pioneering work of Nathaniel Kleitman and Eugene Aserinsky[@aserinsky_1953], who demonstrated the brain's active involvement in sleep. Utilizing electroencephalography (EEG), Kleitman and Aserinsky were able to measure brain activity and identified that it synchronizes over multiple regions, predominantly within specific frequency bands. This groundbreaking discovery enabled them to define distinct "sleep stages" that the brain cycles through, fundamentally transforming the way sleep is measured and understood. A visual example of these sleep stage cycles can be seen in @fig-hypno. Therefore, measuring sleep, given its complexities, not only is critical but also possible thanks to these technological and scientific milestones.

In a relaxed wakeful state, the EEG predominantly displays alpha activity within the frequency band of 8-10 Hz and amplitudes usually ranging from 10-50 \si{\micro\volt}. As an individual begins to fall asleep, they enter the non-rapid eye movement (NREM) sleep stage 1 (N1). During this drowsy phase, the brain's EEG activity transitions to the theta range frequencies of 4-6 Hz. Simultaneously, muscle relaxation is evident, respiratory rates decelerate, and there's a drop in both distal body temperature and heart rate. This initial stage is followed by NREM sleep stage 2 (N2), where the EEG spectra frequencies are further reduced. This stage introduces sleep spindles---periodic high-frequency waves oscillating between 12-14 Hz lasting for 0.5-1.5 seconds---and K-complexes, which are characteristic reactive EEG elements of N2 sleep.

Progressing deeper into sleep, one enters NREM sleep stage 3 (N3), often termed "deep sleep". Here, the EEG mainly exhibits high-amplitude oscillations within the delta band of 1-4 Hz. Following these NREM stages, the sleep cycle culminates in REM sleep. Interestingly, the EEG patterns during REM sleep closely resemble the alpha activity seen in wakeful states. This stage is marked by evident rhythmic eye movements, while the respiration and heart rate display enhanced amplitudes and variability. The brain stem suppresses most voluntary body movements at this time, making it a period of relative physical inactivity. Dreaming tends to be more frequent during REM sleep. A single REM episode can span a few minutes and tends to extend in duration during the latter part of the night. On average, an entire sleep cycle, from N1 to REM, spans 90-110 minutes and is typically repeated multiple times throughout the night.

![Sample hypnogram showing the sleep stage cycles of an eight-hour polysomnography recording. The sleep stages (REM, NREM 1-3) and arousals are shown.](figures/hypnogram.pdf){#fig-hypno}

Polysomnography (PSG) is a gold-standard technique in sleep research, allowing simultaneous assessment of various physiological signals influenced during sleep[@sadeh_2015]. PSG gathers electrophysiological data from the brain using a 6-channel EEG, specifically from locations F3, F4, C3, C4, O1, and O2, contrasted against the contralateral mastoids (M1, M2). In addition, it uses electrooculography (EOG) to assess eye movements, electromyography (EMG) to track chin muscle tone and occasional arm and leg movements, and electrocardiography (ECG) to monitor heart rate. The study is augmented by methods to assess respiratory airflow, respiratory effort indicators, as well as peripheral pulse oximetry (PPG)[@ibáñez_2018]. For enhanced data interpretation, an infrared-equipped video camera captures the sleeping subject. Typically, PSG is carried out overnight in a specialized clinical sleep laboratory, assisting in diagnosing various sleep disorders. This technique provides detailed insights into an individual's sleep architecture, revealing sleep and wake durations as well as aiding in the classification of sleep stages[@sadeh_2015]. Such detailed data enables accurate clinical research and the diagnosis of various sleep disorders, such as sleep apnea and periodic movements during sleep[@sadeh_2015]. While PSG offers an unparalleled depth of sleep data, essential for diagnosing an array of sleep disorders, it comes with its own set of limitations. The procedure can be costly, often restricted to one or two nights in a specialized setting under a technician's supervision. This controlled environment may not truly mirror free-living sleep conditions[@sadeh_2015]. Moreover, PSG necessitates specialized personnel to oversee, score, and interpret the data, making it less feasible for expansive or free-living studies[@girschik_validation_2012] and also introducing inter and intra-rater differences in scoring the PSG data[@levendowski_2017]. Hence, while invaluable, PSG is predominantly reserved for individuals presenting sleep-related complaints and for the conclusive diagnosis of sleep disorders.

## The Zmachine® Insight+

Automated EEG data scoring presents a cost-effective alternative that mitigates the subjectivity tied to manual scoring by technicians[@redline_2013]. While there has been an uptick in technological advancements recently, substantial progress is still needed to create objective, dependable, and valid methods to determine sleep metrics[@berthomier_2013]. From the relatively few studies on automated scoring algorithms, some have shown encouraging outcomes. For instance, Malhotra et al.[@malhotra_2013] explored an automated system, comparing its PSG data scoring with visual evaluations done by PSG professionals. They found that their computer-based method yielded outcomes comparable to those of seasoned technologists. Yet, this algorithm relies on several physiological data channels, including EEG, chin EMG, and electrooculography. In the past decade, single-channel EEG-based sleep staging algorithms have started to gain attention among researchers who have proposed a variety of potential scoring methods that are compared against traditional visual scoring[@koley_2012; @fraiwan_2012; @zhu_2014].

The Zmachine® Insight+ (ZM) emerges as an important asset in sleep studies. With positive validation against PSG[@kaplan_performance_2014; @wang_evaluation_2015], the ZM delivers data on par with this gold standard, but without the hefty expenses or the demand for specialized oversight typical of PSG. Notably, the ZM's user-friendliness[@pedersen_self-administered_2021] allows for multi-night evaluations in real-world settings, capturing genuine sleep pattern fluctuations. This offers an edge over one-night PSG studies, making it an ideal primary data source for machine learning analyses. This is because it offers several nights of data without inconsistencies from different raters. However, despite its advantages, the ZM, much like the PSG, still has considerable costs and demands on participants. This underscores the importance of more convenient and cost-effective options.

## Sleep Questionnaires and Diaries

Sleep has traditionally been assessed using a sleep questionnaire in larger-scale studies. These are cost-effective and quick, making them suitable for first-line diagnosis and assessments. They quantify a patient's subjective perception of their sleep quality. While these questionnaires are inherently subjective, they've been validated as accurate in numerous studies[@silva_2011; @el-sayed_2012; @firat_2012; @luo_2014; @pataka_2014]. Typically, medical professionals are not needed to administer these questionnaires; they can be self-completed, even at home. For example, several apps exist that instantly provides a report after questionnaire completion, assisting those with potential sleep issues to seek specialist care. It's vital to understand that not all questionnaires measure the same aspect of sleep. While some assess sleep quality, others like the FOSQ-10 evaluate sleepiness[@chasens_2009] whereas instruments like the Pittsburgh Sleep Quality Index offers insights into an individual's overall satisfaction with their sleep over a defined time-frame, often a month[@sadeh_2015]. In population studies, self-report sleep assessments are common but flawed. They can overestimate sleep duration and miss subtle sleep quality details. Their design, summarizing sleep data over weeks, risks recall biases, especially when remembering older sleep patterns[@sadeh_2015]. Factors like weight, ethnicity, and regular sleep duration can influence these self-reports' accuracy[@lauderdale_2008].

Stepping away from these broad self-reports, sleep diaries stand out as a more detailed and structured tool. Often framed as the "gold standard" in subjective sleep assessment, they dive deep into various sleep parameters, like total sleep duration, efficiency, onset latency, and wake periods post sleep onset. Their strength lies in offering a day-by-day account, making it easier to spot disturbances, ascertain precise sleep timings, and decipher the rhythm of daily sleep-wake patterns over an extended duration[@ibáñez_2018]. However, like all tools, they aren't perfect. Their accuracy hinges on participants' memory retention and commitment to regular, detailed diary entries. From the researchers' standpoint, sifting through these extensive diaries can be time-intensive and, for participants, the process can sometimes be seen as taxing, potentially affecting their consistency in logging entries[@thurman_2018].

## Accelerometry for Assessing Sleep

To address the limitations of EEG-based systems and self-reported sleep assessments, accelerometers have emerged as a valuable alternative. Generally, an accelerometer is an electronic device that measures both static and dynamic acceleration forces. Static forces, arising from gravitational pull, let devices determine orientation, like a smartphone's landscape or portrait mode. Dynamic forces, resulting from movement or vibrations, are utilized in activities such as step-counting in fitness trackers or detecting collisions in vehicle airbag systems. Through technologies like piezoelectric and MEMS (microelectromechanical systems), accelerometers measure acceleration across various axes, serving diverse applications from aerospace to consumer electronics. Essentially, they are crucial sensors relaying motion or position changes to electronic systems.

While sleep researchers refer to these as "actigraphy devices", those in physical activity studies call them "accelerometers." Both terms denote devices employing accelerometer sensors to detect motion. For physical activity measurement and physical activity type distinction, devices are often placed on the hip or thigh[@migueles_accelerometer_2017; @heesch_2018; @skotte_detection_2014; @brønd_2020; @arvidsson_re-examination_2019], mainly detecting vertical acceleration associated with walking or running. This stemmed from early studies using uni-axial accelerometers that sensed movement in one direction. Devices for sleep, however, are usually wrist-worn. Omnidirectional accelerometers can sense movement in multiple directions, giving a composite signal, whereas triaxial accelerometers, with three orthogonal units, measure acceleration in three planes[@chen_2005]. These tools can offer objective insights into sleep patterns in free-living settings over consecutive nights[@conley_agreement_2019]. Their affordability and non-intrusive nature make them more appealing than PSG systems for population studies. However, many studies focus exclusively on sleep or activity, leading participants to wear the device either during the day for activity or at night for sleep. This can result in inaccuracies, like not wearing the device immediately upon waking or removing it before sleep[@meredith-jones_2016]. To improve consistency, guidelines recommend 24-hour device wear[@tudor-locke_2012].

Over the last 30 years, several research studies have highlighted the dependability and accuracy of actigraphy as an alternative to PSG for determining nocturnal sleep-wake patterns[@sadeh_activity-based_1994; @cole_automatic_1992; @hjorth_measure_2012; @tilmanne_2009; @desouza_2003; @littner_2003; @sazonov_activity-based_2004; @granovsky_actigraphy-based_2018]. The findings from these investigations indicate a consistent epoch-by-epoch concordance of 80 to 95% between accelerometer-based sleep-wake scoring methods and the conventional PSG-based scoring. Due to this high degree of accuracy, the inclusion of actigraphy devices has become a standard in sleep medicine protocols for diagnosing various sleep disorders[@smith_2018].

Actigraphy employs algorithms to distinguish sleep from wake states, using movement as an indicator of wakefulness. Though these algorithms vary depending on factors like device brand and placement, they share a common principle: they categorize each epoch based on surrounding activity levels. Early actigraphy devices, due to technical constraints, converted raw acceleration data into activity counts for storage[@neishabouri_2022]. The first of such algorithms emerged in 1982, validated against PSG[@webster_activity-based_1982]. Its successor, the Cole-Kripke algorithm, became widely accepted by 1992[@cole_automatic_1992]. These algorithms typically analyzed activity count-based features around a specific time frame and utilized linear or logistic regression for binary sleep-wake predictions[@sazonov_activity-based_2004]. In response to technological evolution and research demands, manufacturers began offering raw acceleration data from actigraphy devices. This shift allowed the creation of sleep algorithms rooted directly in raw acceleration rather than aggregated activity counts.

Borazio et al. introduced the Estimation of Stationary Sleep-segments (ESS) algorithm, designed for raw accelerometry data. This algorithm identifies idle segments by detecting a sustained low standard deviation per second over a minimum of 10 minutes[@borazio_2014]. On a related note, van Hees et al.[@hees_novel_2015] developed an algorithm centered on the accelerometer's estimated orientation angle. It identifies time segments where the estimated angle relative to gravity remains within a 5 degrees variance for at least 5 minutes. This method offers a more intuitive understanding than traditional methods, which typically focus on acceleration magnitude and zero-crossing counts. This heuristic algorithm has since gained widespread adoption in research[@difrancesco_sleep_2019; @jones_2019; @koopman-verhoeff_2019; @häusler_2020]. More presently an algorithm developed for data collected from thigh-worn devices that relies on a constantly changing variable called 'sleep index' that is affected by movement[@johansson_development_2023]. While heuristic methods have demonstrated their merit, they inherently don't benefit from increased data availability. With the growing adoption of accelerometer devices, we're seeing an influx of data. This suggests that a shift to machine learning approaches could be advantageous, as they offer advanced capabilities to leverage the full potential of these vast datasets.

## Machine Learning Fundamentals

Machine learning, a subfield of artificial intelligence, revolves around the design and implementation of algorithms that empower computers to extract patterns from data and, consequently, make informed predictions or decisions[@hastie01statisticallearning]. These algorithms, distinct from conventional explicit programming, deploy statistical methods to discern patterns within data[@bishop_2006]. Such iterative and experiential learning allows computational systems to progressively enhance their performance, autonomously adapting to new insights and data.

Different methodologies fall under the broad umbrella of machine learning, which include supervised, unsupervised, and reinforcement learning. In supervised learning, the algorithm has access to both input data and its corresponding desired output. The model, in a "supervised" fashion, learns from this data pairing until it can adeptly predict outputs for previously unseen data. Unsupervised learning, in contrast, works with unlabeled data and aims to identify hidden structures or patterns within this information. Then there's reinforcement learning, a paradigm wherein a model fine-tunes its strategies by interacting with an environment, gathering feedback in the form of rewards or penalties[@sutton_1998]. Within the confines of this thesis, our spotlight is on supervised learning, especially its applications in pinpointing behaviors via accelerometry data.

Central to the philosophy of machine learning is the principle of training. Equipped with a designated training dataset, the algorithm continually predicts and recalibrates its strategies based on any discrepancies or errors it encounters. Over time, this repetitive adjustment sharpens the algorithm's proficiency, whether it's tasked with recognizing images, decoding spoken language, predicting sales trends, or identifying sleep patterns in accelerometry data.

Deep learning represents a specialized branch of machine learning. It capitalizes on multi-layered neural networks, giving it the label "deep", to discern complex patterns within datasets. Two standout architectures in this domain are convolutional neural networks (CNNs) and recurrent neural networks (RNNs). CNNs excel in identifying spatial structures and pinpoint patterns, whereas RNNs are tailored to capture time-based or sequential information. This level of adaptability and precision allows deep learning models to analyze a wide variety of data with exceptional accuracy and detail[@Goodfellow-et-al-2016].

The machine learning landscape is dotted with a diverse array of algorithms tailored for a myriad of supervised learning challenges, spanning from elementary linear regressions to intricate deep learning models. Every algorithm has its niche strengths, but they collectively share the goal of learning patterns from data to yield accurate predictions and insights. For instance, Linear Regression is fundamental, forecasting a continuous result variable based on one or several predictor variables with an assumed linear relationship between them[@kutner_2005]. Support Vector Machines cater to classification and regression by determining the optimal hyperplane that best divides datasets into discrete classes[@cortes_1995]. On the unsupervised end of the spectrum, K-Means Clustering strives to categorize data into clusters grounded in similarity[@macqueen_1967]. Although the world of machine learning boasts a seemingly infinite repository of learning algorithms, this thesis narrows its focus to select algorithms that underpin the models we've developed.

1.  Logistic Regression: Contrary to its name, logistic regression is used for binary classification rather than regression. Logistic Regression estimates a linear decision boundary (or hyperplane in higher dimensions) to differentiate between classes, and it models the probability that a given instance belongs to a particular class using the logistic function applied to a linear combination of the input features.[@McCullagh_1989].

2.  Decision Trees: Decision trees offer a graphical representation of possible outcomes to a decision, making them easily interpretable. By segmenting the dataset based on feature values, they can handle both categorical and numerical data. They are employed in diverse areas, including medical diagnosis and credit risk analysis[@quinlan_1986].

3.  Multilayer Perceptron: Often termed a single-layer, feed-forward neural network, multilayer perceptrons consist of at least three layers: an input layer, a hidden layer, and an output layer. They can model complex relationships by adjusting weights between nodes during training. Activation functions, like the sigmoid or ReLU, introduce non-linearity into the model[@Goodfellow-et-al-2016].

4.  XGBoost: XGBoost is an optimized gradient boosting machine learning algorithm known for its speed and performance. It operates by sequentially adding weak learners, typically decision trees, and making corrections based on errors from previous stages, thereby producing a robust prediction model. This algorithm has notably excelled in various Kaggle competitions and is particularly favored for analyzing structured or tabular data[@Chen_xgboost_2016]

5.  Bidirectional LSTM (Long Short-Term Memory): LSTM networks, a type of RNN, are designed to remember patterns over long sequences and are particularly effective for time-series and natural language processing tasks. The bidirectional variant processes the sequence data from both past-to-future and future-to-past directions, enhancing the context information available to the network[@graves_2005].

## Harnessing Data for Sleep Detection Insights

Machine learning have reshaped various sectors, ranging from healthcare and finance to entertainment and technology. Central to this transformative wave is data, which has been metaphorically dubbed the new "oil" of the digital age, a term introduced by Clive Humby in 2006.

The potency of machine learning models hinges on the quality, volume, and diversity of the training data. This dependence on data has a dual rationale. Firstly, by their nature, machine learning algorithms discern patterns and relationships within data. A dataset that's both rich and varied enhances the algorithm's ability to generalize and make precise predictions on unfamiliar data. However, a sparse dataset can lead to overfitting, causing the model to become too tailored to the training data and thus perform inadequately in real-world situations. Secondly, machine learning algorithms, especially deep learning models, which often employ multi-layered neural networks, have a vast number of parameters which demands a substantial volume of data for effective training. It's akin to fitting pixels in an image together---the more pixels (or data) you have, the clearer the overall picture (or pattern) becomes. Nevertheless, sheer volume isn't the sole metric of value. The data's quality, relevance, and range are of paramount importance. Comprehensive, high-caliber datasets, capturing a wide range of scenarios and edge cases, lay the foundation for robust, adaptive, and trustworthy machine learning models.

With the surge in data availability from wearable devices, the door has opened wide for integrating machine learning methodologies into accelerometry research. These sophisticated algorithms can sift through expansive and intricate datasets, revealing insights that were previously obscured. Machine learning offers enhanced precision and depth to data analysis, whether it's pinpointing nuanced patterns of sleep apnea for early intervention[@cappuccio_sleep_2010] or assessing the efficacy of antidepressants in those afflicted with sleep disturbances[@paruthi_consensus_2016]. The sphere of sleep research has benefited from machine learning and deep learning techniques[@tilmanne_2009; @granovsky_actigraphy-based_2018; @palotti_benchmark_2019; @sundararajan_sleep_2021], particularly in the classification of sleep stages. Notably, Tilmanne et al.[@tilmanne_2009] demonstrated that Multilayer Perceptrons and Decision Trees outperformed the algorithms of Sazonov and Sadeh[@sadeh_activity-based_1994; @sazonov_activity-based_2004]. Similarly, Granovsky et al.[@granovsky_actigraphy-based_2018] employed Convolutional Neural Networks to distinguish sleep-wake patterns, though their results diverged from peer research due to the lack of PSG benchmarking. Additionally, deep learning models have showcased superior accuracy over conventional models in datasets like the MESA sleep[@lutsey_objectively_2015; @palotti_benchmark_2019]. The GGIR[@migueles_ggir_2019] R package originally incorporated the algorithm by van Hees et al. from 2015[@hees_novel_2015]. However, recent advancements have seen a random forest model, also now integrated into GGIR, surpassing the van Hees model in accuracy. This new model also outperforms other well-established algorithms such as Cole-Kripke and Sadeh[@sundararajan_sleep_2021]. Recently, Trevenen et al. applied machine learning for sleep classification, extracting numerous features from the acceleration vector magnitude. They utilized these features in a Hidden Markov Model to distinguish between sleep and wakefulness and to identify the four sleep stages[@trevenen_2019]. While their innovative approach to solely using accelerometer data for sleep stage classification didn't yield high accuracy, especially in detecting REM and differentiating Non-REM stages, they remained hopeful about the potential of such classifications.

While progress has been made, there are still challenges in creating machine learning models for sleep detection. We'll discuss these challenges in the following sections.

## Importance of Accurate Data Annotation

The complexity of a machine learning model is closely tied to the number of model parameters it must learn. As the number of features a model considers increases, there's a proportional demand for more data. For instance, in predicting housing prices where a model evaluates variables such as location, number of bedrooms, and the neighborhood, a diverse dataset is important to understand the influence of each variable[@hastie01statisticallearning]. While basic learning algorithms can often produce satisfactory outcomes with relatively limited data, more intricate learning algorithms, especially those within the deep learning spectrum, have a heightened data requirement. One of the standout attributes of deep learning, in contrast to traditional machine learning, is its ability to draw insights directly from raw data without the need for manual feature engineering. This capability necessitates a richer and more diverse dataset for optimal model performance[@Goodfellow-et-al-2016]. Data volume also depends on the task's complexity and the acceptable error margins for the application. A weather prediction model might tolerate a 20% error, but medical diagnostics require near-perfect accuracy. Lastly, the unpredictability or diversity of input can significantly influence data needs. Take, for instance, an online virtual assistant. Given that users can pose a myriad of queries in various styles and with occasional grammatical errors, the underlying model must be trained on a broad dataset to handle this range of unpredictability[@Goodfellow-et-al-2016].

The complexities and nuances associated with data requirements in machine learning underscore the importance of not only having the right volume and diversity of data but also ensuring its quality and precision[@Goodfellow-et-al-2016]. One of the pivotal aspects ensuring this precision, particularly in supervised learning scenarios, is data labeling. It means marking data with specific tags, guiding models to learn and predict. These labels, which are typically manually added by experts, help the models identify patterns. They can indicate things like categories, feelings, or any task-specific information. The better the annotation, the better the model performs, so thorough annotations is essential. Yet, the manual annotation process, especially by experts, can be complex. Simple visualization might not always offer a comprehensive understanding, potentially leading to inaccurate labels. With the vast data volumes at play, this procedure is not just lengthy but could also be error-prone, especially for lengthy annotation tasks. Inherent limitations in the precision of manual labeling mean that any missteps can profoundly skew results.

In the context of machine learning models tailored for sleep detection in multi-day accelerometry recordings, accurately annotating bedtimes and wake-up moments is essential. While one might consider sourcing annotations from sleep diaries or EEG recordings, many studies have not integrated these within their study design, leading to an information gap which can be circumvented via manually annotating targets of importance. Moreover, many current methods aimed at detecting sleep don't effectively capture the 'in-bed' time also termed the sleep period time. An exception is the HDCZA algorithm by Van Hees et al.[@van_hees_estimating_2018]. The question then becomes whether manual annotations of 'in-bed' times in accelerometry data can serve as a reliable alternative to sleep diaries or EEG recordings. If they can, this would offer a means to enrich a vast amount of existing data that currently lacks associated sleep logs or other 'in-bed' time indicators, making it more suitable for machine learning tasks. However, as of now, there's no research showcasing the effectiveness and precision of such manual annotations.

## Integrity of Accelerometry Data

Data integrity is fundamental to any credible research or analytic endeavor, a principle that's especially evident in accelerometry. Here, the accuracy and completeness of data shape our understanding of human motion and behavior. While it may seem basic, mounting the accelerometry devices correctly---using tape, elastic belts, or other secure mechanisms---is essential. Improper mounting can lead to errors, such as a device being flipped or wrongly reattached after a non-wear period, ultimately compromising the study's results and conclusions.

Non-wear time represents a 'missing data' challenge in accelerometry datasets. This issue arises when devices aren't worn due to activities like swimming, sleeping, or malfunctions, among other reasons. Notably, a valid data day is defined by having at least 10 hours of wear time, and participants must have data from at least one weekend day within a minimum of four valid days [@migueles_accelerometer_2017]. Addressing non-wear time is crucial in data processing, especially when ensuring the accurate differentiation between true non-wear periods and sleep [@choi_validation_2011; @winkler_identifying_2016]. Furthermore, as researchers derive secondary parameters for physical activity, precision in distinguishing between wear and non-wear times becomes paramount [@matthews_2002]. Misclassifications can lead to skewed activity estimates, affecting the reliability of the conclusions [@king_2011].

To handle the non-wear time challenge, some researchers have participants maintain a log diary, though this method has its drawbacks, including potential errors and participant burden [@ainsworth_recommendations_2012]. In pursuit of accuracy, the community has explored both rule-based methods and advanced algorithms. An early strategy, specific to ActiGraph data, identified non-wear time by examining consecutive zero counts [@hecht_methodology_2009; @ruiz_objectively_2011; @troiano_how_2020]. However, minor changes in thresholds can significantly alter outcomes [@aadland_comparison_2018]. The proprietary algorithms in this field have also faced scrutiny due to transparency issues and variable influences like age and obesity, affecting cross-study comparability [@toftager_accelerometer_2013].

Technological advancements in accelerometers have enabled them to store raw acceleration data, promising more detailed data and refined non-wear classifications. Various algorithms have emerged to decode this raw data, with some integrating skin temperature for enhanced accuracy [@duncan_wear-time_2018; @rasmussen_short-term_2020; @zhou_classification_2015]. The open-source GGIR package stands out, offering both a means to detect non-wear time and a method to adress non-wear time by substituting it with imputed values derived from averages of similar time points from other measurement days[@migueles_ggir_2019]. Other statistical imputation techniques, like the zero-inflated Poisson and Log-normal distributions, provide alternative solutions but come with inherent biases [@lee_missing_2018].

While these heuristic methods show broad applicability across diverse datasets and devices, their primary challenge lies in potential misclassifications due to set time intervals. Furthermore, data quantity and quality don't always ensure improved outcomes---a contrast to machine learning models that benefit from increased data.

Emerging technologies have ushered in machine learning methods, like random forests [@sundararajan_sleep_2021] and algorithms involving convolutional neural networks [@syed_novel_2021], optimized for raw accelerometer data classification. However, these models must tread the fine line between variance and bias, ensuring they don't overfit or underfit. Despite their efficacy in testing, these models' generalizability to new, unseen data remains uncertain. This brings the essentiality of external validation to the forefront, a step often overshadowed due to data constraints or research design choices. Some studies combine skin temperature with raw data for improved classification [@duncan_wear-time_2018, @zhou_classification_2015], but the full effects of this combination in machine learning remain largely unexplored. The quest continues for the ideal algorithm or model, one that flawlessly classifies non-wear time in raw accelerometer data across diverse settings.

## Limitations of Current ML Models to Detect Sleep

Although machine learning have been applied to accelerometer data with the goal of predicting sleep for a decade, the field is still in its infancy. Primarily, most methods have been tailored to integrate with data sourced from wrist- and hip-worn devices[@conley_agreement_2019].

The use of machine learning and deep learning on data collected via devices mounted to the thigh remains unexplored despite the potential advantages of this sensor location in estimating physical activity behaviors[@brønd_2020]. Only a few studies have leveraged this sensor-location for heuristic algorithms[@johansson_development_2023; @carlson_validity_2021; @inan-eroglu_comparison_2021; @van_der_berg_identifying_2016; @winkler_identifying_2016]. Given that specific sleep-related behaviors or positions might be better captured by thigh-mounted devices compared to their wrist or hip counterparts, one must wonder how this skewed focus impacts the adaptability and performance of these models in diverse real-world scenarios. Not leveraging this potential data source might lead to overlooked nuances in sleep detection.

A significant hurdle in assessing sleep using accelerometer data is the extraction of the sleep period time window without supplementary data from sleep logs or diaries[@doherty_large_2017; @dasilva_2014; @anderson_assessment_2014]. This reliance on subjective inputs introduces biases, assuming that participants consistently log accurate timings. This approach can be especially problematic in long-term studies or specific demographics, like children who often rely on their parents for accurate bedtime reporting. Typical methodologies that depend on accelerometers often require participants to log their bedtime, sleep initiation, and wake-up moments diligently[@girschik_validation_2012; @lockley_1999; @littner_2003], which can be burdensome and might result in incomplete or inaccurate datasets.

The foundational data for many algorithms, such as the random forests model by Sundarajan et al.[@sundararajan_sleep_2021], is often based on single-night PSG-recordings. While useful for a snapshot of sleep behavior, this approach doesn't account for night-to-night variability in sleep patterns. Encapsulating intra-individual variances across multiple nights could foster more robust and generalized models. However, repeated nights of PSG recording can be challenging due to its intrusiveness, cost, and inconvenience for participants. In this light, alternative systems, like the ZM, emerge as more practical and less intrusive than traditional PSG.

Given these existing challenges and opportunities in the field of accelerometry for sleep detection, it becomes evident that there's an scope for innovation and improvement. By delving into these areas of potential, researchers can foster a holistic understanding of sleep, sedentary behavior, and physical activity. With this backdrop of both the constraints and the immense possibilities that the field offers, we now turn our focus to the central ambitions of this thesis.

# Thesis Aim and Objectives

The rapidly growing field of wearable technology provides opportunities to collect accurate and objective data on human behavior, particularly using free-living accelerometer recordings. This thesis situates itself within this developing domain, with the ambition of harnessing the potential of wearable accelerometer technology for sleep estimation. At its core, the overall aim is to innovate methods and models for the analysis and interpretation of sleep. Building on the challenges delineated in earlier sections, this thesis endeavors to advance the field through a series of papers, as detailed below.

-   The objectives of paper I is to present a method for manually annotating individual bedtime and wake-up times using raw accelerometry data. Furthermore, to validate the accuracy of these annotations by comparing them with in-bed and out-of-bed timestamps as determined by the ZM and by a prospective sleep diary, and lastly, to assess both inter-rater and intra-rater agreement of the manual annotations.

-   Paper II lays down two primary objectives. Firstly, it seeks to evaluate decision tree models developed from data from both thigh and hip-worn accelerometers to detect non-wear time in accelerometry data, also including the role of surface skin temperature. Secondly, it draws a comparison between machine-learned models and heuristic algorithms across accelerometer datasets sourced from devices worn on both the hip, thigh, and wrist.

-   In Paper III, the primary objective is to assess the performance of a selection of machine learning and deep learning models in estimating in-bed and sleep time, benchmarking all included methods against sleep recordings of the ZM. Beyond this, the secondary objective is to evaluate the ability of the developed models to quantify commonly used sleep quality metrics, once again validated against an EEG-based sleep tracking device.

# Paper I: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

This segment of the thesis encompasses the methods, results, and discussion for Paper I. The study underscores the importance of effective machine learning algorithms for sleep/wake cycles, which ideally necessitate correct data annotations over a span of several days. Although sleep diaries or EEG recordings can annotate 'time in bed', many researches exclusively rely on accelerometry. This emphasizes the imperative for valid annotation techniques. Our objective is to introduce a manual annotation method, assess its precision, and determine its consistency. Some of the details presented here were previously mentioned in the published version of Paper I[@skovgaard_manual_2021] (see Appendix I).

## Methods

### Study Population

The data for this study was sourced from the SCREENS pilot trial (www.clinicaltrials.gov, NCT03788525), a two-arm parallel-group cluster-randomized trial with two intervention groups, conducted between October 2018 and March 2019[@rasmussen_feasibility_2021; @rasmussen_short-term_2020]. There was no control group in this trial.

Families from the Middelfart municipality in Denmark were approached for participation if they had a child aged between 6 to 10 years living with them, out of a total of 1686 families. To qualify, the parent's screen media usage had to exceed the median of 2.7 hours per day, based on survey responses from 394 respondents. Additionally, all children in the household needed to be older than 3.9 years to ensure that sleep measurements weren't disrupted by the nocturnal awakenings typical of infants or toddlers. For a comprehensive list of inclusion and exclusion criteria, refer to Pedersen et al.[@pedersen_self-administered_2021].

The present study ultimately included data from 14 children and 19 adults. These participants weren't advised to alter their sleep or bedtime routines for the interventions. While the study focused on nightly sleep time as recorded by the EEG-based sleep staging system, any napping behavior of the participants was deemed irrelevant.

All data collection procedures were reported to the local data protection department, SDU RIO (ID: 10.391), in compliance with the Danish Data Protection Agency's regulations.

### Accelerometry Recordings

Both adults and children participated in 24-hour accelerometry recordings using two triaxial accelerometers, Axivity AX3 (Axivity Ltd., Newcastle upon Tyne, UK). The Axivity AX3 is a compact device, measuring 23 mm × 32.5 mm × 7.6 mm and weighing just 11 g. It was set with a sensitivity of ±8 g and a sampling frequency of 50 Hz. Participants wore the accelerometers at two specific anatomical locations. The first was positioned on the right hip, secured in a pocket attached to a belt around the waist, ensuring the USB connector faced outward from the body's right side. The second accelerometer was placed midway between the hip and knee on the right thigh, housed in a pocket on a belt, with the USB connector also facing away from the body. For both the baseline and follow-up, the devices were worn for a duration of one week (seven consecutive days). This duration aligns with the recommended number of days to reliably assess habitual physical activity[@jaeschke_variability_2018].

For our study, we incorporated a total of seven distinct signal features. The criteria for classifying "lying" in the first feature are explicit: if the inclination of the hip accelerometer surpasses 65 degrees and the thigh accelerometer simultaneously identifies as "sitting" based on Skotte et al.'s activity type classification algorithm[@skotte_detection_2014]. The other signal features, with the exception of "time", are directly obtained from Skotte et al.'s algorithm. These features, delineated in @tbl-man_signal_features, concern the longitudinal axis of the body. Data derived from accelerometry undergoes processing using a window length of two seconds (60 samples) and has a 50% overlap (30 samples), ensuring a resolution of one second. The methodologies from Skotte et al. and those generating the first feature rely exclusively on the accelerometer's inclination(s). While the methodologies from Skotte et al. and the techniques generating the first feature can provide a rough estimate of time spent in bed and identify general postures, they cannot accurately pinpoint the specific moments a participant enters or leaves the bed.

### EEG-based and Self-Report Sleep Recordings

Both adults and children were assessed for their sleep patterns using the ZM model DT-200 (General Sleep Corporation, Cleveland, OH, USA), Firmware version 5.1.0. This assessment was concurrent with the accelerometer recordings. At the baseline, the sleep assessment using the ZM spanned 3--4 nights, while during the follow-up, it was conducted over 3 nights.

The ZM device operates by measuring sleep through a single-channel EEG, specifically from the differential mastoid (A1--A2) EEG location, evaluated on a 30-second epoch basis. Designed for use in everyday settings, the ZM provides an objective measurement of various sleep parameters, including sleep duration, sleep stage classification, and latency to different sleep stages. The ZM's algorithms has been benchmarked against PSG in laboratory settings for both adults with and without chronic sleep issues[@wang_evaluation_2015; @kaplan_performance_2014]. Notably, the device showcased a high accuracy in distinguishing between sleep and wakefulness, with sensitivity, specificity, positive predictive value, and negative predictive values being 95.5%, 92.5%, 98%, and 84.2%, respectively. Previous findings from our lab indicate that the ZM is effectively applicable to both children and adults for multi-day measurements in real-world settings[@pedersen_self-administered_2021].

For the assessment, three electrodes (Ambu A/S, Ballerup, Denmark, type: N-00-S/25) are positioned on the mastoids (for signal) and the nape (as ground). About half an hour before their intended sleep time, participants' skin areas are cleaned with alcohol swabs, after which the electrodes are affixed. An EEG cable connects these electrodes to the ZM device. A preliminary sensor check ensures all electrodes are correctly mounted; any issues are promptly addressed by replacing the problematic electrodes. Additionally, participants, or parents on behalf of their children, recorded their sleep and wake times daily in a dedicated sleep diary.

### Annotation Software

Audacity®️ is a distinguished free audio editing software[@audacity]. The genesis of Audacity can be traced back to the fall of 1999, when it emerged as an innovative project led by Dominic Mazzoni and Roger Dannenberg at Carnegie Mellon University. By May 2000, it was unveiled to the world as an open-source audio editor. Since its inception, Audacity has undergone extensive evolution. The software, developed collaboratively by the community, now boasts of hundreds of unique features, offers complete support for professional-grade 24-bit and 32-bit audio, has a comprehensive manual available in multiple languages, and has witnessed distribution in the millions. Today, a dedicated team of volunteers from various corners of the globe continues to maintain and enhance Audacity. It is disseminated under the GNU General Public License, granting everyone the freedom to utilize the software for personal, educational, or commercial endeavors.

Audacity is not limited to audio processing; it can also serve as a tool for accelerometer data analysis. This software provides researchers with the means to precisely inspect high-resolution raw accelerometer data in great detail. Users can quickly zoom in to explore deeper into specific segments of the recording, like certain patterns around bedtime, or zoom out for a broader perspective, such as data spanning a week. Furthermore, Audacity's features a sophisticated labeling function for annotating the accelerometry data. Any labels created can be saved in a separate file and subsequently incorporated into the training phase of machine learning algorithms. The depth of manual inspection for high-resolution accelerometer data that Audacity provides is, to our knowledge, matched by only a few other software options[@visplore; @label_studio]. However, these alternatives have data import restrictions in their free versions.

Within the Audacity interface, there's the possibility of combining over 100 channels of data. This aids in the merging of distinct signal features derived from acceleration. The integration of multiple signal features is intriguing as it might enhance the visual comprehension and classification of inherent behaviors. Nevertheless, an excessive collection of signal features might obscure the precise identification of targeted behaviors.

To provide a visual perspective, @fig-screen_full and @fig-screen_night depict the Audacity interface displaying all seven signal features as cataloged in @tbl-man_signal_features. @fig-screen_full offers a glimpse of a week's data, whereas @fig-screen_night zooms into an approximate 24-hour span, showcasing a single annotated night.

\pagebreak

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_signal_features
#| tbl-cap: Summary of the specific signal features utilized in Audacity for maunal annotation in-bed and out-of-bed timestamps.
#| tbl-cap-location: bottom

source("code/tables.R")

tbl_signal_features %>% as_latex()
```

```{=tex}
\endgroup
```
![Screenshot of the Audacity interface showing the seven horizontal panels representing the included signal features. See @tbl-man_signal_features for a detailed description of the features.](figures/audacity_full_view.png){#fig-screen_full}

![Screenshot of the Audacity interface when zoomed in on a single night for the labeling of the in-bed period. The seven horizontal panels represent the included signal features. See @tbl-man_signal_features for a detailed description of features.](figures/audacity_single_night.png){#fig-screen_night}

### Annotation Process

Three researchers, experienced in working with accelerometer data, were chosen as raters. Their proficiency ensured that they had the requisite knowledge to accurately interpret the various data channels presented to them. Each rater reviewed and labeled each wav-file, marking specific timestamps that indicated in-bed and out-of-bed activities. These annotations were then saved as individual text files. For ensuring consistency and reliability in the annotations, each wav-file underwent two rounds of labeling by each rater. Importantly, at no point during this process were the raters aware of any prior annotations, either made by themselves or their colleagues. This approach was adopted to prevent any potential biases and ensure the highest degree of objectivity in the annotations.

### Establishing the Ground Truth

The definitive ground truth for in-bed and out-of-bed time frames was obtained from the sleep staging data derived from the ZM device. This was established by identifying the first and last events at night that did not present any sensor-related issues. Nights where the ZM detected sensor problems, either at the onset or conclusion of the recording, were excluded from further consideration. Such sensor issues typically arise due to high impedance because of inadequate attachment of electrodes. To maintain accuracy in data collection, all participants were meticulously instructed to affix the ZM before bedtime and then activate it precisely at their bedtime and to detach it upon waking. These timestamps were then utilized as the ground truth for the study.

### Statistics

For continuous variables, the descriptive characteristics were computed using medians and interquartile ranges. Meanwhile, categorical variables were assessed based on their proportions. To offer a clear distinction, the characteristics for children and adults are presented separately.

In assessing the consistency of our findings, we utilized the intraclass correlation coefficient (ICC) alongside the Bland--Altman analysis. All timestamps were transformed into seconds past midnight to provide a continuous measure for the ICC analyses. Recognizing that the human raters were sampled from a broader population, we used a two-way random-effects model when assessing inter-rater reliability between different human raters. Here, ICC(2,*k*) was chosen, reflecting the absolute agreement between multiple rater's ratings[@shrout_1979]. However, when comparing human raters against the ZM or sleep diaries, a two-way mixed-effects model was used. In this context, the human raters were treated as random effects, while the ZM and sleep diaries were treated as fixed effects. The corresponding ICC that represents this model is known as ICC(3,*k*)[@shrout_1979]. For intra-rater agreement, a two-way mixed effects models was employed treating human raters as random effects and occasion (test/retest) as fixed effects. We adopted the ICC(3,*k*) to estimate the agreement of each individual rater's ratings across occasions[@mcgraw_1996]. In general, the ICC serves as a more nuanced tool than simple correlation; it goes further by evaluating the alignment in magnitude between two datasets, serving as a robust metric for consistency between methodologies. The interpretation of ICC values were as follows:

```{=tex}
\begin{center}
ICC \(< 0.5\) indicates poor agreement

\(0.5 \leq \text{ICC} < 0.75\) indicates moderate agreement

\(0.75 \leq \text{ICC} < 0.9\) indicates good agreement

\(\text{ICC} \geq 0.90\) indicates excellent agreement
\end{center}
```
In this paper, the ICC values are presented as ICC (95% CI) and is interpreted based on their 95% confidence intervals, for example, a CI of 0.83-0.94 indicates "good" to "excellent" agreement, while a CI of 0.92-0.99 is solely "excellent" as even the lowest value surpasses 0.9. By doing so, we adhere to recommended guidelines as presented by Koo et al.[@koo_guideline_2016]. The ICCs were calculated using the R package psych[@psych]. The Bland--Altman analysis evaluates the agreement between two measurement techniques[@bland_measuring_1999]. It calculates the mean difference between the two methods (representing bias) and establishes the limits of agreement. A positive mean difference indicates an underestimation, meaning the in-bed or out-of-bed timestamp is earlier in the day relative to the ZM. Conversely, a negative difference denotes a later timestamp compared to ZM. To visually present this agreement, we used probability density distribution plots, illustrating the symmetry between the methods. All statistical analyses were conducted using R (version 4.0.2) and RStudio (version 1.1.456).

\pagebreak

## Results

Descriptive characteristics of the included subjects of the current study are reported in @tbl-man_describe.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_describe
#| tbl-cap: "Descriptive characteristics of the study participants. ISCE: International Standard Classification of Education"

tbl_man_describe
```

```{=tex}
\endgroup
```
### Intraclass Correlation Coefficient Analyses

The ICCs analysis, as displayed in @tbl-man_icc_zm_man, showed excellent agreement between the ZM and the averaged manual annotations made by the three human raters across all comparisons. ICC values of the baseline comparisons (covering 94 nights) were consistently 0.98 with the lower limit of the 95% confidence intercal only dipped as low as 0.96 indicating excellent agreement. Similarly, all follow-up comparisons (encompassing 54 nights) showed ICCs above 0.95 with the lowest 95% confidence interval scoring 0.92 for the second round of "to-bed" annotations ensuring excellent agreement.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_man
#| tbl-cap: Intraclass correlation coefficients between the ZM and the three human raters. Values are ICC (95% CI).

tbl_icc_zm_man
```

```{=tex}
\endgroup
```
The weakest agreement between self-report- and ZM-determined in-bed periods were observed for the "to-bed" timestamp on the follow-up data yielding a lower limit of the 95% confidence interval of 0.94 still indicating excellent absolute agreement (see @tbl-man_icc_zm_self).

\pagebreak

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_zm_self
#| tbl-cap: Intraclass correlation coefficients between self-report and the ZM. Values are ICC (95% CI).

tbl_icc_self_zm
```

```{=tex}
\endgroup
```
Assessing the agreement between the three human raters when annotating timestamps for 'to bed' and 'out of bed' events, the ICC values reflected good to excellent agreement between the raters across both rounds and timestamps. Specifically, the lower bounds of the 95% confidence intervals dipped below 0.9 (ICC \> 0.9 indicating excellent agreement) for round 1 and round 2 "to-bed" on the baseline data, with the least value being 0.88 (see @tbl-man_icc_man_man).

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_man_man
#| tbl-cap: Intraclass correlation coefficients between the three human raters. Values are ICC (95% CI).

tbl_icc_man_man
```

```{=tex}
\endgroup
```
Across baseline and follow-up and on both to-bed and out-of-bed timestamps, each rater displayed good to excellent test-retest agreement, with the lower limits of the 95% confidence interval of hte ICCs values ranging from 0.86 to 0.99. Notably, while Raters 1 and 3 demonstrated a minor dip in their baseline to-bed agreement compared to out-of-bed measures, Rater 2 showed lower agreement on the follow-up to-bed timestamp compared to out-of-bed (refer to @tbl-man_icc_test_retest).

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-man_icc_test_retest
#| tbl-cap: Test–retest intraclass correlation coefficients between the first and second round of manual annotations. Values are ICC (95% CI).

tbl_icc_test_retest
```

```{=tex}
\endgroup
```
### Bland-Altman Analyses

@tbl-7 outlines the Bland--Altman analyses comparing both human raters and self-report against the ZM in-bed and out-of-bed timestamps. The bias observed for human raters against the ZM fluctuates between -6 minutes to 5 minutes, suggesting a relatively narrow range of mean differences across evaluations. Comparatively, the self-report's bias against ZM is slightly more constrained. The ranges of the limits of agreement remained fairly consistent irrespective of the method being contrasted with ZM at -43.81 to -21.3 minutes for the lower LOAs and 20.87 to 35.85 minutes for the upper LOAs. This uniformity in limits of agreement suggest similar agreements of both manual annotations and self-reports when compared with the ZM in-bed periods.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-7
#| tbl-cap: Bland–Altman analysis comparing manual annotation and self-report to ZM measurements, with all data presented in minutes.

tbl_7

```

```{=tex}
\endgroup
```
### Density Plots

@fig-ridge_plot presents the probability density distribution of the differences between the "to-bed" and "out-of-bed" timestamps, comparing manual annotations and self-reports to the ZM. These plots offer a visual illustration of the bias and spread around zero, showcasing how manual annotations and self-reports diverge from ZM, as previously highlighted[@van_hees_estimating_2018].

![Density distributions of timestamp differences: Manual annotations vs ZM and Self-report vs ZM for in-bed and out-of-bed times.](figures/paper1_ridge_plot.pdf){#fig-ridge_plot}

## Discussion

In this study, we introduced a methodology for manually annotating periods spent in bed using accelerometry data. The accuracy of this method was evaluated across multiple raters, within raters (test/retest) and then compared to in-bed and out-of-bed timestamps as detected with the ZM. Furthermore, the comparison of self-reported in-bed and out-of-bed timestamps as obtained from prospective sleep diaries was tested against the ZM. All comparisons were made using ICC and Bland-Altman analyses. When examining the limits of the 95% confidence interval of the ICC analyses, we found several noteworthy results. First, the method exhibited good-to-excellent inter-rater agreement between human raters. Second, intra-rater agreement of the human raters also showed good to excellent agreement across all three raters between their first and second rounds of annotations. Third, when comparing human raters to the ZM, the ICC indicated excellent agreement. Fourth, comparing in-bed and out-of-bed timestamps of self-reported vs ZM, we also observed excellent agreement between the two. Additionally, Bland-Altman analysis indicated that the mean bias between both the manual annotations and self-reported sleep times compared to ZM was within a range of ±6 minutes, with LOAs not exceeding ±45 minutes. Probability density distribution plots further substantiated these findings, showing comparable symmetry, spread around zero, and positioning of outliers when the manual annotations and self-reported sleep times were compared to ZM.

The high accuracy observed between the ZM and the prospective sleep diaries in this study can be attributed to the sleep diaries being synchronized with ZM. Having participants manually start and end the ZM recordings every morning and evening enhances their ability to accurately recall their times of going to bed and getting out of bed. This minimizes the usual discrepancies often seen between objectively and subjectively measured sleep durations[@aili_reliability_2017]. If participants had been instructed to log their sleep using a retrospective sleep questionaire or without these protocol anchors of the ZM, we would expect to see less strong agreement between the manual annotations and sleep diaries compared to the ZM timestamps.

Compared to ZM, our study found that the manual annotation of in-bed and out-of-bed timestamps was more prone to errors when estimating the time of going to bed. This was expected as the issue mainly arose from raters having difficulties in differentiating between inactive behaviors before bed time and actual bed time. Despite this limitation, the manual annotations displayed reassuring accuracy, especially considering the limited formal instructions provided to the raters. This ease of use was aided by the specific signal features we selected for study in Audacity, as evidenced by the excellent ICC agreement scores between the raters. Interestingly, our findings suggest a learning curve for the raters, as evidenced by the narrower LOAs and the density plots in the second round of manual scoring. These indicators suggest that additional rounds of scoring could further improve result consistency or that preliminary training could be beneficial for the raters.

While other tools are available for annotating time series data, including Label Studio[@label_studio], Visplore[@visplore], our research determined that Audacity was sufficiently suited for our specific requirements. Label Studio is open-source and free, making it a viable alternative; however, when dealing with extensive datasets, such as week-long accelerometer data comprising over 100 million entries, it might face challenges due to browser restrictions, and backend configuration. On the other hand, while Visplore is tailored for visual exploration of time series data, its free version comes with a data import limitation of 50 MB and offers only a subset of its features.

We deliberately tailored our feature selection to prevent overwhelming the raters with superfluous information, choosing a concise yet effective set of features rooted in domain expertise. This strategy could be adapted for annotating other activities, such as walking, which would, however, necessitate a different feature set. The human raters in this study gained valuable insights despite the absence of explicit guidelines for data annotation, highlighting the intuitive nature of the method. It's important to note that labeling data inherently involves a certain level of understanding of human behavior in accelerometry data. If such labels could be accurately determined based on a set of formal rules (i.e., a heuristic algorithm), it raises the question of whether training a machine learning model would even be necessary. Examining the impact of varying feature sets could yield further insights that would streamline the manual annotation of accelerometer time series data.

To date, many studies comparing actigraphy and self-report methods to PSG or EEG-based methodologies have primarily focused on evaluating aggregate sleep measures like total sleep time, wake after sleep onset, sleep latency, and sleep efficiency[@haghayegh_application_2020; @yavuz-kodat_2019]. These measures inherently incorporate aspects like sleep onset and wake onset times, akin to the "to-bed" and "out-of-bed" timestamps of our study. However, the precision of these specific time points has been scarcely assessed, making direct comparisons with our study difficult. One study by van Hees and colleagues reported mean absolute errors of 39.9 minutes for sleep onset time and 29.9 minutes for wake-up time, with 95% limits of agreement surpassing ±3 hours when comparing the HDCZA algorithm to PSG[@van_hees_estimating_2018]. It's worth noting that determining exact timestamps of specific events is inherently more challenging than summarizing broader measures like total sleep time. The former demands high precision, while the latter averages variations across longer periods, naturally minimizing potential discrepancies. Our research underscored the strengths of manual annotations. One consistent observation was their reliability across a broad age and gender spectrum, which was reflected in our diverse sample that included both children and adults from both genders. This highlights the adaptability of manual annotations, given that sleep behaviors are often influenced by developmental stages and gender-specific factors. Furthermore, despite being more labor-intensive, manual annotations seem to offer superior precision, particularly when identifying exact moments compared to the HDCZA algorithm which seem to struggle with nuances that human raters more easily detect. Therefore, it seems that manual annotating the in-bed and out-of-bed timestamps is better at delivering consistent results across varied groups which indicates its potential for broader applicability. This consistency is crucial when extending findings, especially in studies focusing on typical sleep patterns.

Identifying sleep periods as opposed to merely lying down in bed is a critical aspect of 24-hour behavior profiling. Traditional studies on sleep detection often rely on participants to self-report their time in bed, sleep onset, and wake-up times[@littner_2003; @lockley_1999; @girschik_validation_2012]. However, the manual annotation methodology offers an alternative that not only reduces the burden on participants but also mitigates the recall bias inherent in self-reported measures. This method of manual annotations can be easily applied to free-living data, making it incredibly versatile for various applications beyond sleep detection. For instance, the manual annotations is useful for annotating non-wear time, manually synchronizing clocks across different devices, and examining the validaty of raw data and more. Its applicability also extends to multi-channel data, providing a comprehensive overview that can incorporate variables like orientation from gyroscopic data, temperature, battery voltage, and light. Audacity stands out for its capability to handle large multi-channel data effortlessly. Researchers can quickly zoom to any resolution and scroll through time without experiencing lag, which makes it an ideal tool for adding labels. This fluidity in workflow suggests that Audacity could become a standard tool for researchers working with raw data for labelling purporses and in relation to machine learning applications.

For years, the transition from raw sensor data to operational predictive models has relied on labeled data. Despite this, no previous research has offered a insights into the precision of manual annotations compared to self-report measures and in-bed periods as determined by an EEG-device which allows researchers to optimally utilize their available accelerometry data. Our study demonstrates that with a careful selection of features, manual annotation for identifying in-bed and out-of-bed timestamps can yield results comparable to those achieved with other methodologies. However, it's crucial to clarify that we are not advocating that manual annotations should replace more established techniques for sleep estimation, such as EEG or tracheal-sound-based methods, in ongoing studies. Instead, manual annotations can be valuable as a post-hoc procedure to enrich existing datasets with an additional measure of human behaviors.

This study boasts several strengths, notably the continuous, multi-day data collection of accelerometry, sleep diary, and ZM recordings carried out in participants' homes, which provides high-quality free-living data. However, the study is not without limitations. One such limitation pertains to rater generalizability. The three manual raters were all experienced working with accelerometry data, and as such, their proficiency might not be representative of the broader population of potential raters, possibly affecting the replicability of our findings with less experienced individuals. Nevertheless, given the minimal pre-briefing instructions for labeling raw data, we believe this methodology should be generalizable to other researchers working with accelerometer data. Another concern is the challenge of recording true free-living behavior using participant-mounted devices like ZM, as wearing such a device during sleep could affect participants' natural behavior, thereby posing a study limitation. Finally, the study did not consider napping behavior; its focus was solely on in-bed periods as it relates to circadian rhythms. As such, future research is required to validate the utility of this manual annotation methodology for detecting naps.

# Paper II: Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings

This segment of the thesis encompasses the methods, results, and discussion for Paper II. Despite advancements in sensor technology and software development, the accurate classification of non-wear time in raw accelerometer data remains a challenge. This raises an important question: which heuristic algorithm or machine-learning model can best classify non-wear time when analyzing unseen accelerometer data? To address this, three datasets were generated, each comprising raw accelerometer data manually annotated for wear and non-wear times and inclusive of surface skin temperature measurements. This study specifically aimed to train three decision tree models using data from thigh and hip-worn accelerometers to classify non-wear time. Furthermore, the importance of surface skin temperature was evaluated, and the potential advantages of limiting the number of features in the decision tree model were explored. Lastly, the comparative performance of the developed decision tree models against basic heuristic algorithms and recently developed random forest and convolutional neural network models was assessed. Some of the details presented here were previously mentioned in the published version of Paper II[@skovgaard_generalizability_2023] (see Appendix II).

## Methods

### Reference Methods Overview

A total of four additional non-wear classification methods were incorporated to assess generalizability and contrast their performance with the three developed decision tree models. These pre-existing methods were deliberately chosen to encompass a range of methodological flexibility. This selection ensured representation from the simplest and most commonly used techniques to the latest and most complex ones.

1.  Consecutive Zeros-Algorithm (\texttt{cz\_60}): Over the years, there have been various consecutive zero-algorithms designed for accelerometer data, with the aim of identifying non-wear periods within predefined timeframes, such as 30-, 60-, or 90-minute intervals[@hecht_methodology_2009; @troiano_physical_2008; @choi_validation_2011]. In research by van Hees et al.[@van_hees_estimation_2011], the potential of a simple summary measure derived from raw triaxial accelerometer data to aid in the estimation of PA-related energy expenditure in both pregnant and non-pregnant women was explored. The study involved 108 women from Sweden and 99 women from the United Kingdom who wore a triaxial GENEA accelerometer for durations of 10 and 7 days, respectively. The researchers developed an algorithm to discern wear and non-wear time, basing their estimates on the standard deviation and value range of each accelerometer axis over 30-minute intervals. Intervals were designated as non-wear time if the standard deviation was below 3.0 mg (1 mg = 0.00981 ms⁻²) for at least two of the three axes, or if the value range was under 50 mg for at least two of the three axes. In a subsequent study by van Hees et al.[@hees_separating_2013], the interval length was extended to 60 minutes to reduce the likelihood of misidentifying sedentary periods as non-wear time. Furthermore, a 15-minute sliding window was introduced to account for overlapping episodes and to pinpoint non-wear episode boundaries more accurately. Another method utilizes a 135-minute interval with adjusted hyperparameters, as introduced by Syed et al.[@syed_evaluating_2020]. In our study, we adopted a straightforward approach to this concept. Using Actigraphy counts, we identified periods of no movement that registered zero counts for at least 60 continuous minutes. Notably, these Actigraphy counts operate with a deadband set at 68 mg, which denotes the minimum detectable acceleration threshold.

2.  Heuristic Algorithm (\texttt{heu\_alg}): As detailed by Rasmussen and colleagues[@rasmussen_short-term_2020], this algorithm merges raw acceleration data with surface skin temperature measurements. Non-wear time is determined for periods surpassing 120 minutes with accelerations less than 20 mg. For durations between 45 to 120 minutes, non-wear is identified if the temperature falls below a personalized non-moving temperature threshold. Additionally, the algorithm can spot non-wear periods ranging from 10 to 45 minutes, but only if these intervals end within the anticipated awake hours (06:00 AM to 10:00 PM).

3.  Random Forests Model (\texttt{sunda\_RF}): Sundararajan et al.[@sundararajan_sleep_2021] delineated a non-wear classification technique grounded in a random forest ensemble model. This model was informed by raw accelerometer data derived from 134 participants aged between 20 to 70 years. These subjects were fitted with an accelerometer on their wrist for a singular overnight PSG recording session. The ground truth labels for non-wear periods were anchored in the assumption that the accelerometer was always worn during the PSG recording. Any epoch with a standard deviation in the acceleration signal exceeding 13.0 mg outside the PSG recording time period was classified as wear time. The model utilized 36 predictors, and a nested cross-validation method was employed both to ascertain the model's generalization capability and to tune its hyperparameters.

4.  Deep Convolutional Neural Network (\texttt{syed\_CNN}): This method, introduced by Syed et al.[@syed_novel_2021], employs a unique approach. It's built upon a deep convolutional neural network that diverges from traditional techniques. Initially, all potential non-wear episodes are discerned using a standard deviation threshold. However, instead of examining the acceleration within these intervals, the focus shifts to the signal shape of the raw acceleration immediately before and after a non-wear episode. Through the convolution neural network, the method discerns non-wear periods by detecting the moments when the accelerometer is removed and reattached. For our study's purposes, we chose a window length of 10 seconds on each side of the identified non-wear episode, as this yielded the most accurate results. The training dataset that informed the CNN consisted of data from hip-mounted accelerometers worn by 583 participants. These individuals ranged in age from 40 to 84 years, with an average age of 63 years and a standard deviation of 10.

### Data Sources

Data for the current study were sourced from the PHASAR study[@pedersen_protocol_2018] and an in-house validation study with both studies utilizing the Axivity AX3 accelerometer (Axivity Ltd., Newcastle upon Tyne, UK) to record raw acceleration data along with surface skin temperature. The device, weighing a mere 11 g and with dimensions of 23 mm × 32.5 mm × 7.6 mm, measures acceleration in gravity units (g) across three axes (vertical, mediolateral, and anteroposterior). The sampling frequencies were collected at 50 Hz for the PHASAR study and 25 Hz for the in-house study. However, all recorded data from both studies were uniformly resampled to 30 Hz.

The PHASAR study involved a representative sample of over 2000 school-aged children from 31 public schools in Denmark. The study, conducted between 2017 and 2018, captured data from 1,315 boys (49%) and 1,358 girls (51%), aged between 8.1 to 17.9 years (mean age = 12.1, SD = 2.4). Accelerometers were placed at two specific anatomical sites: the right hip and midway on the right thigh. They were worn for a recommended seven consecutive days to reliably estimate habitual physical activity. For this analysis, data from 64 randomly selected participants from the PHASAR cohort were used. A dataset indicating ground truth non-wear time was created via manual annotation, a method elaborated in Paper I. Essentially, non-wear periods were determined by visually examining raw accelerations coupled with skin temperature readings. True non-wear episodes with specific start and end times were manually labelled in each dataset and were utilized as reference labels in subsequent analyses.

The in-house validation study consisted of accelerometer data from 42 youth athletes, evenly split between boys and girls, aged 14.5 to 16.4 years (mean age = 15.4, SD = 0.4 years). These athletes, part of a specialized talent program in the Region of Southern Denmark, wore the Axivity accelerometer on their non-dominant wrist for 14 consecutive days. This study was initiated in the spring of 2021. A dataset with ground truth non-wear time was created mirroring the dataset drawn from the PHASAR study, including all 42 participants.

The PHASAR study was reviewed by the Regional Committee on Health Research Ethics for Southern Denmark (ID: S-20170031) and was determined not to require an ethics review, as per Danish regulations, which mandate only biomedical research or risk-involved studies to undergo a formal ethics review. Documentation regarding this decision is available upon request from the corresponding author. Conversely, the in-house validation study received an ethical approval waiver from the Research & Innovation Organization and the legal department of the University of Southern Denmark. All participants, or their legal guardians, provided written informed consent for both studies, which adhered to the Danish Data Protection Agency (2015-57-0008) standards and globally recognized guidelines like the Declaration of Helsinki.

![Flowchart illustrating the division of the PHASAR dataset into training and testing datasets. On the left, boxes represent 79.2% of the PHASAR data allocated for training across five-fold resamples. In the middle, boxes represent 20.8% of the PHASAR data delineated for testing, specifically marking the hip and thigh data. The box on the right-hand side signifies our in-house test dataset obtained from wrist-worn devices.](figures/paper2_flowchart.pdf){#fig-paper2_flowchart}

### Development of Decision Tree Models

For our decision tree models, we sourced 12 features from the raw PHASAR accelerometer data, which encompassed elements like temperature, time of day, indicators for device placement, day of the week, and moving average statistics (detailed in @tbl-8). These moving average metrics were collated in 10-second increments. To train the model, we utilized 79.2% of the PHASAR data, incorporating data from both hip- and thigh-worn devices (as shown in @fig-paper2_flowchart). We made certain that data from individual participants was exclusively allocated to either the training or test datasets. This strategy was to ensure that the model could effectively generalize to unfamiliar data, rather than overfitting to specific participant data. During the tuning phase, to boost model accuracy and avoid overfitting, we opted for a five-fold cross-validation approach. This process entailed refining several hyperparameters, such as the tree's depth, its cost-complexity, and the minimum amount of data points necessary in a node for it to split further. To effectively explore the hyperparameter space, we employed Latin hypercube sampling. This method systematically divides the parameter range into segments, randomly drawing a value from each segment, resulting in a well-distributed set of parameter combinations. In our case, we established a 10-level parameter grid to search the hyperparameter space. The F1 score was treated as the optimization metric.

Following this procedure, we introduced three distinct model variations:

1.  A full-scope model (\texttt{tree\_full}) incorporating every feature.

2.  A refined model (\texttt{tree\_imp6}) centered on the six most crucial feature, as established by permutation feature importance (@fig-importance).

3.  A model excluding surface skin temperature as a feature (\texttt{tree\_no\_temp}).

In sum, our methodology generated 50 distinct models for each decision tree variant. Given our data's distribution - 55.8% wear time compared to 44.2% non-wear time - there was no need to adopt synthetic minority oversampling methods like SMOTE or other balancing techniques.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-8
#| tbl-cap: Features extracted from the raw sensor signals.

tbl_8

```

```{=tex}
\endgroup
```
![Permutation importance plot depicting the relative importance of predictors in the full decision tree model (\texttt{tree\_full}). The top six predictors informed the \texttt{tree\_imp6} model, while a third model, \texttt{tree\_no\_temp}, was trained using all predictors except temperature.](figures/paper2_vip.pdf){#fig-importance}

### Statistics

Classification performance was evaluated against a ground truth test dataset, encompassing over 7 million epochs of 10 seconds each, derived from 104 unique subjects. Accurate identification of true non-wear time and true wear time yielded true positives (TP) and true negatives (TN), respectively. These correct classifications are essential for ensuring the algorithm's precision in determining non-wear time. Conversely, misclassifications, where true non-wear time is identified as wear time or vice versa, contributed to false negatives (FN) and false positives (FP). By analyzing these 10-second acceleration data intervals against the ground truth, a confusion matrix was constructed. From this matrix, the following performance metrics were extracted:

\
$accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

\
$sensitivity=\frac{TP}{TP+FN}​$

\
$precision=\frac{TP}{TP+FP}$

\
$​F1\ score=\frac{2 \cdot TP}{2 \cdot TP+FP+FN}​$

The F1-score, which represents the harmonic mean of precision and sensitivity, serves as a robust metric for evaluating classification
performance, with higher F1-scores indicating better classification efficacy. Additionally, we utilized permutation feature importance to
assess the contribution of each feature in the full decision tree model. This method assesses the relative importance of each feature by evaluating the decrease in model accuracy when the data for that particular feature is randomized. A significant drop in performance upon
randomizing a feature suggests an important role for that feature in the model.

The decision tree models were trained using data from devices worn on the hip and thigh. On the other hand, the random forest model by
Sundarajan et al. was developed using data from wrist-worn accelerometers, while the convolutional neural network by Syed et al.
was trained on data from hip-worn devices. To assess the generalizability of all machine learning models, each underwent external
validation. This process entailed testing them on datasets representing populations and wear locations different from those on which they were initially trained, enabling an assessment of their generalizability across varied anatomical positions and age spans (see @fig-paper2_flowchart).

For all analyses and model development, we utilized R (version 4.1.2, Bird Hippie) and RStudio (version 2021.9.1.372, Ghost Orchid). The machine learning tasks were primarily facilitated by the Tidymodels[@kuhn_tidymodels_2020] suite of packages, and we used the rpart[@rpart] package as the engine for our decision tree algorithms.

## Results

In our datasets spanning three wear locations, there were 1,598 non-wear time episodes. Of these, 1,148 episodes (or 71.8%) lasted 60 minutes or more, with an average duration of about 13 hours (794 minutes with a standard deviation of 1,142 minutes). In contrast, episodes lasting 60 minutes or less made up 28.2% (450 episodes) with an average duration of 26.4 minutes (SD = 16.4). Interestingly, the briefest episodes (less than 60 minutes) made up just 1.3% of the total non-wear time across all wear locations (refer to @tbl-9). @fig-paper2_nw_dists depicts the frequency distribution for episodes shorter than 60 minutes and those 60 minutes or longer. The PHASAR dataset (hip and thigh) showed a bimodal distribution for shorter episodes, with longer episodes peaking around 10 hours. For the in-house wrist-worn dataset, shorter episodes displayed a more uniform distribution with a peak around 15 minutes in duration, while longer episodes were significantly right-skewed.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-9
#| tbl-cap: Overview of non-wear episodes grouped in short and long non-wear episodes, min = minutes, hrs = hours.

tbl_9
```

```{=tex}
\endgroup
```
![Distribution of the length of the non-wear episodes across hip, thigh, and wrist data. Distributions are shown for episodes shorter than 60 min and longer than 60 min.](figures/nw_dist_test.pdf){#fig-paper2_nw_dists}

### Classification Performance

In assessing classification performance, @fig-paper2_preds_ex visually contrasts the results from machine-learned models and rule-based algorithms against the ground truth non-wear time, which is highlighted with a light grey background. This visualization underscores that while tree-based models tend to be precise, they can also be unpredictable. On the other hand, threshold-based methods, such as \texttt{heu\_alg} and \texttt{cz\_60}, offer more consistency. Notably, both \texttt{cz\_60} and \texttt{heu\_alg} algorithms fall short in identifying shorter non-wear episodes.

![Visual example of a single day of the output of non-wear detection models and algorithms for a random person from the in-house wrist dataset. The grey shade is ground-truth non-wear time. \texttt{syed\_CNN}, \texttt{cz\_60}, and \texttt{tree\_full} are vertically offset for easier interpretation.](figures/paper2_plot_preds_example.pdf){#fig-paper2_preds_ex}

@fig-paper2_performance_all compiles performance metrics from all methods evaluated in this study on all non-wear periods. The \texttt{tree\_full}, \texttt{tree\_imp6}, cv_60, and \texttt{heu\_alg} all achieved similar performance, achieving an accuracy and F1 scores above 90% on both the thigh, hip, and wrist data. These methods showcased a uniform performance across wear location with exception of the three_full model eliciting a slight drop in performance on the wrist data. On the other hand, some models demonstrated varied performances when transitioning from one dataset to another. For instance, the \texttt{tree\_no\_temp} and the \texttt{syed\_CNN} model's F1 score on the hip and thigh datasets hovered around 65% and 68%, respectively, but when applied to the wrist dataset, there was a noticeable dip to 47% and 59%, respectively. The \texttt{sunda\_RF} model, however, showed an upward trajectory, moving from an accuracy of approximately 57% on the hip and thigh datasets to a significantly improved 93% on the wrist dataset.

Further, @fig-paper2_performance_short zeroes in on performance metrics for episodes 60 minutes or shorter. The consecutive zeros algorithm was unable to detect any non-wear, a result absent from the figure. Syed et al.'s deep learning model underperformed, detecting a mere 1--2% of all non-wear time, leading to F1 scores below 5%. Although the \texttt{heu\_alg} algorithm boasted high precision, its lackluster sensitivity resulted in F1 scores spanning from 12% to 16% across wear locations. The random forest model displayed average results for thigh and wrist data but faltered with hip data, recording F1 scores of 46%, 57%, and 8%, respectively. Among the trio of decision tree models, the one leveraging the six most important features outshone the rest, with F1 scores between 72% and 79%. Meanwhile, the decision tree model encompassing all predictors faced challenges with hip data due to a 23% sensitivity score. The decision tree model excluding the surface skin temperature exhibited commendable precision; however, its low sensitivity culminated in F1 scores ranging from 45% to 57%.

![Classification performance metrics for all non-wear episodes as assessed by all included methods for classifying non-wear time. Metrics are displayed for three different ground-truth datasets: hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_all.pdf){#fig-paper2_performance_all}

![Classification performance for episodes no longer than 60 min in length. Metrics are shown for the three different gold-standard datasets: hip-worn, thigh-worn, and wrist-worn raw accelerometer data.](figures/paper2_performance_short.pdf){#fig-paper2_performance_short}

## Discussion

In this study, we evaluated various methods for classifying non-wear episodes in free-living accelerometer data, focusing on all non-wear episodes and those shorter than 60 minutes. Our findings showed that the simplest methods, specifically \texttt{cz\_60} and \texttt{heu\_alg}, excelled in identifying non-wear episodes longer than 60 minutes across all three sensor wear locations: wrist, hip, and thigh. They were closely followed in performance by the decision tree models that included surface skin temperature as a predictive variable (\texttt{tree\_full} and \texttt{tree\_imp6}). On the other hand, the random forest model demonstrated excellent performance only on data from the wrist, delivering mediocre results on hip and thigh data. The \texttt{syed\_CNN} and \texttt{tree\_no\_temp} yielded mediocre results across all wear-locations. When we shifted our focus to short non-wear episodes lasting less than 60 minutes, the limitations in the \texttt{cz\_60} and \texttt{heu\_alg} algorithms due to their built-in minimum episode durations of 60 and 20 minutes, respectively, became evident. Similarly, the deep learning model (\texttt{syed\_CNN}) showed poor results, mainly attributable to very low sensitivity scores across all wear-locations that led to many episodes being misclassified as non-wear time. The random forest model's performance was also poor on the hip and only mediocre on the thigh and wrist. The decision tree models, both without temperature and with all predictors, showed mediocre performance as well. However, the decision tree model trained on the six most important predictors stood out as the best performer for short non-wear episodes on all wear-locations. This study also highlighted the value of incorporating surface skin temperature as a predictor to enhance the performance of non-wear time classification. Overall, these results provide valuable insights into the effectiveness of various methods for classifying non-wear episodes in accelerometer data, emphasizing the potential of simple algorithms like \texttt{cz\_60} and \texttt{heu\_alg}, especially for longer non-wear episodes, and the benefit of including surface skin temperature as a predictive variable.

We discovered that most non-wear episodes in our ground truth datasets had a duration exceeding 60 minutes, with a noticeable peak around the 10-hour mark. This finding contrasts with previous research that typically reported shorter episodes as being more prevalent[@aadland_comparison_2018; @jaeschke_variability_2018; @hutto_identifying_2013]. Our data prominently features children and physically active adolescents. While this demographic, known to spend less time in sedentary activities and frequently interrupt such periods[@cooper_objectively_2015], might remove wearables for sports or other activities, the extended non-wear durations, especially those of several hours, are unlikely to be mistaken for sedentary behavior. Such prolonged episodes more definitively signal non-wear rather than inactivity. This clearer distinction minimizes potential overlaps or ambiguities between non-wear and sedentary periods in our dataset. This also meant that the data favored the simple heuristic algorithms for classifying non-wear time, largely because the limitations imposed by minimum window lengths had a negligible impact on the proportion of non-wear time that was incorrectly classified. These algorithms achieved excellent precision scores, confirming that neither sedentary time nor sleep was misclassified as non-wear time. This is a significant finding, given that multiple previous studies have pointed out the complexities in making this very distinction[@troiano_physical_2008; @duncan_wear-time_2018; @choi_validation_2011; @doherty_large_2017; @barouni_ambulatory_2020; @mn_non-wear_2020].

Creating a model to classify non-wear time seems to be a relatively straightforward task. A primary reason is the distinct data patterns that wear and non-wear times produce. For instance, non-wear times should typically yield consistent zero or near-zero readings due to the absence of movement, while wear times elicit varied readings reflecting activity levels. This distinction implies that the decision boundary involved in this classification is likely close to linear. Given the binary nature of this classification task -- the device is either worn or not -- such clear data patterns could make more complex models unnecessary. In fact, using complex models, like the ones employed in the current study, can introduce the risk of overfitting. Overfitting tends to capture the random fluctuations or noise inherent in the training dataset, ultimately compromising the model's performance on unseen, out-of-sample data. In light of these observations, we hypothesize that a simpler, well-optimized logistic regression model could be just as effective, if not more so, than the complex models that was evaluated. The strength of logistic regression in this context lies in its ability to establish a separating linear hyperplane. This hyperplane may effectively differentiate between wear and non-wear times without the added intricacy of non-linear decision boundaries. Furthermore, complex models, while offering sophisticated decision-making capabilities, may be redundant for this task and could introduce unnecessary complications. This is especially evident when the objective is creating a universally applicable machine learning model suited for diverse populations and various wear locations. To optimize the performance of any chosen model, be it simple or complex, the quality and diversity of the training data are important. It is vital to source data that spans multiple wear locations and encompasses a variety of physical activity profiles. Training on such a comprehensive dataset ensures that the model is equipped to discern between wear and non-wear times across different scenarios and populations. While our discussion leans towards simpler models given the nature of the decision boundary, the emphasis remains on the crucial role of a rich and varied dataset in ensuring model efficacy.

This study emphasizes the potential of the consecutive zeros algorithm as a leading approach for identifying non-wear episodes exceeding 60 minutes in children and adolescents. This method showed efficacy across various wear locations, such as the hip, thigh, and wrist. However, due to the distinctive behaviors and routines of these younger age groups, our results may not be seamlessly extended to older populations. In comparison, models like the \texttt{syed\_CNN}, which detect accelerometer mounting and unmounting events, present certain complexities. The motions associated with putting on or taking off a device may not be universally consistent, as they could vary depending on age, dexterity, or other factors related to the population in question. Therefore, while the \texttt{syed\_CNN} model employs a standardized approach, its results might still be influenced by age or other population-specific factors. Though our research provides valuable insights for younger demographics, it's essential to approach the \texttt{syed\_CNN}'s results with an understanding of its potential limitations across varied age groups in accelerometer-based studies as well.

Incorporating surface skin temperature for the classification of non-wear time has been scarcely explored in the field of machine learning. One study did indicate that using acceleration data along with rate-of-change in surface skin temperature could create a robust decision tree model for detecting non-wear time[@vert_detecting_2022]. This aligns with previous studies that have shown improved predictive performance when temperature data is included in heuristic algorithms[@duncan_wear-time_2018; @zhou_classification_2015]. Our own findings also corroborate this, as we observed that adding surface skin temperature as a feature enhanced the performance of the non-wear decision tree models. However, to accurately detect transitions between wear and non-wear periods, it may be beneficial to account for the slow response time of temperature sensors. Solely using temperature data can cause classification delays. By incorporating lagged and leading temperature features, we can better anticipate previous and upcoming temperature changes. Therefore, a combined approach using both temperature, with its nuanced features, and acceleration data is advised, as supported by Zhou and colleagues.[@zhou_classification_2015]. During our study, we noted a 20-minute step response in the Axivity temperature sensor, which could be attributed to the design of the device's casing. The sensor's response time may also be influenced by the attachment method used. If more material is placed between the skin and the device, delays are likely to be amplified, suggesting that machine learning models should perhaps consider the type of sensor attachment in their feature set. Additionally, different brands of devices have been found to have varying optimal temperature thresholds, further complicating the issue. As noted by Duncan et al. and Zhou et al., algorithmic modifications are needed for devices to function optimally in different latitudes[@duncan_wear-time_2018; @zhou_classification_2015]. Therefore, the type of device and its attachment method can be critical components for improving the accuracy of non-wear time classification models.

Evaluating the performance of a machine learning model is a critical step in ensuring its reliability. Typically, researchers use a portion of the dataset they trained the model on, segmenting it for testing purposes. This is termed as "internal validation." While standard and useful, this method isn't without its shortcomings. To illustrate, the studies conducted by Syed et al. and Sundararajan et al. showcase impressive performance metrics, such as sensitivity, specificity, and accuracy in their task to classify non-wear time. Yet, a closer look reveals that these high-performance results are rooted in cross-validation techniques that didn't incorporate external validation datasets[@syed_evaluating_2020; @sundararajan_sleep_2021]. This omission leads us to question the generalizability of their models. Can these models perform as well in real-world scenarios with diverse data as they did during testing? Models that are highly flexible and adaptive pose a risk, especially if they are not subjected to rigorous validation. There's the potential danger of them overfitting to the training data, meaning they might excel at recognizing patterns from the specific dataset they were trained on but falter when faced with new, unseen data. Such models might inadvertently learn the unique quirks and nuances of the training dataset rather than universally applicable patterns. This potential for overfitting becomes especially concerning when considering the methodology of Syed et al.'s model. Their approach, while promising, would greatly benefit from training on a dataset enriched with data from a broader spectrum of participants. Factors like age, lifestyle, or health can lead to subtle differences in how devices are mounted or unmounted, creating variations in signal shapes. By including a more diverse population in their training data, the model's ability to generalize and correctly interpret signals from different user groups would likely improve. Considering these insights, it's evident that the next wave of research in this domain should prioritize validating their models using independent, external datasets before they're widely accepted or published. We understand the challenges tied to accumulating extensive and diverse datasets, especially in niche fields. However, the promise of crafting a model that's both reliable and universally applicable makes this a worthwhile pursuit.

When analyzing acceleromtry data, the ideal scenario is to employ a single model that performs reliably across different wear locations and populations. To evaluate the generalizability and robustness of the developed decision tree models used in the present study, we included a dataset from wrist-worn devices for external validation. This ensures that the performance metrics of our decision tree models are not artificially inflated due to overfitting or lack of variance between the training and testing data. External validation involves testing a model with independently sourced datasets to confirm its performance. If a predictor set has been inaccurately selected or if model parameters have been overly tuned to characteristics inherent to the training data, such as technical or sampling bias, the model is likely to perform poorly during external validation[@steyerberg_prediction_2016]. The rationale for using external validation is compelling: although data from various sources might exhibit differences, they can still encapsulate crucial domain-specific information. A well-trained model that focuses on truly informative predictors should retain its performance when applied to new, previously unseen data. Therefore, the external validation in our study acts as a verification step, ensuring that our decision tree models that pass this criterion are not just robust but also likely to be interpretable within the domain[@altman_prognosis_2009]. While Syed et al.'s methodology for identifying non-wear time is innovative and logically coherent, we believe its performance may vary depending on the age of the population in the dataset and the employed attachment method since their approach focuses on identifying the specific shape of the acceleration signal at the start and end of a non-wear episode. In contrast, methods that simply identify non-wear time based on the absence of acceleration are less dependent on the characteristics of the population, since zero movement during non-wear is a universal trait. Our results support this idea. The \texttt{syed\_CNN} model showed poor performance across all wear locations in our study.The diminished performance of the \texttt{syed\_CNN} model might be attributed to differences in population characteristics, given that accelerometers in both the PHASAR study and the Tromsø Study[@sagelv_2019]---which served as training data for \texttt{syed\_CNN}---were affixed using elastic belts. Furthermore, the reduced accuracy on wrist data was anticipated, as the signal shape generated during device removal from the wrist is distinct from that of the thigh or hip. The \texttt{syed\_CNN} model was trained on an older population, aged between 40-84 years (mean = 62.74, SD = 10.25), whereas our study involved datasets of younger individuals aged 8.1-17.9 years (mean = 12.14, SD = 2.40) for hip and thigh data, and 14.5-16.4 years (mean = 15.4, SD = 0.37) for wrist data. Contrastingly, the \texttt{sunda\_RF} model showed acceptable performance in identifying non-wear episodes shorter than 60 minutes on both the thigh and wrist data. This suggests that the model by Sundararajan et al. is less affected by dataset differences compared to the \texttt{syed\_CNN} model. Another point worth noting is that the \texttt{syed\_CNN} model was originally trained on data with a frequency of 100 Hz, while we applied it to data with frequencies of 50 Hz and 25 Hz. Although it's unclear whether this frequency difference impacted the model's performance, we believe that the 25 Hz data is sufficient for capturing true movement behavior, given that movement frequencies are generally below 5 Hz.

The robustness of this study is significantly enhanced by the use of external validation, which offers strong evidence of methodological generalizability. However, there are limitations to consider. One major issue is the absence of a universally accepted gold standard for ground truth datasets with non-wear periods in this research area. This lack of a benchmark makes it challenging to compare performance metrics across different studies. Despite this, our approach remains transparent since it relies on raw accelerometer data, and no part of our data collection or analysis process is proprietary. It's important to note that our findings are based on a study population consisting of children and adolescents. Consequently, the results may not be directly applicable to older age groups. Additionally, while we chose to develop decision tree models for their balance of complexity and interpretability, future research could explore the efficacy of other machine learning methods like logistic regression, gradient boosting, or support vector machines. These alternative algorithms may offer different insights or advantages that could improve upon our current model.

# Paper III: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

This segment of the thesis encompasses the methods, results, and discussion for Paper III which is in preparation for SLEEP (see Appendix III). Polysomnography, the premier method for sleep evaluation, is not always feasible for extensive research due to its high costs and impracticality. Wearable accelerometers present an affordable solution. While wrist and hip-worn devices dominate sleep studies, the potential of thigh-worn accelerometers remains largely untapped. The primary aim of this paper was to assess various machine and deep learning models designed to estimate in-bed and sleep time using raw data from a tri-axial, thigh-worn accelerometer. For a robust validation, the outcomes of these models were compared with results from the ZM, which served as our gold standard for sleep assessment in this study. Additionally, a secondary objective was to assess the efficacy of these models in determining key sleep quality metrics, including sleep period time (SPT), total sleep time (TST), sleep efficiency (SE), latency until persistent sleep (LPS), and wake after sleep onset (WASO).

## Methods

### Dataset and Participants

The current study uses data from the SCREENS trial[@rasmussen_short-term_2020; @pedersen_effects_2022], which took place from June 2019 to March 2021 in the Region of Southern Denmark. The trial aimed to evaluate the impact of limiting screen media usage among Danish families. We specifically analyzed data from children between the ages of 4 and 17, with a mean age of 9.1 years, who were part of the SCREENS cohort. Our primary sources of data were accelerometer readings from Axivity AX3 devices, and EEG-derived sleep states and sleep quality metrics from the ZM device. The children wore the Axivity AX3, a discreet 3-axis accelerometer, on their right thigh, halfway between the hip and knee, to capture their movement data.

The ZM, developed by General Sleep Corporation, was utilized to extract sleep state information. The ZM device integrates advanced EEG technology and signal processing algorithms. Participants were instructed to attach three self-adhesive, disposable sensors outside their hairline when going to bed and to detach them upon waking up, ensuring consistent and clear EEG signal acquisition. Two key algorithms underpin the ZM: Z-ALG and Z-PLUS. Z-ALG is employed for sleep detection, proving ideal for single-channel EEG at-home monitoring[@kaplan_performance_2014]. Supplementing Z-ALG, Z-PLUS effectively discerns between sleep stages, aligning closely with expert-assessed PSG data[@wang_evaluation_2015]. However, in our study, we grouped all sleep stages (light sleep (N1 & N2), deep sleep (N3), and REM sleep) into a single "asleep" category. This approach streamlined the machine learning process as differentiating between sleep stages wasn't essential for our target sleep quality metrics.

@fig-paper3_flow outlines the selection criteria for children's recordings from the SCREENS study. Only recordings from ZM with complete accompanying accelerometer data and durations between 7 and 14 hours were considered. Any nights where ZM indicated sensor issues were discarded. This left us with a total of 585 nights from 151 children, averaging 3.87 nights per child (SD = 1.86). The age of these children averaged at 9.4 years with a standard deviation of 2.1 years. Across these recordings, ZM predictions spanned 696,779 epochs, each lasting 30 seconds with around 84% of the overall ZM recording time classified as sleep.

Lastly, we affirm that the SCREENS trials adhered to ethical guidelines, receiving approval from the Regional Scientific Committee of Southern Denmark. All data handling processes were in compliance with the General Data Protection Regulation (GDPR), ensuring the secure and ethical management of participant information.

![Flowchart depicting the selection process for eligible ZM recording nights included in the study.](figures/paper3_flowchart_blue.pdf){#fig-paper3_flow}

### Data preprocessing and Feature Extraction

In this study, we began by processing raw accelerometer data through a low-pass filtration step, utilizing a 4th order Butterworth filter with a 5 Hz cut-off frequency to remove high-frequency noise as described by Skotte and colleagues[@skotte_detection_2014]. Non-wear data was identified and removed using the decision tree classifier, \texttt{tree\_imp6}, as outlined in Paper II[@skovgaard_generalizability_2023], and the remaining data was then resampled into 30-second epochs to match the granularity of the ZM recordings.We then conducted feature extraction, generating 64 features that offered a comprehensive characterization of the data. These features were derived from both accelerometer and temperature signals and included temporal elements, which utilized both lag and lead values to capture dynamic data trends. Additionally, we took inspiration from Walch et al.[@walch_sleep_2019] to include sensor-independent features that encapsulate circadian rhythms, offering unique insights that are not directly discernible from sensor outputs (see @fig-paper3_sensor_independent). Including both a cosine function and a linear function to represent circadian rhythm offers a nuanced depiction of time-based patterns in sleep data. The cosine function captures the inherent 24-hour rhythmicity of the circadian cycle, reflecting the natural ebb and flow of human behaviors and biological processes like melatonin secretion[@lewy_1980]. On the other hand, the linear function provides a continuous representation of time, illustrating the progression throughout the night and accounting for gradual changes in sleep propensity as one transitions from early night to early morning. By integrating both these features, the models can holistically understand the repetitive nature of circadian rhythms and the distinct characteristics of each night, thereby potentially enhancing its predictive accuracy. We further enriched the feature set by incorporating signal characteristics such as vector magnitude, mean crossing rate, skewness, and kurtosis for each of the x, y, and z dimensions. All features are summarized in @tbl-features. The ZM recordings and the corresponding accelerometer data were then merged. Any time overlap between these two sets of data was categorized as 'in-bed' time, while the remaining time was considered 'out-of-bed.' This process yielded a comprehensive dataset that provided a 24-hour view of each participant's activity and sleep patterns, with a target feature which consisted of three classes of interest; "out of bed awake", "in bed awake", and "in bed asleep".

```{=tex}
\begingroup
```
\footnotesize

| Feature Category          | Count  | Summary of Features                                              |
|-------------------|-------------------|----------------------------------|
| Misc Features             | 3      | age, weekday, vector_magnitude                                   |
| Inclination & Orientation | 2      | incl, theta                                                      |
| Signal Means              | 4      | mean for x, y, and z                                             |
| Signal SDs                | 8      | SDs for temp, x, y, and z (30-sec and 15-min windows) and sd_max |
| Clock Proxies             | 2      | clock_proxy_cos, clock_proxy_linear                              |
| Time-Dependent            | 36     | 1-, 5-, and 10-minute lag and lead features                      |
| Crossing Rates            | 3      | mean crossing rate for x, y, and z                               |
| Signal Distributions      | 6      | kurtosis and skewness for x, y, and z                            |
| **Total**                 | **64** |                                                                  |

: All extracted features grouped by category. {#tbl-features tbl-colwidths="\[28,7,65\]"}

```{=tex}
\endgroup
```
![Sensor-independent features of circadian rhythms across two consecutive nights. A) cosinus feature, B) linear feature.](/home/esbenlykke/projects/thesis/figures/paper3_sensor_independent.pdf){#fig-paper3_sensor_independent}

Upon examining the raw ZM predictions, we observed that the device appeared to overestimate the number of awakenings among the children studied. Although the ZM software addresses many of these awakenings by counting only three consecutive awake epochs towards wake time, this approach renders the raw predictions less suitable as training data for machine learning algorithms. In fact, many of these awakenings, labeled by the ZM, would be more aptly described as arousals rather than actual awakenings. Separately, the ZM device's sleep efficiency rating for our sample was 83%, which is below recognized standards. An efficiency of 85% is considered good, and over 90% is seen as ideal. This contrasts with prior research on similar child cohorts that reported a sleep efficiency of 88.3%[@galland_2018]. Recommendations from an expert panel by the National Sleep Foundation emphasized that fewer than 2 awakenings lasting more than 5 minutes each night qualify as good sleep across all age groups[@ohayon_2017]. Additionally, it's widely recognized that children typically experience between five to eight sleep cycles every night, with awakenings most likely occurring at the conclusion of each cycle[@galland_normal_2012]. However, definitions of a "waking bout" vary across studies. Some demand at least 5 continuous minutes of wakefulness for it to be counted as one bout, while others find a 1-minute duration adequate. Of particular note, the vast majority short arousal epochs labeled as awake by the ZM did not show any relevant responses in the accelerometer signal as inferred by visual examination. This misalignment might distort underlying patterns for machine learning algorithms. While this might not be outright mislabeling, categorizing all such epochs as true awakenings would introduce noise, jeopardizing model accuracy. In light of these observations, we opted to process both the raw ZM output and versions with 5-minute and 10-minute median filtering for our model training and evaluation. This approach minimized noise and offered an awakening count more aligned with typical patterns in children's sleep (see @tbl-10 and @fig-paper3_raw_filt for details).

![The difference in number of awakenings between the raw ZM predictions vs. 5-minute, and 10-minute median filtered predictions for a random night (boy, 9 years). Grey line is the raw predictions, black line is the median filtered predictions. A: 5-minute median filter on raw ZM predictions, B: 10-minute median filter on raw ZM predictions.](/home/esbenlykke/projects/thesis/figures/paper3_zm_raw_vs_filtered.pdf){#fig-paper3_raw_filt}

### Algorithms and Modelling Strategies

The task of analyzing sleep patterns based on accelerometer data has an inherent hierarchical nature. Broadly, the goal is to distinguish between the states of being 'out-of-bed' versus 'in-bed'. Once the 'in-bed' state is determined, the finer classification into 'awake' or 'asleep' states follows. This hierarchical structure of the problem informs the first of our modeling strategies.

Our first model strategy leveraged this hierarchical structure by deploying a sequence of two models, each functioning as a binary classifier. This method simplified the prediction task by breaking down the multiclass problem into two binary phases: first identifying 'in-bed' periods, followed by determining 'sleep' periods. The output from the first binary classifiers in sequence, which estimated in-bed time, underwent a 5-minute median filter to eliminate blips of in-bed time. This step allowed us to define a singular continuous interval recognized as the SPT, representing the total duration spent in bed attempting to sleep. This SPT subsequently served as input for the next stage of binary classifiers, which predicted sleep periods within the SPT.

Four machine learning algorithms were employed in pairs in this sequential strategy:

1.  Logistic Regression: Logistic regression served as a simple and fast baseline model. However, due to its linear nature, it may struggle with capturing complex relationships and non-linear patterns present in the accelerometer data.

2.  Decision Tree: Decision trees are capable of handling non-linear patterns and are easily interpretable. However, they are prone to overfitting, particularly when dealing with complex patterns that require simultaneous consideration of multiple features. To combat this, we used a maximum tree depth of 8.

3.  Single-layer Feed-forward Neural Network (MLP): Also know as a multi-layer perceptron, MLPs can effectively capture non-linear relationships, even with their relatively simple structure. However, they tend to be more challenging to interpret compared to simpler models. Additionally, careful tuning of the network's architecture and training process is required to mitigate the risk of overfitting.

4.  XGBoost: XGBoost is a powerful algorithm known for its ability to provide highly accurate predictions and handle complex, non-linear patterns in the data. It also incorporates built-in methods to prevent overfitting. However, training XGBoost models can be computationally intensive, and interpreting the predictions it generates can pose challenges.

In contrast to the hierarchical approach, we also experimented with a standalone strategy using a bidirectional Long Short-Term Memory (biLSTM) neural network[@hochreiter_long_1997] as a multiclass classifier. Opting not to use the biLSTM in the sequential approach was about strategy diversification rather than concerns over complexity or interpretability. We wanted to compare a sequence of traditional machine learning models using an hierarchical approach with a standalone deep learning approach using the biLSTM. This distinction would underscore the advantages and limitations of each approach. Our choice to employ the biLSTM arose from its inherent capability to capture temporal sequences in accelerometer data, especially when distinguishing between the three states: 'out-of-bed', 'in-bed-awake', and 'in-bed-asleep'. Each of these states doesn't exist in isolation; they transition from one to another in patterns that are critical for accurate predictions. For instance, understanding the sequential pattern leading from 'in-bed-awake' to 'in-bed-asleep' can provide vital context, and the biLSTM is adept at capturing such temporal dependencies. Specifically designed for these three classes, the model comprised of four layers and 128 hidden units per layer, balancing complexity and efficiency. The bidirectional setup enhanced data comprehension while mitigating overfitting risks. The model processed 10-minute tensor sequences with a one-epoch step size. Previous studies, such as Sano et al.[@sano_multimodal_2019] and Chen et al.[@chen_attention_2021], have evidenced the effectiveness of LSTM models aiding in detecting sleep from accelerometer data.

### Model Training

We trained a total of four pairs of models in sequence, with each pair distinguishing between in-bed/out-of-bed and asleep/awake classes, respectively. The dataset was randomly split into a training and a testing set, each containing approximately half of the subjects. We made sure that data from the same night was not distributed across both sets. This approach was adopted to ensure that the model could effectively generalize to unfamiliar data, rather than overfitting to specific participant data similar to the data partitioning in Paper II. To tune the hyperparameters of our models, we used a specific set of hyperparameters for each type of machine learning algorithm. For the Decision Tree, we tuned the cost complexity, tree depth, and minimum number of samples required at a leaf node. The decision tree model was set up using the rpart[@rpart] engine, with tree depth ranging from 3 to 7. For Logistic Regression, implemented using the glmnet[@friedman_glmnet_2010] engine, we tuned the penalty and mixture hyperparameters controlling regularization. The MLP was implemented with a single-layer feed-forward architecture using the nnet[@nnet] engine, with the maximum number of allowable weights (MaxNWts) set to 7000 as a form of regularization to constraint the model from becoming too large and possibly overfitting the training data. The hyperparameters we tuned for this model were the number of hidden units, the penalty, and the number of epochs. The range for the number of hidden units was between 3 and 27. Lastly, the XGBoost model was configured with the xgboost[@xgboost] engine. The hyperparameters subjected to tuning included tree depth, learning rate, loss reduction, minimum number of samples required at a leaf node, sample size, and number of trees. For this algorithm, the number of trees was specifically tuned within a range of 200 to 800 to constraint training time. See @tbl-hyperparameters for an overview of the model-specific hyperparameters. We optimized all model hyperparameters using a 10-fold Monte Carlo cross-validation technique, which involves randomly partitioning the dataset into training and validation sets across multiple iterations. The optimization of hyperparameters was carried out via a grid search paired with latin hypercube sampling. By defining specific ranges for each hyperparameter, latin hypercube sampling systematically divides these ranges into segments and draws a random value from each, ensuring a well-distributed set of hyperparameter combinations. The grid search then methodically assessed various combinations of these hyperparameters to determine which combination produced the best model performance.

```{=tex}
\blandscape
\footnotesize
```
| Model                  | Hyperparameter          | Description                                         | Range                    |
|------------------|------------------|-------------------|------------------|
| Logistic Regression    | Regularization Strength | Controls magnitude of regularization penalty        | \[-10, 0\] (log scale)   |
|                        | Lasso Proportion        | Mix between Ridge (0) and Lasso (1) regularization  | \[0, 1\]                 |
| Decision Tree          | Cost Complexity         | Controls the trade-off between tree's depth and fit | \[-10, -1\] (log scale)  |
|                        | Tree Depth              | Maximum depth of the tree                           | \[3, 7\]                 |
|                        | Minimal Node Size       | Minimum number of samples required to split a node  | \[2, 40\]                |
| Multi-layer Perceptron | Hidden Units            | Number of units in the hidden layer(s)              | \[1, 10\]                |
|                        | Regularization Strength | Controls magnitude of regularization penalty        | \[-10, 0\] (log scale)   |
|                        | Number of Epochs        | Number of complete passes through the dataset       | \[10, 1000\]             |
| XGBoost                | Learning Rate           | Step size shrinkage to prevent overfitting          | \[-10, -1\] (log scale)  |
|                        | Minimum Loss Reduction  | Minimum loss reduction required for partition       | \[-10, 1.5\] (log scale) |
|                        | Minimal Node Size       | Minimum number of samples required to split a node  | \[2, 40\]                |
|                        | Observations Sampled    | Proportion of samples used per iteration            | \[0.5, 1\]               |
|                        | Number of Trees         | Total number of trees to train                      | \[200, 800\]             |

: Details of the hyperparameters tuned for each machine learning model, their descriptions, and the specific range from which values were sampled during grid search optimization. {#tbl-hyperparameters tbl-colwidths="\[20,20,40, 20\]"}

\elandscape

After identifying the best-performing hyperparameters, we proceeded to fit the models to the full training dataset. This approach allowed us to use all available data for model parameter estimation, thereby maximizing performance. To tackle the imbalance in the extracted in-bed time from our sequential modelling strategy, where the 'awake in-bed' class made up only about 15% of the training data for the sleep/wake classifiers, we employed the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE works by generating synthetic samples in the feature space to balance out the classes. Specifically, it creates synthetic observations by randomly selecting a minority class instance and its nearest neighbors, then producing a new instance that is a blend of the two. This method, as outlined by Chawla et al. [@chawla_smote_2002], mitigates biases during model training that arise when models are skewed towards the majority class. Using the themis R package [@themis], we implemented SMOTE to achieve a balanced distribution of training samples for both classes. The optimization was driven by the F1 score as a metric since it harmonizes precision and recall, rendering it more resilient to class imbalance.

In parallel to these sequential models, we trained the biLSTM model to classify three distinct classes: "out-of-bed awake", "in-bed awake", and "in-bed asleep". The data for this model was divided into training, validation, and test sets, adhering to a 50/25/25 split ratio. Again, caution was exercised to avoid having data from the same night across different sets. For efficient and adaptive learning, the Adam optimizer was used during the training process. Given that we were dealing with a multiclass classification task with mutually exclusive classes, the cross-entropy loss function was employed. At the output layer, a softmax activation function was applied to obtain a probability distribution over the classes. We employed early stopping with a patience of 3 epochs, ceasing training if no improvement in the validation loss was observed over three consecutive epochs.

### Model Validation

In the current study, we utilized standard evaluation metrics derived from confusion matrices to assess the performance of each model on an epoch-to-epoch basis. These include $$accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$ $$sensitivity = \frac{TP}{TP+FN}$$ $$specificity = \frac{TN}{TN+FP}$$ $$precision = \frac{TP}{TP+FP}$$ $$NPV = \frac{TN}{TN + FN}$$ $$F_1 = 2 \cdot \frac{precision \cdot sensitivity}{precision + sensitivity}$$

where $NPV$ is negative predictive value, $F_1$ is the F1 score, $TP$ is true positives, $FP$ is false positives, $TN$ is true negatives, and $FN$ is false negatives.

In our sequential modelling strategy, the models in the first stage that carried out the binary classification task of distinguishing between in-bed and out-of-bed states was evaluated using the F1-score, accuracy, sensitivity, specificity, and precision. In the second stage of the sequential modelling strategy, models responsible for distinguishing between 'asleep' and 'awake' states were evaluated using the same metrics, with the addition of the negative predictive rate. Given the class imbalance, the F1 score was calculated as an unweighted macro-average, allowing the metric to represent predictions for both classes effectively. We also scrutinized a multiclass biLSTM classifier using the same metrics, interpreting its multiclass output as two separate binary classifications: out-of-bed versus all other states, and in-bed-awake versus in-bed-asleep. Moreover, to give a comprehensive view of model performance, we present confusion matrices for the entire dataset, covering both in-bed and out-of-bed data. These matrices report relative counts, column percentages for accurate predictions of the true class, and row percentages for correctly classified predictions. Both in-bed/out-of-bed and awake/asleep classification tasks were treated as binary, designating 'in-bed' and 'asleep' as positive labels and 'out-of-bed' and 'awake' as negative labels, in line with prior studies [@hjorth_measure_2012; @kushida_comparison_2001].

To evaluate how well our models performed in generating sleep quality metrics, we employed Bland-Altman plots and Pearson correlations. Specifically, the Bland-Altman approach was used to estimate the level of agreement between two different measurement techniques. Given the nature of our dataset, which contains multiple observations per subject but not necessarily equal number of observations, we used a bootstrap procedure to account for this extra variability. We initially calculated the mean difference or bias, and then determined the limits of agreement (LOA) as the bias ± 1.96 times the standard deviation of these differences. Given the possibility of non-normal distribution and skewness in our data, we opted for a bias-corrected and accelerated bootstrap method [@diciccio_bootstrap_1996]. This allowed for more accurate estimation, taking into account intra-subject variability. Using 10,000 bootstrap replicates, we estimated the 95% confidence intervals for both the bias and LOA, thereby ensuring robust measurements. The sleep quality metrics conformed to ZM definitions and included the following:

1.  Sleep Period Time (SPT) - This refers to the total duration of time in bed with the intention to sleep, which is defined as the time from the start to the end of the ZM recording.
2.  Total Sleep Time (TST) - This is the time spent asleep within the SPT.
3.  Sleep Efficiency (SE) - This is the ratio between TST and SPT, representing the proportion of the sleep period that was actually spent asleep.
4.  Latency Until Persistent Sleep (LPS) - This metric represents the time it takes to transition from wakefulness to sustained sleep. It is calculated as the time from the beginning of the ZM recording until the first period when 10 out of 12 minutes are scored as sleep.
5.  Wake After Sleep Onset (WASO) - This refers to the time spent awake after initially falling asleep and before the final awakening. In our analysis, a period is counted as 'awake' only if it consists of 3 or more contiguous 30-second epochs which is also how the ZM summarizes WASO.

The technical frameworks used for model development and analyses were R version 4.3.0 [@rcoreteam_2023] along with the Tidymodels[@kuhn_tidymodels_2020] and Tidyverse[@wickham_tidyverse_2019] package suites. For the biLSTM model, we used Python version 3.10.6 [@vanrossum_python_2009] and PyTorch [@paszke_pytorch_2019].

## Results

As indicated in @tbl-10, the application of 5-minute and 10-minute median filters led to modifications in the sleep quality metrics derived from ZM predictions. SPT remained consistent between raw and filtered data sets, with a mean duration of 9.2 ± 2.1 hours, which aligns with the length of the ZM recording. TST and SE increased in the filtered data, implying the filters designate some wakefulness as sleep. Specifically, the mean TST rose from 7.7 ± 1.9 hours in the raw data to 8.1 ± 2.0 hours with a 5-minute filter and to 8.2 ± 2.1 hours with a 10-minute filter. Similarly, SE increased from an initial mean of 82.6 ± 12.0% to 86.4 ± 12.7% and 87.5 ± 12.9% for the 5-minute and 10-minute filters, respectively. Furthermore, the LPS also saw an increase, implying that the filters are removing brief asleep periods at the onset of sleep, thereby lengthening the time it takes to achieve persistent sleep (i.e., 10 out 12 minutes classified as asleep). On the other hand, the WASO metric decreased from a raw average of 39.0 ± 33.6 minutes to 30.6 ± 46.8 minutes and 22.3 ± 55.4 minutes in the 5-minute and 10-minute filtered data, respectively. Notably, the application of these filters also led to a significant reduction in the average number of awakenings per night. In the unfiltered data, the mean number of awakenings (or arousals) stood at 34.46 ± 11.33, which dropped to 4.43 ± 3.26 and 1.95 ± 2.01 in the 5-minute and 10-minute filtered datasets, respectively.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-10
#| tbl-cap: "Overview of characteristics of the ZM sleep quality summaries per night (585 nights from 151 children). Values are represented as mean (SD). Hrs: hours, min: minutes."

tbl_10
```

```{=tex}
\endgroup
```
### Performance on Epoch-to-Epoch Basis

As delineated in @tbl-11, the epoch-to-epoch evaluation for predicting in-bed time shows virtually identical performance across the various model types. The F1 score fluctuates slightly, ranging from 94.4% in the Decision Tree model to 95.4% in the XGBoost model. Likewise, accuracy varies minimally from 95.3% for the Decision Tree model to 96.1% for the XGBoost model. Other metrics such as sensitivity, precision, and specificity also exhibit uniform performance across the different models. While the XGBoost model does exhibit the highest performance with an F1 score of 95.4% and an accuracy of 96.1%, it only marginally surpasses the other models in these metrics.

\pagebreak

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-11
#| tbl-cap: Performance metrics of the classification of in-bed/out-of-bed time of the included models.

tbl_11
```

```{=tex}
\endgroup
```
@tbl-12 illustrates the performance metrics for differentiating sleep/wake of all included models. In raw ZM predictions, the XGBoost model stood out with an F1 score of 76.2% and a precision of 92.8%. However, the biLSTM model struggled in this category, particularly with specificity, which was the lowest at 26.9%, even though its sensitivity was notably high at 98.1%. When a 5-minute median filter was applied, the XGBoost model further improved with an F1 score of 79.2%, and an increased NPV of 74.0%. The Decision Tree, during this phase, achieved an F1 score of 75.5% but a decreased specificity of 59%. The Neural Network's performance remained consistent, with an F1 score around 71.7% and precision of around 95.8%. The application of a 10-minute median filter saw the XGBoost's performance peak with an F1 score of 80.9% and a precision of 94.9%. In contrast, the biLSTM improved slightly with an F1 score of 70.9% but still lagged in specificity at 42.4%. Overall, while models like XGBoost seemed to demonstrate the most potential, the consistent challenge across models remained in achieving high specificity values.

```{=tex}
\begingroup
```
\footnotesize

```{r}
#| echo: false
#| message: false
#| label: tbl-12
#| tbl-cap: Performance metrics of the sleep/wake classification of the included models.

tbl_12
```

```{=tex}
\endgroup
```
@fig-bin_conf_mat and @fig-mul_conf_mat presents a comprehensive set of confusion matrices generated from data that includes both out-of-bed and in-bed periods. These matrices offer insights into the epoch-to-epoch performance of all sequential models when differentiating between 'awake' and 'asleep' states, irrespective of whether the subject is in bed or out of bed. However, it's crucial to acknowledge that the sequential models, owing to their binary nature, are not equipped to directly classify the 'in-bed-awake' state. In contrast, the biLSTM model, which does identify the 'in-bed-awake' state as a separate class, seems to be less successful in classifying this specific state.

![Confusion matrices for the binary predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage. i) decision tree, ii) logistic regression, iii) MLP, iv) XGBoost](figures/all_binary_conf_mats.pdf){#fig-bin_conf_mat}

![Confusion matrices for the biLSTM predictions. The middle of each tile is the normalized count (overall percentage). The bottom number of each tile is the column percentage and the right side of each tile is the row percentage.](figures/all_multiclass_conf_mats.pdf){#fig-mul_conf_mat}

### Evaluation of Sleep Quality Metrics

@tbl-13 presents a comparative analysis of the included models used to predict various sleep quality metrics (SPT, TST, SE, LPS, WASO) using the 5-minute median filtered ZM predictions.

Against the sleep quality metrics derived from the ZM raw data, the biLSTM model showed a -36.7-minute bias for SPT, while XGboost stood out for its minimal bias and correlation score of 0.56. Across metrics, varying biases emerged; for instance, in SE, the MLP and biLSTM displayed contrasting biases. For LPS and WASO, biLSTM generally showed strong biases, with overall correlations being poor.

Using the sleep quality metrics of the 5-minute median filtered ZM data, biLSTM's bias for SPT remained large. XGboost consistently showed the best correlations and smallest biases. The correlations for SE, LPS, and WASO, however, remained weak across most models whith XGboost again performing best.

Lastly, against the sleep quality metrics of 10-minute median filtered ZM data, biLSTM's bias persisted, especially in SPT, whereas XGboost and the Decision Tree exhibited moderate correlations. Notably, Logistic Regression and the MLP displayed pronounced biases, especially in TST and SE. Overall, while biases varied across models and metrics, again, XGboost consistently showed the highest correlations with concurrent small biases, indicating its stronger predictive performance relative to other models.

Overall, the decision tree model consistently underestimated SPT, TST, and SE, and overestimated LPS and WASO in comparison to ZM. The logistic regression model had similar trends, with more pronounced underestimation in TST and overestimation in LPS. The MLP also exhibited similar bias as the decision tree and the logistic regression models, but with a higher overestimation in WASO. On the other hand, the XGBoost model showed least bias among all, especially in its 5-minute median predictions. The biLSTM was the only model to overestimate TST, and as a consequence overestimate SE and underestimate LPS and WASO. This was true across raw and 5-minute filtered data but this trend did not apply to the 10-minute filtering. Considering LOA, the decision tree had higher variability in the differences across different sleep quality metrics and filterings, particularly for LPS and WASO, which indicates lower agreement with ZM. Other models had comparable LOA but with notable exceptions. For example, TST LOA for the logistic regression model was particularly wide in the 5-minute median predictions. Correlation-wise, the pearson coefficient, revealed that the XGBoost model consistently had the highest correlation with ZM across all sleep quality metrics and filtering methods Notably, the XGBoost's 5-minute median predictions showed the strongest correlation (0.66) for TST among all models and filterings.

```{=tex}
\begingroup
```
\scriptsize

```{r}
#| echo: false
#| message: false
#| label: tbl-13
#| tbl-cap: Summary of bias, limits of agreement, and Pearson correlation for the included sleep quality metrics (SPT, TST, SE, LPS, WASO) across all included machine learning and deep learning models (decision tree, logistic regression, MLP, XGBoost, and biLSTM) on raw ZM predictions, 5-minute and 10-minute median predictions. Each value is provided with its 95% confidence interval.

tbl_13

```

```{=tex}
\endgroup
```
As established from the @tbl-13, the XGBoost model trained on the 5-minute median filtered ZM data seemed to be the best performing model configuration. The Bland-Altman plot and scatterplot presented in @fig-xgb_ba_cor illustrate the level of agreement between the this XGBoost model and ZM-derived sleep quality metrics that were also median-smoothed over 5 minutes. For the sleep quality metrics SPT and TST, the bias is notably close to zero, revealing a minimal average difference with the ZM. The scatterplot for SPT also suggests a moderate linear correlation between the model's predictions and the ZM-derived metrics. The TST scatterplot further indicates a slightly higher correlation, mainly due to the lack of extreme outliers. In contrast, the remaining sleep quality metrics, namely SE, LPS, and WASO, show signs of heteroscedasticity unlike SPT and TST. While a moderate positive linear correlation exists between the XGBoost model and the ZM-derived SE metrics, poorer correlations are observed for LPS and WASO.

![Comparison of sleep quality metrics derived from the XGBoost model trained on the 5-minute smoothed ZM predictions. The left column displays Bland-Altman plots. Dashed lines represent the bias (the average difference between the two measurements) and LOA, with the 95% confidence intervals represented as the grayed areas. The right column displays scatter plots of XGBoost-derived vs ZM-derived sleep quality metrics. The dashed line represents the identity line, while the full-drawn line represents the best linear fit. Pearson's correlations are annotated in the upper left corner](figures/median_5_xgboost_ba_cor.pdf){#fig-xgb_ba_cor}

## Discussion

In this paper, we assessed a range of machine learning models for estimating in-bed time, sleep time, and derived sleep quality metrics using data from thigh-worn accelerometers. We approached the task both as a hierarchical classification problem, applying models in sequence, and as a multiclass problem using a deep learning model specially tailored for time time series data. These models were trained and assessed using both raw and median-filtered sleep estimates derived from the ZM EEG-based sleep monitor. Overall, all models exhibited strong performance in predicting in bed time on an epoch-to-epoch basis. However, distinguishing between wakefulness and sleep during these in-bed periods proved to be more challenging. Interestingly, while the multiclass biLSTM model performed well in terms of F1 score, precision, and NPV for detecting epoch-to-epoch sleep, it lagged behind in deriving sleep quality metrics when compared to the XGBoost model due to low specificity. The XGBoost model outperformed all included models across every evaluation metric, including epoch-to-epoch prediction and sleep quality metrics. Nonetheless, it's worth noting that all models struggled with relatively low specificity values, indicating a common difficulty in accurately identifying awake epochs during time spent in bed. We also observed performance improvement in all models when 5-minute and 10-minute median filters were applied. This filtering approach resulted in increased total sleep time and sleep efficiency metrics while reducing wake after sleep onset and the number of awakenings. Of all the models, the XGBoost demonstrated the smallest bias and the highest correlation with the ZM sleep quality metrics, making it the most robust choice for this particular application.

While there is limited existing research on the epoch-to-epoch effectiveness of thigh-worn accelerometers in classifying in-bed time, Carlson and colleagues[@carlson_validity_2021] have offered valuable insights. Their study demonstrated that both a third-party algorithm called "ProcessingPal" and a proprietary algorithm, to the activePAL device, named "CREA" were able to achieve high accuracies of 91% and 86%, respectively. Evaluated against self-reported measures in adolescents and adults, these algorithms achieved impressive F1 scores of up to 95% and 96%. These results align with our models, which also achieved over 95% in both F1 and accuracy scores when identifying in-bed epochs, representing time in the SPT. However, it's worth noting that all models in our study, with the exception of XGBoost, tended to underestimate SPT. The biLSTM model displayed the most significant underestimation, with a bias of -36 minutes. This aligns with previous research by Winkler et al.[@winkler_identifying_2016], who reported a similar trend in both young-middle-aged and older adults. Their algorithm showed a moderate correlation with diary-recorded waking times but overestimated waking wear time by more than 30 minutes, resulting in an underestimation of in-bed time. This underestimation was further validated by Inan-Eroglu et al.[@inan-eroglu_comparison_2021], who found an underestimation of 9.8 minutes when comparing Winkler et al.'s algorithm to self-reported measures in middle-aged adults. Contrastingly, another study reported only a slight underestimation of in-bed time in middle-aged and older adults[@van_der_berg_identifying_2016]. They used a unique algorithmic approach that quantified the number and duration of sedentary periods to ascertain time in bed and active periods to identify wake times. Lastly, it's essential to clarify that strong predictive performance in identifying in-bed time doesn't automatically imply accurate predictions for broader sleep quality metrics. Capturing awake periods during in-bed time, a critical factor for assessing other derived sleep quality metrics, isn't effectively handled by simply predicting in-bed time. This distinction between actual sleep and time spent in bed while awake is often overlooked but is vital for a more comprehensive understanding of sleep quality.

To our knowledge, Johansson and colleagues[@johansson_development_2023] are the only researchers who have gone beyond merely reporting "waking time" and "in-bed time" to provide epoch-to-epoch performance metrics for sleep scoring with thigh-worn accelerometers. Utilizing a single-night evaluation dataset comprising 71 adult subjects, they managed a mean sensitivity of 0.84, a specificity of 0.55, and an accuracy of 0.80. Similarly, our models achieved a high sensitivity of above 97%, but struggled, like Johansson et al.'s algorithm, in detecting in-bed awake epochs. This struggle is manifested in our low specificity scores, which ranged from 54.7% to 76.4%. This challenge is not solely confined to thigh-worn devices. Conley et al.'s[@conley_agreement_2019] meta-analysis reported issues with wrist-worn accelerometers as well, noting mean values of 0.89 for sensitivity, 0.88 for accuracy, and a low 0.53 for specificity among healthy adults. Patterson and colleagues[@patterson_40_2023] also recently summarized various heuristic algorithms, machine learning, and deep learning models for sleep prediction, finding mean sensitivity and specificity scores of 93% and 60% in data from wrist-worn devices, respectively. These data collectively highlight the persistent difficulty in automating the identification of periods when individuals are awake yet still in bed. Interestingly, our study revealed a divergence from most previous research concerning the overestimation of LPS and WASO by several of our models. This overestimation is reflected in the low NPV scores, indicating that a limited portion of the wake predictions were accurate with the exception of the biLSTM achieving NPV scores up tp 81.8%. This inconsistency might be attributed to the SMOTE we used to balance the dataset. If the synthetic 'wake' samples created by SMOTE don't accurately represent the actual 'wake' data, it could cause the models to misclassify certain 'sleep' epochs as 'wake'. Consequently, this could lead to inflated LPS and WASO estimates, as the models would incorrectly identify more instances of wakefulness during sleep.

The application of the SMOTE in our study likely enhanced the performance of various models by addressing class imbalance issues. However, the introduction of synthetic "wake" samples through this method posed a challenge as they might not be fully indicative of genuine wake data. This could explain why most models, excluding the biLSTM which was not trained on SMOTE-processed data, underestimated TST and SE. Contrarily, the XGBoost model, trained on SMOTE-processed data, managed to navigate these synthetic "wake" samples more effectively and did not overestimate TST as much as other models. This is evident from the Bland-Altman statistics for the XGBoost model trained on 5-minute median-filtered ZM predictions showing a mean difference of -7 minutes for TST and -1.1% for SE. The limits of agreement for these metrics spanned from -95.5 to 81.4 minutes and -15.6% to 13.3% respectively. This suggests that the XGBoost model successfully maintained a balance between sensitivity and specificity without being overly swayed by the synthetic "wake" samples. The robustness of the XGBoost model, especially when faced with synthetic samples, may arise from its gradient boosting mechanism, which involves learning iteratively from the errors of prior models. Unlike other models, such as the single decision tree, which makes decisions based on impurity measures in one go, or logistic regression, which tries to minimize the loss across the entire dataset, gradient boosting in XGBoost specifically corrects for errors made by previously trained trees with each subsequent tree. Even when compared to iterative algorithms like MLP or biLSTM, XGBoost's approach of directly focusing on and correcting mistakes might make it particularly resilient to inaccuracies or noise introduced by synthetic data. This attribute could lead to better overall model performance.

Sleep detection methods are generally used in two distinct scenarios: night-only recordings and 24-hour recordings. For night recordings, SE and LPS can be readily derived since SPT can be inferred from the length of the recording itself, as indicated by studies from Conley et al. and Patterson et al[@conley_agreement_2019; @patterson_40_2023]. In contrast, when applied to 24-hour recordings, most sleep detection methods face the challenge of inferring SPT without the aid of sleep diaries, as presented by several studies[@girschik_validation_2012; @doherty_large_2017; @anderson_assessment_2014; @dozydave2023snooze]. This limitation prevents these methods from generating sleep quality metrics dependent on SPT. To address this issue, we designed models capable of distinguishing between in-bed awake time and in-bed asleep time, as well as out-of-bed awake time, over a full 24-hour period. This innovation allows our models to estimate a comprehensive range of commonly used sleep quality metrics. In a similar vein, Van Hees et al.[@van_hees_estimating_2018] proposed an algorithm for determining SPT using wrist-worn devices, an approach that was subsequently validated by Plekhanova and her team[@plekhanova_validation_2023]. When combined with other methods, this algorithm enables the estimation of additional sleep quality metrics based on the identified SPT. Van Hees et al. reported favorable results with low mean differences when compared to self-reported measures and PSG for SPT, a finding later corroborated by Plekhanova et al. However, both studies also highlighted challenges in achieving good agreement on metrics such as LPS and WASO, revealing low agreement with PSG. These challenges in accurately detecting wakefulness during in-bed time are similar to the issues we encountered in our own study.

In evaluating various sleep quality metrics, our study identified that LPS consistently exhibited the largest mean error in relation to the actual time allocated to it. This challenge in accurately classifying the initial periods of SPT was further corroborated by poor Pearson correlations between LPS obtained from model predictions and the ZM. Among all models assessed, the XGBoost model emerged as the most reliable, yet it overestimated LPS by an average of 26.4 minutes for models trained on unfiltered ZM predictions. This increased to 28.5 and 34.5 minutes when the training data was 5-minute and 10-minute filtered ZM predictions, respectively. This discrepancy is not unique to our study; it is on par with the mean error of 23 minutes in sleep latency reported by Johansson et al[@johansson_development_2023]. Johansson et al. suggest that the discrepancy with the gold standard is likely due to the multifaceted nature of the sleep state, which is a complex physiological process. Short awakenings or sleep episodes may not necessarily correspond to noticeable changes in thigh movement, making them difficult to detect and accurately classify. These observations are consistent with findings on wrist-worn devices by Conley and colleagues[@conley_agreement_2019], who reported that correlations between accelerometer data and PSG sleep onset latency (equivalent to LPS) varied greatly across studies. The mean correlation was only 0.2, underscoring the challenges in leveraging accelerometry alone for estimating LPS.

Moreover, when compared to other models like the Van Hees algorithm[@hees_novel_2015], Oakley rsc[@palotti_benchmark_2019], and LSTM-50[@palotti_benchmark_2019] as evaluated in the Patterson et al. study[@patterson_40_2023], our XGBoost model displayed narrower LOAs for TST, SE, and WASO. Interestingly, the LOAs were also narrower when pitted against the algorithm tailored for thigh-worn devices by Johansson et al[@johansson_development_2023], albeit not for SPT. Despite these promising facets, it is important to note that all methods, both from this study and the literature, showcase wide LOAs. This implies a high level of variability in sleep quality metrics derived from accelerometry, cautioning against its use as a stand-alone alternative to EEG-based ZM or PSG for individual-level sleep assessments. The presence of extreme outliers in our study appeared to exacerbate the width of the LOAs, suggesting that current methods are better suited for group-level sleep quality metrics. As a result, there is a pressing need for further refinement to enhance the reliability and validity of these models for individual sleep assessments.

In our study, we opted to use the ZM as the reference method for sleep measurement, as opposed to the generally accepted gold standard, PSG. While this choice could contribute to the observed discrepancies between our models and ZM outcomes, we argue that the use of ZM has distinct advantages. For one, ZM facilitates multiple consecutive nights of recording in a free-living environment[@pedersen_self-administered_2021], thereby capturing intra-individual variations in sleep patterns. This is an aspect often impractical to achieve with PSG. Additionally, the use of ZM allowed us to incorporate more nights into our study than is typically possible with PSG-based studies. This is evident when comparing our data set to the more limited Newcastle dataset, which consists of only 28 participants each with a single night of recoring[@hees_newcastle_2018]. Despite its benefits, we found that the raw ZM outputs were not ideally suited for developing machine learning models, primarily due to a low signal-to-noise ratio, as indicated in @fig-paper3_raw_filt. The ZM device itself employs certain filtering processes to mitigate this issue when generating sleep quality metrics. For example, WASO is determined using contiguous epochs of 3 minutes, and sleep only contributes to sleep quality metrics if 10 out of 12 minutes are categorized as sleep. To enhance the effectiveness of our machine learning algorithms, we applied median filters to the raw ZM predictions, which had a notable impact on the derived sleep quality metrics. The application of these filters led to several changes. Specifically, the mean WASO dropped from 39 minutes in the raw predictions to 30.6 minutes when using a 5-minute median filter, and further decreased to 22.3 minutes with a 10-minute median filter. Likewise, TST, SE, and LPS all increased upon the application of the 5-minute and 10-minute median filters. These shifts suggest that the median filters reclassify brief instances of wakefulness as sleep, and similarly, eliminate short awakenings. Despite these alterations, the sleep quality metrics derived from the median-filtered predictions remained largely consistent with those from the raw predictions, validating the approach we took in this study.

Our current study offers contributions to the field of sleep estimation methods, particularly in the use of thigh-worn accelerometers. One of the primary strengths of the research lies in its ability to distinguish between in-bed awake time and in-bed asleep time, as well as out-of-bed time. This distinction is crucial for extracting essential sleep quality metrics without the assistance of sleep diaries or other ways to determine the SPT. Additionally, by evaluating multiple nights per subject, the study offers valuable insights into intra-subject sleep variability, another important factor for sleep assessment. However, there are limitations to our approach. Most notably, we utilized the ZM as our reference method, which is not considered the gold standard for sleep measurement. This choice might have impacted the validity of our findings. For future work, it would be beneficial to employ PSG as a reference, despite its own set of limitations, to provide a more accurate comparison and to easier extend to comparisons of other studies utilizing PSG as the gold standard. Another limitation is the lack of external validation for our models, which confines the applicability of our findings primarily to child populations of normal sleepers. Finally, another significant limitations of importantce is that the sequential strategy, during its initial step, identifies only one continuous SPT window. Consequently, it cannot detect multiple SPTs in a single night. So, if participants wake up for an extended period, engage in significant physical activity, and then return to sleep later, the detected SPT will terminate at the wake point, and any subsequent sleep will not be recognized.

# Thesis Conclusions

The importance of sleep health and the limitations of conventional PSG examinations have highlighted the potential of wearable sensor systems as complementary measurement methods. In the previous sections of this thesis, it has been shown that accelerometers as a platform for long-term activity monitoring can be used to assess quantitative and qualitative aspects of sleep. Specifically, the findings of this thesis offer a deep dive into the methods and tools employed in the manual annotation of in-bed periods, the classification of non-wear periods, and the application of various machine learning models to detect sleep in accelerometry data.

Regarding the objectives set in the beginning of the thesis, it can be stated that:

-   We presented a method using Audacity for manually annotating individual bedtime and wake-up times in raw accelerometry data from thigh- and hip-worn accelerometers. The annotations showed good to excellent absolute agreement with in-bed and out-of-bed timestamps as determined by the ZM sleep tracking device and with prospective sleep diaries, addressing the objective of validating the accuracy of these annotations. Additionally, the manual annotations displayed excellent inter- and intra-rater agreement between the three human raters and between test/retest conditions.

-   In alignment with the first objective of Paper II, we evaluated decision tree models using data from thigh and hip-worn accelerometers to detect non-wear time, with a focus on the role of surface skin temperature. Our findings highlighted the strong performance of these models, particularly when incorporating surface skin temperature, closely mirroring the effectiveness of the simple hueristic algorithms, \texttt{cz\_60} and \texttt{heu\_alg}, for non-wear episodes longer than 60 minutes across all wear-locations: wrist, hip, and thigh. Addressing the second objective, our comparative analysis showed notable performance distinctions among machine-learned models and heuristic algorithms. Specifically, the decision tree model trained on the six most important predictors excelled for short non-wear episodes on all wear-locations, while some the more complex models, the random forest (\texttt{sunda\_RF}) and deep learning model (\texttt{syed\_CNN}), faced limitations, especially in identifying shorter non-wear episodes.

-   In accordance with the primary objective of Paper III, we evaluated various machine learning and deep learning models to estimate in-bed and sleep time, benchmarked against sleep recordings of the ZM. All models showcased strong performance in predicting in-bed time on an epoch-to-epoch basis, albeit with challenges in distinguishing wakefulness and sleep during these periods. The secondary objective entailed assessing the models' capability in quantifying commonly used sleep quality metrics, validated against the ZM sleep tracking device. Among the evaluated models, the XGBoost model excelled across every evaluation metric, including epoch-to-epoch prediction and sleep quality metrics, outperforming others like the multiclass biLSTM model, especially in deriving sleep quality metrics due to its higher specificity. The application of 5-minute and 10-minute median filters resulted in a performance uplift for all models, although these filterings at the same time increased predicted time spent asleep, thus, increasing sleep quality metrics such as TST and SE while reducing WASO and the number of awakenings. The XGBoost model, exhibiting the smallest bias and highest correlation with the ZM sleep quality metrics, emerged as the most robust choice for this application.

# Perspectives

While the findings of this thesis has bridged knowledge gaps, further horizons beckon exploration. The following will discuss implications of the findings in this thesis and potential avenues for future reseach.

## Implications of Findings

As demonstrated in Paper I, the manual annotation method described for labeling in-bed periods using raw accelerometry data eliminates the need for sophisticated preprocessing to extract meaningful sleep information. This manual annotation serves as a valid and consistent alternative to sleep diaries or in-bed time determined by EEG recordings. Accurate and correct labeling is paramount, not just in sleep research, but across many fields, as it forms the foundation for effective machine learning applications. Reliable labels, such as those provided by manual annotations, allow for more accurate model training, which in turn leads to better predictions and insights. An accelerometry dataset enhanced with these manual annotations---especially in the absence of accompanying sleep diaries or other in-bed time estimates---can streamline and expedite data analysis workflows in sleep research, resulting in increased efficiency.

Through the utilization of simple decision tree models and by recognizing the role of surface skin temperature, as presented in Paper II, we have advanced in our capability to accurately identify non-wear time. This may reduce potential data inaccuracies and biases towards excessive sleep or sedentary time, ensuring the integrity of our accelerometry datasets. Additionally, our detailed evaluation of different wear locations---including the hip, thigh, and wrist---offers researchers critical insights into the efficacy variances of different methods based on the device's position. We've also placed significant emphasis on the value of external validation, highlighting the essential need for stringent validation protocols before the declaration of model performance. Our findings clearly indicate that heuristic algorithms stand out as both population and wear-time agnostic. They offer enhanced flexibility in study design. However, when it comes to detecting shorter non-wear episodes---a domain where traditional algorithms often falter---it's worth considering machine learning model-based approaches but such should be carefully selected.

By assessing a range of machine learning and deep learning models against an EEG-based sleep monitor in Paper III, we've gained critical insights into the algorithms most apt for precise sleep detection given the employed feature set and data at hand. This knowledge promotes the selection of the most effective models, enhancing the precision of sleep detection from accelerometry-based devices. Moreover, these validated methods using accelerometers not only facilitate sleep detection without the need for sleep logs or diaries but also allow for easy assessment of previously collected data for sleep patterns. These approaches offer a promising, cost-efficient, and less obtrusive replacement for traditional EEG-based sleep monitoring methods. By doing so, they can make sleep studies more universally accessible, ensuring that a broader spectrum of the population can benefit from and participate in such studies.

Conclusively, the research presented in this thesis, while significant in its attributions, reinforces a timeless academic tradition: with each answer, new questions arise, directing future research in the intersection of sleep health and technology.

## Initial Study Design Considerations

Research is inherently iterative, building upon existing knowledge while continuously adapting based on insights and practical experiences. At the outset of our project, we intended to harness the advanced capabilities of the SomnoTouch™️ RESP[@somnotouch], an FDA-approved ambulatory PSG system, to monitor in-home sleep patterns alongside concurrent accelerometry assessments. Our goal was to create a comprehensive, multi-dimensional dataset. With manual annotation of in-bed periods, as outlined in Paper I, we aimed to establish a gold standard dataset, laying the groundwork for training various machine learning models dedicated to sleep detection. Yet, as is customary in research, we sometimes encounter roadblocks when translating theoretical designs into practical applications. Using the SomnoTouch™️ RESP for children's in-home monitoring proved challenging. Despite the device's advanced features and accuracy, achieving consistent and comfortable adherence of its many sensors with children presented substantial hurdles. Often, we were only able to obtain a few hours of high-quality data before losing connectivity with the electrodes, typically because the children were lying restlessly in bed while awake or experiencing restless sleep later on. Additionally, there were several instances where the signal was lost as the children went to bed, likely due to the 'diving head first into the pillow' effect.

Before terminating my data collection, I collected data from 55 children, employing stronger EEG-electrode adherence paste with each trial and providing progressively stricter instructions on how to carefully manage the wiring and electrodes when going to bed. Despite these measures, the data quality remained subpar. Such challenges prompted a reevaluation and adaptation of our initial methodology. These experiences underscore the importance of research tools being versatile, not just in their precision, but also in accommodating the peculiarities of the target population and environment. Future trials with improved attachment methods, or perhaps utilizing an EEG helmet, could be beneficial. However, a balance needs to be struck: how many sensors and how much attachment equipment can a child comfortably sleep with before it ceases to represent natural, free-living sleep? Ultimately, the ideal dataset for sleep research would seamlessly blend free-living accelerometer data with gold standard sleep recordings over multiple days. Although attaining this high-quality dataset is challenging, it's pivotal for advancing sleep research.

## Addressing Non-Wear Challenges

In physical activity research, accurate monitoring with wearable devices is essential, especially for non-wear detection. Misclassifying non-wear periods can lead to overestimated assessments, altering our understanding of actual sleep or sedentary behaviors. Such inaccuracies might affect the trust in wearable devices for clinical purposes. This accuracy becomes particularly vital in long-term studies spanning several weeks. Participants may intermittently remove their devices during these periods, leading to varied non-wear breaks, making accurate detection crucial.

In Paper II, while our data showed a dominance of non-wear episodes longer than 60 minutes, other research points in the opposite direction[@vert_detecting_2022; @aadland_comparison_2018; @jaeschke_variability_2018; @hutto_identifying_2013]. This variation stresses the need for refining existing non-wear detection algorithms. For instance, Zhou et al.'s algorithm[@zhou_classification_2015] was based on 15-minute minimum reference periods for wear and non-wear. Although the researchers acknowledged limitations for shorter durations, they believed the broader implications were minimal. Yet, errors in identifying brief non-wear periods could skew individual outcomes, especially in longitudinal studies where these errors accumulate.

With the shift to raw accelerometer data for non-wear detection, we face challenges and uncertainties in data management and processing. One significant concern is the sampling frequency. Algorithms are often designed for specific sampling rates, and using them on data with different frequencies could lead to errors. The nuances of resampling to different frequencies remain murky. Interpolative sampling algorithms, with their inherent assumptions, might not always provide accurate results. It's essential to understand how this affects accelerometer values. Additionally, accelerometer calibration, typically done during manufacturing, plays a role. Sometimes, recalibrating data, as with methods like auto-calibration[@hees_2014], can be beneficial, and understanding its impact on non-wear detection is crucial.

The ongoing challenge is crafting algorithms that can consistently detect non-wear across different devices, user demographics, and wear locations. If a one-size-fits-all model proves elusive, specialized models for specific scenarios might be the solution. Paper II underlined the difficulties in creating universal methods for short non-wear periods. However, by incorporating skin temperature data, we achieved commendable F1 scores across wear locations, even though some of the results may not meet the threshold for individual reliability.

## Improving Our Sleep Models

In Paper III, we highlighted a limitation associated with our model's ability to detect only one SPT per night. Given the specific sample we analyzed, the detected SPTs ranged between 7.4 to 12 hours, with an average of 9.9 hours and a standard deviation of 0.8 hours, as determined by the XGBoost model. Given this range, it's improbable that our dataset experienced interrupted SPTs. However, it's worth noting that the nights under study were meticulously selected based on the ZM recordings to exclude instances with abnormally short or extended SPTs. In different datasets, there's potential for some nights to be truncated due to extended periods of wakefulness characterized by significant physical activity, or even napping behaviors. This limitation is a noteworthy drawback in our methodology, warranting attention in future revisions. Addressing this concern and allowing for the detection of multiple SPTs should be a feasible programming enhancement, though it was not the primary focus during the development of our methodology. Daytime sleep behavior is another area of potential research interest. While the health implications of nocturnal insufficient and poor sleep are well-established, there's evidence indicating an increased mortality associated with habitual daytime sleep[@burazeri_2003]. However, other findings have shown potential benefits of daytime napping, particularly in enhancing glycaemic control for individuals with type 2 diabetes[@makino_2018]. Daytime sleep could also influence the relationship between physical activity and sleep. Consequently, forthcoming research could emphasize the creation of algorithms adept at detecting multiple sleep periods, capturing both daytime naps and nocturnal sleep.

As observed in @tbl-10, the variation in SPTs as identified by the ZM (mean = 9.2 hours, sd = 2.1) exceeds that of the SPTs determined by the XGBoost model. This disparity may stem from the disproportionate emphasis the model places on circadian rhythm features (clock proxy features), which might inadvertently limit its ability to recognize more diverse SPTs. Future investigations should delve into this matter, perhaps refining the features related to circadian rhythms to ensure a more balanced and accurate model output. Moreover, expanding the feature set could offer a potential solution, allowing the model to capture a wider range of sleep patterns and behaviors. Additionally, it would be prudent to explore more comprehensive biLSTM architectures. Integrating the biLSTM model in a sequential manner, similar to the approach taken with the XGBoost models, might also yield improved results and merits further investigation. Finally, the biLSTM would likely benefit from another loss function like a soft version of the F1 score like described by Pastor-Pellicer et al.[@pastor-pellicer_2013] to better account for the class imbalance.

As presented in this thesis, the estimation of sleep, as evidenced by the limits of agreements, is not sufficiently reliable to be conducted on an individual basis. Looking beyond the present scope of our research, there lies an intriguing possibility in the development of adaptive machine learning models that tailor predictions based on individual sleep patterns[@oyebode_2023]. Such models, known for their real-time adaptability, could cater to outliers or individuals with specific sleep disorders, ensuring personalized insights. This adaptive approach presents an exciting avenue for future sleep research, and while it may seem distant, the potential benefits in terms of accuracy and personalization are significant.

## Generalizability of Our Sleep Models

In Paper III, we focused on a sample population of children with a mean age of 9.2 years and a standard deviation of 2.1. By focusing on a pediatric demographic, our study provides unique insights into the sleep patterns and habits of children, a population that may sometimes be underrepresented in sleep studies. As children undergo significant developmental changes, understanding their sleep behavior is crucial. However, to ensure our algorithm's utility across the life course, the need for diverse validation becomes all the more imperative. As noted by Mukherjee et al.[@mukherjee_2015], sleep physiology and habits evolve throughout an individual's life. This brings forward a critical consideration about the broader applicability of our algorithm across different age groups and clinical demographics. While studies have indicated that algorithms tailored for adults might not always align with children's sleep patterns, often detecting wakefulness when they are likely asleep[@quante_2018], our study was already grounded in pediatric sleep patterns. Nevertheless, the elderly and certain clinical populations exhibit distinct sleep profiles, often characterized by sleep-related challenges[@cassidy_2016; @espiritu_2008]. Conditions such as obesity and type 2 diabetes, which are associated with a heightened risk of obstructive sleep apnoea[@altaf_2017; @heinzer_2015], could introduce variations in sleep detection. The algorithm developed by van Hees et al. highlighted performance disparities between healthy sleepers and those with sleep disorders[@van_hees_estimating_2018]. Given our findings within the pediatric demographic, future work should involve the validation of our algorithm across a broader age spectrum and in varied clinical contexts to ensure its comprehensive reliability.

Transitioning to the topic of external validation, all splits of data for Paper III was sourced from one specific research project. To arrive at more concrete conclusions and to fortify the model's generalizability, diversifying datasets for external validation in future studies is important. However, the inherent challenge remains that amassing such data is a difficult task, demanding significant time and resources. Especially as there, to my knowledge, does not exist any publicly available datasets from thigh-worn accelerometers coupled with valid sleep records like the Newcastle or MESA datasets[@hees_newcastle_2018; @zhang_2018; @chen_2015]. It is thus essential to recognize the potential need for data that encapsulates diverse sleep behaviors, including napping, to ensure comprehensive sleep detection in future research.

## Multimodal Sensor Integration

Every year sees the release of an increasing number of devices. While the accuracy of early commercial devices faced skepticism based on scientific evaluations, recent multi-sensor devices now showcase precision that parallels leading research tools[@dezambotti_2019; @rentz_2021]. A prime example of advancements in consumer sleep wearables is the Oura-Ring. Designed to be worn on the finger, this device combines the functionalities of an accelerometer and a pulse-oximeter. The first-generation Oura Ring detected sleep with a sensitivity of 96% and was able to pinpoint REM sleep with an accuracy of 61% among adolescents and young adults[@dezambotti_2019]. Its successor, the second generation, touted an accuracy rate of 94% in sleep detection using accelerometer-based features and reached 96% accuracy when augmented with autonomic nervous system-derived features[@altini_2021]. Using a four-stage model for sleep detection---which encompasses light sleep (N1 and N2 stages), deep sleep (N3), REM, and wake---the device managed an accuracy of 57% with accelerometer-derived data and jumped to 79% when oximeter data was included. However, performance varies across devices. For instance, various Fitbit Wristband models exhibit differing accuracies for specific sleep stages: they range between 69%-81% for light sleep, 36%-89% for deep sleep, and 62%-89% for REM sleep\[\@haghayegh_2019\]. Such fluctuations indicate that relying solely on acceleration data may not always yield a comprehensive view of sleep stages. Further evidence points to heart rate variability (HRV) as a significant marker for sleep stage detection[@herzig_2018; @chouchou_2014]. A notable study achieved a striking 89% accuracy in identifying deep sleep when HRV was combined with respiratory signals[@long_2017]. Another research effort merged HRV with accelerometer data, achieving accurate detection for 75% of deep sleep and over 70% of REM sleep instances, though light sleep identification varied between 42% and 52%[@muzet_2016]. This thesis, however, zeroed in on algorithms rooted in acceleration and surface skin temperature data. While the inclusion of heart rate and other physiological markers could enhance sleep classification, this enhancement isn't without its trade-offs.

Choosing between a streamlined sensor system and a robust multi-sensor setup largely depends on the specific application. Short-term sleep assessments, typically spanning a week or two for research or clinical purposes, may benefit from a broader sensor array, given the depth of information it offers. Conversely, for longer evaluations stretching over months, prioritizing comfort and minimizing data loss risks, whether due to technical hitches or user compliance, gains prominence. Even with the integration of multimodal sensors, this approach remains cost-efficient compared to PSG and also opens doors to expansive research opportunities and interventions in sleep science.

As a closing remark, the journey embarked upon in this thesis is not a culmination but a commencement. The vision is clear: to reenvision sleep health, making it more accessible, accurate, and actionable for society at large. As technology and healthcare coalesce, the dream of a well-rested world, empowered by data-driven insights, inches ever closer to reality.

# Code Availability

All code associated with Paper II, used for data processing and analysis, and for producing figures, tables, and results, is available at <https://github.com/esbenlykke/nonwear_project>. This specific codebase requires substantial refactoring and commenting to adhere to best coding practices. However, upon request, the corresponding author can assist in utilizing the code if needed.

For Paper III, all code used for data processing, analysis, figure and table generation, results production, and manuscript pdf document creation is available at <https://github.com/esbenlykke/sleep_study>. This codebase largely adheres to good coding practices and should be deployable 'out of the box', provided all necessary dependencies are installed. To facilitate this, I plan to provide a conda/mamba environment description in the repository to encompass all required dependencies. Note that the repository currently contains some redundant scripts; however, I intend to streamline the content in the future. Additionally, plans are underway to introduce a Snakemake file to automate the entire process.

I've also developed a tool based on the findings from Paper III. This tool leverages the best-performing models (specifically, the XGBoost models trained on 5-minute median-filtered ZM sleep predictions) and is available at <https://github.com/esbenlykke/get_sleep_stats>. When given the path to a folder with .wav or .cwa raw accelerometer files, the tool first extracts all relevant features from the raw data. It then predicts and extracts in-bed periods. Lastly, it predicts sleep time based on the extracted in-bed periods. The final output includes timestamps for going to bed and leaving bed, SPT, TST, and SE for each accelerometer recording.

No code is available for Paper I.

# References

::: {#refs}
:::

# List of Appendices

-   **Appendix I**: Manual Annotation of Time in Bed Using Free-Living Recordings of Accelerometry Data

-   **Appendix II**: Generalizability and performance of methods to detect non-wear with free-living accelerometer recordings

-   **Appendix III**: Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking

\newpage

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix I}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix I}

\vspace{2cm}

\textsf{\Huge Manual Annotation of Time in Bed Using Free-Living Accelerometry Data}

\vspace{5cm}

This paper was published in \textbf{Sensors} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.3390/s21248442}{https://doi.org/10.3390/s21248442}

\end{center}
```
\includepdf[pages=-]{my_papers/paper1.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix II}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix II}

\vspace{2cm}

\textsf{\Huge Generalizability and Performance of Methods to Detect Non-Wear With Free-Living Accelerometer Recordings}

\vspace{5cm}

This paper was published in \textbf{Scientific Reports} and is used here under the terms and conditions of the Creative Commons Attribution (CC BY) license (\href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/})

\vspace{1cm}

DOI: \href{https://doi.org/10.1038/s41598-023-29666-x}{https://doi.org/10.1038/s41598-023-29666-x}

\end{center}
```
\includepdf[pages=-]{my_papers/paper2.pdf}

```{=tex}
\begin{center}

\textbf{\textsf{\Huge Appendix III}}

\phantomsection

\addcontentsline{toc}{subsection}{Appendix III}

\vspace{2cm}

\textsf{\Huge Improving Sleep Quality Estimation in Children and Adolescents: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking}

\vspace{5cm}

This manuscript is under preparation for submission to \href{https://academic.oup.com/sleep}{\textbf{SLEEP}}, the official journal of the Sleep Research Society (SRS).

\vspace{1cm}

\end{center}
```
\includepdf[pages=-]{my_papers/paper3.pdf}
